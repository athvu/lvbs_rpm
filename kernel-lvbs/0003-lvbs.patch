From 722674ea461352fa813ff68c0b1476b262271cf6 Mon Sep 17 00:00:00 2001
From: Thara Gopinath <tgopinath@microsoft.com>
Date: Mon, 24 Jun 2024 07:05:06 -0400
Subject: [PATCH 03/22] Enable VTL1 and boot primary cpu in VTL1

Bring up VTL1 partition. Load the secure loader and secure kernel and
allow primary/boot cpu to boot in VTL1.

Co-developed-by: Angelina Vu <angelinavu@microsoft.com>
Signed-off-by: Angelina Vu <angelinavu@microsoft.com>
Signed-off-by: Thara Gopinath <tgopinath@microsoft.com>
---
 arch/x86/include/asm/hyperv-tlfs.h | 101 ++++
 drivers/hv/Kconfig                 |   5 +
 drivers/hv/Makefile                |   2 +-
 drivers/hv/hv_common.c             |  10 +-
 drivers/hv/hv_vsm.h                |  33 +
 drivers/hv/hv_vsm_boot.c           | 935 +++++++++++++++++++++++++++++
 drivers/hv/hv_vsm_boot.h           | 383 ++++++++++++
 include/asm-generic/hyperv-tlfs.h  |   4 +
 8 files changed, 1466 insertions(+), 7 deletions(-)
 create mode 100644 drivers/hv/hv_vsm.h
 create mode 100644 drivers/hv/hv_vsm_boot.c
 create mode 100644 drivers/hv/hv_vsm_boot.h

diff --git a/arch/x86/include/asm/hyperv-tlfs.h b/arch/x86/include/asm/hyperv-tlfs.h
index 2ff26f53cd62..28d110087227 100644
--- a/arch/x86/include/asm/hyperv-tlfs.h
+++ b/arch/x86/include/asm/hyperv-tlfs.h
@@ -799,6 +799,107 @@ struct hv_get_vp_from_apic_id_in {
 	u32 apic_ids[];
 } __packed;
 
+/* Types for the EnablePartitionVtl hypercall */
+union hv_enable_partition_vtl_flags {
+	u8 as_u8;
+
+	struct {
+		u8 enable_mbec : 1;
+		u8 reserved : 7;
+	};
+};
+
+struct hv_input_enable_partition_vtl {
+	u64 partition_id;
+	u8 target_vtl;
+	union hv_enable_partition_vtl_flags flags;
+	u16 reserved16_z;
+	u32 reserved32_z;
+} __packed;
+
+enum hv_register_name {
+	HvRegisterVsmCodePageOffsets = 0x000d0002,
+	HvRegisterVsmVpStatus = 0x000d0003,
+	HvRegisterVsmPartitionStatus = 0x000d0004,
+	HvRegisterVsmCapabilities = 0x000d0006,
+};
+
+union hv_register_value {
+	u64 as_u64;
+	u32 as_u32;
+	u16 as_u16;
+	u8 as_u8;
+};
+
+union hv_register_vsm_partition_status {
+	u64 as_u64;
+
+	struct {
+		u64 enabled_vtl_set : 16;
+		u64 max_vtl : 4;
+		u64 mbec_enabled_vtl_set: 16;
+		u64 reserved_z : 28;
+	};
+};
+
+struct hv_input_get_vp_registers {
+	u64 partition_id;
+	u32 vp_index;
+	union hv_input_vtl input_vtl;
+	u8 reserved8_z;
+	u16 reserved16_z;
+	__aligned(8) enum hv_register_name names[1];
+};
+
+union hv_register_vsm_vp_status {
+	u64 as_u64;
+
+	struct {
+		u64 active_vtl : 4;
+		u64 active_mbec_enabled : 1;
+		u64 reserved_z0 : 11;
+		u64 enabled_vtl_set : 16;
+		u64 reserved_z1 : 32;
+	};
+};
+
+/* Types for the EnableVpVtl hypercall */
+struct hv_initial_vp_context {
+	u64 rip;
+	u64 rsp;
+	u64 rflags;
+
+	/* Segment selector registers together with their hidden state */
+	struct hv_x64_segment_register cs;
+	struct hv_x64_segment_register ds;
+	struct hv_x64_segment_register es;
+	struct hv_x64_segment_register fs;
+	struct hv_x64_segment_register gs;
+	struct hv_x64_segment_register ss;
+	struct hv_x64_segment_register tr;
+	struct hv_x64_segment_register ldtr;
+
+	/* Global and Interrupt Descriptor tables */
+	struct hv_x64_table_register idtr;
+	struct hv_x64_table_register gdtr;
+
+	/* Control registers and MSRs */
+	u64 efer;
+	u64 cr0;
+	u64 cr3;
+	u64 cr4;
+	u64 msr_cr_pat;
+};
+
+struct hv_input_enable_vp_vtl {
+	u64 partition_id;
+	u32 vp_index;
+	u8 target_vtl;
+	u8 reserved_z0;
+	u16 reserved_z1;
+	struct hv_initial_vp_context vp_vtl_context;
+};
+
 #include <asm-generic/hyperv-tlfs.h>
 
 #endif
diff --git a/drivers/hv/Kconfig b/drivers/hv/Kconfig
index 37f84cc0bf91..650be40fb55b 100644
--- a/drivers/hv/Kconfig
+++ b/drivers/hv/Kconfig
@@ -66,4 +66,9 @@ config HYPERV_VSM
 	  establish an interface between VTL0 and VTL1 to request for
 	  VSM services.
 
+config HYPERV_VSM_DEBUG
+	bool
+	depends on HYPERV_VSM
+	default n
+
 endmenu
diff --git a/drivers/hv/Makefile b/drivers/hv/Makefile
index d6811bb9580c..44efca6b2993 100644
--- a/drivers/hv/Makefile
+++ b/drivers/hv/Makefile
@@ -15,4 +15,4 @@ hv_utils-y := hv_util.o hv_kvp.o hv_snapshot.o hv_fcopy.o hv_utils_transport.o
 
 # Code that must be built-in
 obj-$(subst m,y,$(CONFIG_HYPERV)) += hv_common.o
-obj-$(subst m,y,$(CONFIG_HYPERV_VSM)) += hv_vsm_securekernel.o
+obj-$(subst m,y,$(CONFIG_HYPERV_VSM)) += hv_vsm_securekernel.o hv_vsm_boot.o
diff --git a/drivers/hv/hv_common.c b/drivers/hv/hv_common.c
index ccad7bca3fd3..9db40de8cf9d 100644
--- a/drivers/hv/hv_common.c
+++ b/drivers/hv/hv_common.c
@@ -329,11 +329,8 @@ int __init hv_common_init(void)
 	hyperv_pcpu_input_arg = alloc_percpu(void  *);
 	BUG_ON(!hyperv_pcpu_input_arg);
 
-	/* Allocate the per-CPU state for output arg for root */
-	if (hv_root_partition) {
-		hyperv_pcpu_output_arg = alloc_percpu(void *);
-		BUG_ON(!hyperv_pcpu_output_arg);
-	}
+	hyperv_pcpu_output_arg = alloc_percpu(void *);
+	BUG_ON(!hyperv_pcpu_output_arg);
 
 	hv_vp_index = kmalloc_array(num_possible_cpus(), sizeof(*hv_vp_index),
 				    GFP_KERNEL);
@@ -373,11 +370,12 @@ int hv_common_cpu_init(unsigned int cpu)
 	 * allocated if this CPU was previously online and then taken offline
 	 */
 	if (!*inputarg) {
+		pgcount = 2;
 		mem = kmalloc(pgcount * HV_HYP_PAGE_SIZE, flags);
 		if (!mem)
 			return -ENOMEM;
 
-		if (hv_root_partition) {
+		if (pgcount > 1) {
 			outputarg = (void **)this_cpu_ptr(hyperv_pcpu_output_arg);
 			*outputarg = (char *)mem + HV_HYP_PAGE_SIZE;
 		}
diff --git a/drivers/hv/hv_vsm.h b/drivers/hv/hv_vsm.h
new file mode 100644
index 000000000000..5c14034fb482
--- /dev/null
+++ b/drivers/hv/hv_vsm.h
@@ -0,0 +1,33 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (c) 2023, Microsoft Corporation.
+ *
+ * Author:
+ *
+ */
+
+#ifndef _HV_VSM_H
+#define _HV_VSM_H
+
+extern bool hv_vsm_boot_success;
+extern bool hv_vsm_mbec_enabled;
+extern union hv_register_vsm_code_page_offsets vsm_code_page_offsets;
+
+struct vtlcall_param {
+	u64	a0;
+	u64	a1;
+	u64	a2;
+	u64	a3;
+} __packed;
+
+union hv_register_vsm_code_page_offsets {
+	u64 as_uint64;
+
+	struct {
+		u64 vtl_call_offset : 12;
+		u64 vtl_return_offset : 12;
+		u64 reserved_z : 40;
+	};
+} __packed;
+
+#endif /* _HV_VSM_H */
diff --git a/drivers/hv/hv_vsm_boot.c b/drivers/hv/hv_vsm_boot.c
new file mode 100644
index 000000000000..99a383d86a0a
--- /dev/null
+++ b/drivers/hv/hv_vsm_boot.c
@@ -0,0 +1,935 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * VSM boot framework that enables VTL1, loads secure kernel
+ * and boots VTL1.
+ *
+ * Copyright (c) 2023, Microsoft Corporation.
+ *
+ */
+
+#include <asm/hyperv-tlfs.h>
+#include <asm/mshyperv.h>
+#include <asm/e820/api.h>
+#include <linux/hyperv.h>
+#include <linux/module.h>
+#include <linux/file.h>
+#include <linux/fs.h>
+#include <linux/cpumask.h>
+
+#include "hv_vsm_boot.h"
+#include "hv_vsm.h"
+
+extern struct resource sk_res;
+static struct file *sk_loader, *sk;
+static phys_addr_t vsm_skm_pa;
+static void *vsm_skm_va;
+union hv_register_vsm_code_page_offsets vsm_code_page_offsets;
+
+bool hv_vsm_boot_success = false;
+bool hv_vsm_mbec_enabled = true;
+
+#ifdef CONFIG_HYPERV_VSM
+
+static int vsm_arch_has_vsm_access(void)
+{
+	if (!(ms_hyperv.features & HV_MSR_SYNIC_AVAILABLE))
+		return false;
+	if (!(ms_hyperv.priv_high & HV_ACCESS_VSM))
+		return false;
+	if (!(ms_hyperv.priv_high & HV_ACCESS_VP_REGS))
+		return false;
+	return true;
+}
+
+static int hv_vsm_get_code_page_offsets(void)
+{
+	u64 status;
+	unsigned long flags;
+	struct hv_input_get_vp_registers *hvin = NULL;
+	union hv_register_value *hvout = NULL;
+
+	hvin = *this_cpu_ptr(hyperv_pcpu_input_arg);
+	hvout = *this_cpu_ptr(hyperv_pcpu_output_arg);
+
+	hvin->partition_id = HV_PARTITION_ID_SELF;
+	hvin->vp_index = HV_VP_INDEX_SELF;
+	hvin->input_vtl.as_uint8 = 0;
+	hvin->reserved8_z = 0;
+	hvin->reserved16_z = 0;
+	hvin->names[0] = HvRegisterVsmCodePageOffsets;
+
+	local_irq_save(flags);
+	status = hv_do_rep_hypercall(HVCALL_GET_VP_REGISTERS, 1, 0, hvin, hvout);
+	local_irq_restore(flags);
+
+	if (hv_result_success(status)) {
+		vsm_code_page_offsets.as_uint64 = hvout->as_u64;
+		return 0;
+	} else {
+		return -EFAULT;
+	}
+}
+
+static int hv_vsm_get_partition_status(u16 *enabled_vtl_set, u8 *max_vtl, u16 *mbec_enabled_vtl_set)
+{
+	u64 status;
+	unsigned long flags;
+	struct hv_input_get_vp_registers *hvin = NULL;
+	union hv_register_value *hvout = NULL;
+	union hv_register_vsm_partition_status vsm_partition_status = { 0 };
+
+	local_irq_save(flags);
+
+	hvin = *this_cpu_ptr(hyperv_pcpu_input_arg);
+	hvout = *this_cpu_ptr(hyperv_pcpu_output_arg);
+
+	hvin->partition_id = HV_PARTITION_ID_SELF;
+	hvin->vp_index = HV_VP_INDEX_SELF;
+	hvin->input_vtl.as_uint8 = 0;
+	hvin->reserved8_z = 0;
+	hvin->reserved16_z = 0;
+	hvin->names[0] = HvRegisterVsmPartitionStatus;
+
+	status = hv_do_rep_hypercall(HVCALL_GET_VP_REGISTERS, 1, 0, hvin, hvout);
+	local_irq_restore(flags);
+
+	if (!hv_result_success(status))
+		return -EFAULT;
+
+	vsm_partition_status = (union hv_register_vsm_partition_status)hvout->as_u64;
+	*enabled_vtl_set = vsm_partition_status.enabled_vtl_set;
+	*max_vtl = vsm_partition_status.max_vtl;
+	*mbec_enabled_vtl_set = vsm_partition_status.mbec_enabled_vtl_set;
+	return 0;
+}
+
+static int hv_vsm_get_vp_status(u16 *enabled_vtl_set, u8 *active_mbec_enabled)
+{
+	u64 status;
+	unsigned long flags;
+	struct hv_input_get_vp_registers *hvin = NULL;
+	union hv_register_value *hvout = NULL;
+	union hv_register_vsm_vp_status vsm_vp_status = { 0 };
+
+	hvin = *this_cpu_ptr(hyperv_pcpu_input_arg);
+	hvout = *this_cpu_ptr(hyperv_pcpu_output_arg);
+
+	hvin->partition_id = HV_PARTITION_ID_SELF;
+	hvin->vp_index = HV_VP_INDEX_SELF;
+	hvin->input_vtl.as_uint8 = 0;
+	hvin->reserved8_z = 0;
+	hvin->reserved16_z = 0;
+	hvin->names[0] = HvRegisterVsmVpStatus;
+
+	local_irq_save(flags);
+	status = hv_do_rep_hypercall(HVCALL_GET_VP_REGISTERS, 1, 0, hvin, hvout);
+	local_irq_restore(flags);
+
+	if (!hv_result_success(status))
+		return -EFAULT;
+
+	vsm_vp_status = (union hv_register_vsm_vp_status)hvout->as_u64;
+	*enabled_vtl_set = vsm_vp_status.enabled_vtl_set;
+	*active_mbec_enabled = vsm_vp_status.active_mbec_enabled;
+
+	return 0;
+}
+
+#ifdef CONFIG_HYPERV_VSM_DEBUG
+/* Walks page tables, starting at the PML4 (top level). */
+static void hv_vsm_dump_pt(u64 root, int lvl)
+{
+	int n;
+
+	u64 entry_pa;
+	u64 *entry_va;
+	u64 next_pa;
+
+	if (lvl == 5)
+		return;
+
+	for (n = 0; n < VSM_ENTRIES_PER_PT; n++) {
+		entry_pa = root + (n * sizeof(u64));
+		entry_va = VSM_NON_LOGICAL_PHYS_TO_VIRT(entry_pa);
+
+		if (*entry_va)
+			pr_info("\t\t Entry: %i/%i - 0x%llx\n", n, lvl,
+				*entry_va);
+
+		if (*entry_va & VSM_PAGE_PRESENT) {
+			next_pa = *entry_va & VSM_PAGE_BASE_ADDR_MASK;
+			hv_vsm_dump_pt(next_pa, lvl + 1);
+		}
+	}
+}
+
+static void hv_vsm_dump_secure_kernel_memory(void)
+{
+	pr_info("%s: Dumping Secure Kernel Memory\n", __func__);
+	print_hex_dump(KERN_INFO, "\t", DUMP_PREFIX_ADDRESS, 32, 4,
+		(void *)vsm_skm_va, 1024, 0);
+}
+#endif
+
+static __init void __hv_vsm_init_vtlcall(struct vtlcall_param *args)
+{
+	u64 hcall_addr;
+
+	hcall_addr = (u64)((u8 *)hv_hypercall_pg + vsm_code_page_offsets.vtl_call_offset);
+	register u64 hypercall_addr asm("rax") = hcall_addr;
+
+	asm __volatile__ (	\
+	/* Keep copies of all the registers */
+		"pushq	%%rdi\n"
+		"pushq	%%rsi\n"
+		"pushq	%%rdx\n"
+		"pushq	%%rbx\n"
+		"pushq	%%rcx\n"
+		"pushq	%%rax\n"
+		"pushq	%%r8\n"
+		"pushq	%%r9\n"
+		"pushq	%%r10\n"
+		"pushq	%%r11\n"
+		"pushq	%%r12\n"
+		"pushq	%%r13\n"
+		"pushq	%%r14\n"
+		"pushq	%%r15\n"
+	/*
+	 * The vtlcall_param structure is in rdi, which is modified below, so copy it into a
+	 * register that stays constant in the instructon block immediately following.
+	 */
+		"movq	%1, %%r12\n"
+
+	/* Copy values from vtlcall_param structure into registers used to communicate with VTL1 */
+		"movq	0x00(%%r12), %%rdi\n"
+		"movq	0x08(%%r12), %%rsi\n"
+		"movq	0x10(%%r12), %%rdx\n"
+		"movq	0x18(%%r12), %%r8\n"
+	/* Make rcx 0 */
+		"xorl	%%ecx, %%ecx\n"
+	/* VTL call */
+		CALL_NOSPEC
+	/* Restore r12 with vtlcall_param after VTL call */
+		"movq	104(%%rsp), %%r12\n"
+
+	/* Copy values from registers used to communicate with VTL1 into vtlcall_param structure */
+		"movq	%%rdi,  0x00(%%r12)\n"
+		"movq	%%rsi,  0x08(%%r12)\n"
+		"movq	%%rdx,  0x10(%%r12)\n"
+		"movq	%%r8,  0x18(%%r12)\n"
+
+	/* Restore all modified registers */
+		"popq	%%r15\n"
+		"popq	%%r14\n"
+		"popq	%%r13\n"
+		"popq	%%r12\n"
+		"popq	%%r11\n"
+		"popq	%%r10\n"
+		"popq	%%r9\n"
+		"popq	%%r8\n"
+		"popq	%%rax\n"
+		"popq	%%rcx\n"
+		"popq	%%rbx\n"
+		"popq	%%rdx\n"
+		"popq	%%rsi\n"
+		"popq	%%rdi\n"
+		: ASM_CALL_CONSTRAINT
+		: "D"(args), THUNK_TARGET(hypercall_addr)
+		: "cc", "memory");
+}
+
+__init int hv_vsm_init_vtlcall(struct vtlcall_param *args)
+{
+	unsigned long flags = 0;
+	u64 cr2;
+
+	local_irq_save(flags);
+	kernel_fpu_begin_mask(0);
+	cr2 = native_read_cr2();
+	__hv_vsm_init_vtlcall(args);
+	native_write_cr2(cr2);
+	kernel_fpu_end();
+	local_irq_restore(flags);
+
+	return (int)args->a3;
+}
+
+static __init void hv_vsm_boot_vtl1(void)
+{
+	struct vtlcall_param args = {0};
+	u16 vp_enabled_vtl_set = 0;
+	u8 active_mbec_enabled = 0;
+
+	args.a0 = num_possible_cpus();
+	args.a1 = sk_res.start;
+	args.a2 = sk_res.end + 1;
+	args.a3 = max_pfn << PAGE_SHIFT;
+
+	hv_vsm_init_vtlcall(&args);
+
+	hv_vsm_get_vp_status(&vp_enabled_vtl_set, &active_mbec_enabled);
+	if (!active_mbec_enabled) {
+		pr_err("Failed to enable MBEC for VP0\n");
+		hv_vsm_mbec_enabled = false;
+	}
+}
+
+static int __init hv_vsm_enable_partition_vtl(void)
+{
+	u64 status = 0;
+	unsigned long flags;
+	struct hv_input_enable_partition_vtl *hvin = NULL;
+
+	local_irq_save(flags);
+
+	hvin = *this_cpu_ptr(hyperv_pcpu_input_arg);
+	hvin->partition_id = HV_PARTITION_ID_SELF;
+	hvin->target_vtl = 1;
+	hvin->flags.enable_mbec = 1;
+	hvin->flags.reserved = 0;
+	hvin->reserved16_z = 0;
+	hvin->reserved32_z = 0;
+
+	status = hv_do_hypercall(HVCALL_ENABLE_PARTITION_VTL, hvin, NULL);
+
+	if (status)
+		pr_err("%s: Enable Partition VTL failed. status=0x%x\n", __func__, hv_result(status));
+
+	local_irq_restore(flags);
+
+	return hv_result(status);
+}
+
+static int __init hv_vsm_reserve_sk_mem(void)
+{
+	void *va_start;
+	struct page **page, **pages;
+	unsigned long long paddr, sk_size;
+	unsigned long size;
+	int i, npages;
+
+	if (!sk_res.start) {
+		pr_err("%s: No memory reserved in cmdline for secure kernel", __func__);
+		return -ENOMEM;
+	}
+	vsm_skm_pa = sk_res.start;
+	vsm_skm_va = 0;
+
+	/* Allocate an array of struct page pointers  */
+	sk_size = sk_res.end - sk_res.start + 1;
+	npages = sk_size >> PAGE_SHIFT;
+	size = sizeof(struct page *) * npages;
+	pages = vmalloc(size);
+
+	if (!pages) {
+		pr_err("%s: Allocating array of struct page pointers failed (Size: %lu)\n",
+				__func__, size);
+		return -ENOMEM;
+	}
+
+	/* Convert each page frame number to struct page.
+	   Memory was allocated using memblock_phys_alloc_range() during boot */
+	page = pages;
+	paddr = vsm_skm_pa;
+	for (i = 0; i < npages; i++) {
+		*page++ = pfn_to_page(paddr >> PAGE_SHIFT);
+		paddr += PAGE_SIZE;
+	}
+
+	/* Map Secure Kernel physical memory into kernel virtual address space */
+	va_start = vmap(pages, npages, VM_MAP, PAGE_KERNEL);
+
+	if (!va_start) {
+		pr_err("%s: Memory mapping failed\n", __func__);
+		vfree(pages);
+		return -ENOMEM; 
+	}
+
+	vsm_skm_va = va_start;
+
+	pr_info("%s: secure kernel PA=0x%lx, VA=0x%lx\n",
+			__func__, (unsigned long)vsm_skm_pa, (unsigned long)vsm_skm_va);
+
+	memset(vsm_skm_va, 0, sk_size);
+	vfree(pages);
+	return 0;
+}
+
+static void __init hv_vsm_init_cpu(struct hv_initial_vp_context *vp_ctx)
+{
+	/* Offset rip by any secure kernel header length */
+	vp_ctx->rip = (u64)VSM_VA_FROM_PA(PAGE_AT(vsm_skm_pa,
+		VSM_FIRST_CODE_PAGE));
+
+	vp_ctx->cr0 =
+		CR0_PG |	// Paging
+		CR0_WP |	// Write Protect
+		CR0_NE |	// Numeric Error
+		CR0_ET |	// Extension Type
+		CR0_MP |	// Math Present
+		CR0_PE;		// Protection Enable
+
+	vp_ctx->cr4 =
+		CR4_PSE	|	// Page Size Extensions
+		CR4_PGE	|	// Page Global Enable
+		CR4_PAE;	// Physical Address Extensions
+
+	vp_ctx->efer =
+		MSR_LMA |	// Long Mode Active
+		MSR_LME |	// Long Mode Enable
+		MSR_NXE |	// No Execute Enable
+		MSR_SCE;	// System Call Enable
+
+	/*
+	 * Intel CPUs fail if the architectural read-as-one bit 1 of RFLAGS is not
+	 * set. See Intel SDM Vol 3C, 26.3.1.4 (RFLAGS).
+	 *
+	 * TODO: Has Hyper-V implemented setting this automatically?
+	 */
+	vp_ctx->rflags = 0b10;
+
+	vp_ctx->msr_cr_pat =
+		VSM_PAT(0, WB)       | VSM_PAT(1, WT) |
+		VSM_PAT(2, UC_MINUS) | VSM_PAT(3, UC) |
+		VSM_PAT(4, WB)       | VSM_PAT(5, WT) |
+		VSM_PAT(6, UC_MINUS) | VSM_PAT(7, UC);
+
+#ifdef CONFIG_HYPERV_VSM_DEBUG
+	pr_info("%s : Printing Initial VP Registers..\n", __func__);
+	pr_info("\t\t RIP: 0x%llx\n", vp_ctx->rip);
+	pr_info("\t\t CR0: 0x%llx\n", vp_ctx->cr0);
+	pr_info("\t\t CR4: 0x%llx\n", vp_ctx->cr4);
+	pr_info("\t\t EFER: 0x%llx\n", vp_ctx->efer);
+	pr_info("\t\t RFLAGS: 0x%llx\n", vp_ctx->rflags);
+	pr_info("\t\t CR PAT: 0x%llx\n", vp_ctx->msr_cr_pat);
+#endif
+}
+
+static void __init hv_vsm_init_gdt(struct hv_initial_vp_context *vp_ctx)
+{
+	phys_addr_t gdt_pa;
+	phys_addr_t tss_pa;
+	phys_addr_t kstack_pa;
+
+	union vsm_code_seg_desc cseg;
+	union vsm_data_seg_desc dseg;
+	union vsm_sys_seg_desc sseg;
+	union vsm_tss *tss;
+
+	u64 tss_sk_va;
+
+	size_t cseg_sz = sizeof(cseg);
+	size_t dseg_sz = sizeof(dseg);
+	size_t sseg_sz = sizeof(sseg);
+	size_t tss_sz = sizeof(*tss);
+
+	size_t gdt_offset = 0;
+
+	/* Get a page for the GDT */
+	gdt_pa = PAGE_AT(vsm_skm_pa, VSM_GDT_PAGE);
+	/* Get a page for the TSS */
+	tss_pa = PAGE_AT(vsm_skm_pa, VSM_TSS_PAGE);
+	tss = VSM_NON_LOGICAL_PHYS_TO_VIRT(tss_pa);
+	/* Get a page for the secure kernel initial stack */
+	kstack_pa = PAGE_AT(vsm_skm_pa, VSM_KERNEL_STACK_PAGE);
+
+	/* Make and add the NULL descriptor to the GDT */
+	cseg = MAKE_CSD_LM(0, 0, 0, 0);
+	memcpy(VSM_NON_LOGICAL_PHYS_TO_VIRT(gdt_pa + gdt_offset), &cseg, cseg_sz);
+	gdt_offset += cseg_sz;
+
+	/* Make and add a code segment descriptor to the GDT */
+	cseg = MAKE_CSD_LM(0, 0, 1, 0);
+	memcpy(VSM_NON_LOGICAL_PHYS_TO_VIRT(gdt_pa + gdt_offset), &cseg, cseg_sz);
+	gdt_offset += cseg_sz;
+
+	/* Make and add a data segment descriptor to the GDT */
+	dseg = MAKE_DSD_LM(1, 0);
+	memcpy(VSM_NON_LOGICAL_PHYS_TO_VIRT(gdt_pa + gdt_offset), &dseg, dseg_sz);
+	gdt_offset += dseg_sz;
+
+	/* Compute the VA that secure kernele will see for the TSS */
+	tss_sk_va = VSM_VA_FROM_PA(tss_pa);
+
+	/* Make and add a system segment descriptor for the TSS in the GDT */
+	sseg = MAKE_SSD(
+		tss_sz,
+		tss_sk_va,
+		VSM_SYSTEM_SEGMENT_TYPE_TSS,
+		0,
+		1,
+		0,
+		0,
+		0,
+		tss_sk_va >> 24);
+	memcpy(VSM_NON_LOGICAL_PHYS_TO_VIRT(gdt_pa + gdt_offset), &sseg, sseg_sz);
+	gdt_offset += sseg_sz;
+
+#ifdef CONFIG_HYPERV_VSM_DEBUG
+	pr_info("%s: Printing GDT..\n", __func__);
+	pr_info("\t\t 0: 0x%llx\n", (u64)0);
+	pr_info("\t\t 1: 0x%llx\n", *(u64 *)&cseg);
+	pr_info("\t\t 2: 0x%llx\n", *(u64 *)&dseg);
+	pr_info("\t\t 3: 0x%llx\n", *(u64 *)&sseg);
+#endif
+	/* Set up the GDT register */
+	vp_ctx->gdtr.base = VSM_VA_FROM_PA(gdt_pa);
+	vp_ctx->gdtr.limit = gdt_offset - 1;
+
+#ifdef CONFIG_HYPERV_VSM_DEBUG
+	pr_info("%s Printing GDTR..\n", __func__);
+	pr_info("\t\t Base:  0x%llx\n", vp_ctx->gdtr.base);
+	pr_info("\t\t Limit: 0x%x\n",   vp_ctx->gdtr.limit);
+#endif
+	/* Set the code segment (CS) selector */
+	vp_ctx->cs.base = 0;
+	vp_ctx->cs.limit = 0;
+	vp_ctx->cs.selector = 1 << 3;
+	vp_ctx->cs.segment_type = VSM_CODE_SEGMENT_TYPE_EXECUTE_READ_ACCESSED;
+	vp_ctx->cs.non_system_segment = 1;
+	vp_ctx->cs.descriptor_privilege_level = 0;
+	vp_ctx->cs.present = 1;
+	vp_ctx->cs.reserved = 0;
+	vp_ctx->cs.available = 0;
+	vp_ctx->cs._long = 1;
+	vp_ctx->cs._default = 0;
+	vp_ctx->cs.granularity = 0;
+
+#ifdef CONFIG_HYPERV_VSM_DEBUG
+	pr_info("%s Printing CS...\n", __func__);
+	pr_info("\t\t Sel:   0x%x\n",   vp_ctx->cs.selector);
+	pr_info("\t\t Base:  0x%llx\n", vp_ctx->cs.base);
+	pr_info("\t\t Limit: 0x%x\n",   vp_ctx->cs.limit);
+	pr_info("\t\t Attrs: 0x%x\n",   vp_ctx->cs.attributes);
+#endif
+
+	/* Set the data segment (DS) selector */
+	vp_ctx->ds.base = 0;
+	vp_ctx->ds.limit = 0;
+	vp_ctx->ds.selector = 2 << 3;
+	vp_ctx->ds.segment_type = VSM_DATA_SEGMENT_TYPE_READ_WRITE_ACCESSED;
+	vp_ctx->ds.non_system_segment = 1;
+	vp_ctx->ds.descriptor_privilege_level = 0;
+	vp_ctx->ds.present = 1;
+	vp_ctx->ds.reserved = 0;
+	vp_ctx->ds.available = 0;
+	vp_ctx->ds._long = 0;
+	vp_ctx->ds._default = 1;
+	vp_ctx->ds.granularity = 0;
+
+	/* Set the ES, FS and GS to be the same as DS, for now */
+	vp_ctx->es = vp_ctx->ds;
+	vp_ctx->fs = vp_ctx->ds;
+	vp_ctx->gs = vp_ctx->ds;
+
+#ifdef CONFIG_HYPERV_VSM_DEBUG
+	pr_info("%s: Printing DS/ES/FS/GS...\n");
+	pr_info("\t\t Sel:   0x%x\n",   vp_ctx->ds.selector);
+	pr_info("\t\t Base:  0x%llx\n", vp_ctx->ds.base);
+	pr_info("\t\t Limit: 0x%x\n",   vp_ctx->ds.limit);
+	pr_info("\t\t Attrs: 0x%x\n",   vp_ctx->ds.attributes);
+#endif
+
+	/* Set the stack selector to 0 (unused in long mode) */
+	vp_ctx->ss.selector = 0;
+
+	/* Set the initial stack pointer for the kernel to point to bottom of kernel stack */
+	tss->rsp0 = VSM_VA_FROM_PA(kstack_pa) + VSM_PAGE_SIZE - 1;
+	vp_ctx->rsp = tss->rsp0;
+
+#ifdef CONFIG_HYPERV_VSM_DEBUG
+	pr_info("%s: Printing TSS...\n", __func__);
+	pr_info("\t\t RSP0: 0x%llx\n", tss->rsp0);
+#endif
+
+	/* Set the task register selector  */
+	vp_ctx->tr.base = tss_sk_va;
+	vp_ctx->tr.limit = tss_sz - 1;
+	vp_ctx->tr.selector = 3 << 3;
+	vp_ctx->tr.segment_type = VSM_SYSTEM_SEGMENT_TYPE_BUSY_TSS;
+	vp_ctx->tr.non_system_segment = 0;
+	vp_ctx->tr.descriptor_privilege_level = 0;
+	vp_ctx->tr.present = 1;
+	vp_ctx->tr.reserved = 0;
+	vp_ctx->tr.available = 0;
+	vp_ctx->tr._long = 0;
+	vp_ctx->tr._default = 0;
+	vp_ctx->tr.granularity = 0;
+
+#ifdef CONFIG_HYPERV_VSM_DEBUG
+	pr_info("%s: Printing TR...\n", __func__);
+	pr_info("\t\t Sel:   0x%x\n",   vp_ctx->tr.selector);
+	pr_info("\t\t Base:  0x%llx\n", vp_ctx->tr.base);
+	pr_info("\t\t Limit: 0x%x\n",   vp_ctx->tr.limit);
+	pr_info("\t\t Attrs: 0x%x\n",   vp_ctx->tr.attributes);
+#endif
+}
+
+/* ToDo: Evaluate whether IDT need to be populated */
+static void __init hv_vsm_init_idt(struct hv_initial_vp_context *vp_ctx)
+{
+	phys_addr_t idt_pa;
+	u128 seg;
+	u64 target;
+	u16 target_lo;
+	u128 target_hi;
+	u64 type;
+	u16 selector;
+	size_t seg_sz = sizeof(seg);
+	int n;
+
+	idt_pa = PAGE_AT(vsm_skm_pa, VSM_IDT_PAGE);
+
+	/* Fill in the IDT to point to the ISR stubs */
+	selector = 1 << 3;
+	for (n = 0; n <= 21; n++) {
+		type = VSM_GATE_TYPE_INT;
+		target = (u64)VSM_VA_FROM_PA(PAGE_AT(vsm_skm_pa,
+			VSM_ISRS_CODE_PAGE) + (n * 8));
+
+		target_lo = (u16)target;
+		target_hi = (u128)target << 32;
+
+		seg = target_lo | target_hi | ((u64)selector << 16) |
+			(VSM_GATE_TYPE_INT << 40) | ((u64)1 << 47);
+		
+		memcpy(VSM_NON_LOGICAL_PHYS_TO_VIRT(idt_pa + (n * seg_sz)), &seg, seg_sz);
+	}
+
+	/* Set the IDT register */
+	vp_ctx->idtr.base = VSM_VA_FROM_PA(idt_pa);
+	vp_ctx->idtr.limit = (seg_sz * 22) - 1;
+}
+
+static void __init hv_vsm_fill_pte_tables(u64 *pde, int pd_index, int num_pte_tables)
+{
+	/* 
+	 * ToDo: Make a generic solution that can take any PA start and size and adjust
+	 * the number of page tables and determine where to start filling them. 
+	 */
+	u16 i, j;
+	phys_addr_t pte_pa;
+	u64 *pte;
+
+	/* Fill page tables with entries */
+	for (i = 0; i < num_pte_tables; i++) {
+		pte_pa = PAGE_AT(vsm_skm_pa, VSM_PTE_0_PAGE + i);
+		pte = VSM_NON_LOGICAL_PHYS_TO_VIRT(pte_pa);
+		*(pde + pd_index + i) = pte_pa | VSM_PAGE_PTE_OPTEE;
+		for (j = 0; j < VSM_ENTRIES_PER_PT; j++) {
+			*(pte + j) =
+				(vsm_skm_pa + ((j + (i * VSM_ENTRIES_PER_PT)) * VSM_PAGE_SIZE)) |
+					VSM_PAGE_PTE_OPTEE;
+		}
+	}
+}
+
+static void __init hv_vsm_init_page_tables(struct hv_initial_vp_context *vp_ctx)
+{
+	u64 pml4_index;
+	u64 pdp_index;
+	u64 pd_index;
+	phys_addr_t pml4e_pa;
+	phys_addr_t pdpe_pa;
+	phys_addr_t pde_pa;
+	u64 *pml4e;
+	u64 *pdpe;
+	u64 *pde;
+	int num_pte_tables;
+	
+	/* Get offset to know where to start mapping. Note vsm_skm_pa is the VA for OP-TEE */
+	pml4_index = VSM_GET_PML4_INDEX_FROM_VA(vsm_skm_pa);
+	pdp_index = VSM_GET_PDP_INDEX_FROM_VA(vsm_skm_pa);
+	pd_index = VSM_GET_PD_INDEX_FROM_VA(vsm_skm_pa);
+
+#ifdef CONFIG_HYPERV_VSM_DEBUG
+	pr_info("%s: pml4_index = 0x%llx, pdp_index = 0x%llx, pd_index=0x%llx\n",
+			__func__, pml4_index, pdp_index, pd_index);
+#endif
+
+	pml4e_pa = PAGE_AT(vsm_skm_pa, VSM_PML4E_PAGE);
+	pdpe_pa = PAGE_AT(vsm_skm_pa, VSM_PDPE_PAGE);
+	pde_pa = PAGE_AT(vsm_skm_pa, VSM_PDE_PAGE);
+
+	pml4e = VSM_NON_LOGICAL_PHYS_TO_VIRT(pml4e_pa);
+	pdpe = VSM_NON_LOGICAL_PHYS_TO_VIRT(pdpe_pa);
+	pde = VSM_NON_LOGICAL_PHYS_TO_VIRT(pde_pa);
+
+	/* N.B.: Adding '+ 1' to a pointer moves the underlying value forward by 8 bytes! */
+	*(pml4e + pml4_index) = pdpe_pa | VSM_PAGE_OPTEE;
+	*(pdpe + pdp_index) = pde_pa | VSM_PAGE_OPTEE;
+
+	/* Initial page tables map only the first SK_INITIAL_MAP_SIZE size of memory.
+	 * This memory will be used for the Secure Loader.
+	 */
+	num_pte_tables = (SK_INITIAL_MAP_SIZE / VSM_PAGE_SIZE) / VSM_ENTRIES_PER_PT;
+	hv_vsm_fill_pte_tables(pde, pd_index, num_pte_tables);
+
+	vp_ctx->cr3 = pml4e_pa;
+
+#ifdef CONFIG_HYPERV_VSM_DEBUG
+	pr_info("%s: Physical Range..\n", __func__);
+	pr_info("\t\t Start: 0x%llx\n", vsm_skm_pa);
+	pr_info("\t\t End:   0x%llx\n", vsm_skm_pa + SK_INITIAL_MAP_SIZE - 1);
+	pr_info("%s: Page Table Physical Addresses\n", __func__);
+	pr_info("\t\t PML4:   0x%llx\n", pml4e_pa);
+	pr_info("\t\t PDPE:   0x%llx\n", pdpe_pa);
+	pr_info("\t\t PDE:    0x%llx\n", pde_pa);
+	pr_info("\t\t PTE 0: 0x%llx\n", PAGE_AT(vsm_skm_pa, VSM_PTE_0_PAGE));
+	pr_info("\t\t PTE 1: 0x%llx\n", PAGE_AT(vsm_skm_pa, VSM_PTE_0_PAGE + 1));
+	pr_info("\t\t PTE 2: 0x%llx\n", PAGE_AT(vsm_skm_pa, VSM_PTE_0_PAGE + 2));
+	pr_info("\t\t PTE 3: 0x%llx\n", PAGE_AT(vsm_skm_pa, VSM_PTE_0_PAGE + 3));
+	pr_info("%s: Page Table Dump\n", __func__);
+	pr_info("\t\t Entry: Idx/Lvl - Raw Value\n");
+	hv_vsm_dump_pt(pml4e_pa, 1);
+#endif
+}
+
+static void __init hv_vsm_arch_init_vp_context(struct hv_initial_vp_context *vp_ctx)
+{
+	/* The CPU expects these structures to be properly laid out. */
+	compiletime_assert(sizeof(union vsm_code_seg_desc) == 8,
+		"Code Segment Descriptor is not 8 bytes.");
+	compiletime_assert(sizeof(union vsm_data_seg_desc) == 8,
+		"Code Segment Descriptor is not 8 bytes.");
+	compiletime_assert(sizeof(union vsm_sys_seg_desc) == 16,
+		"System Segment Descriptor is not 16 bytes.");
+	compiletime_assert(sizeof(union vsm_call_gate_seg_desc) == 16,
+		"Call-Gate Segment Descriptor is not 16 bytes.");
+	compiletime_assert(sizeof(union vsm_int_trap_gate_seg_desc) == 16,
+		"Interrupt-/Trap-Gate Segment Descriptor is not 16 bytes.");
+
+	hv_vsm_init_cpu(vp_ctx);
+	hv_vsm_init_gdt(vp_ctx);
+	hv_vsm_init_idt(vp_ctx);
+	hv_vsm_init_page_tables(vp_ctx);
+}
+
+static int __init hv_vsm_enable_vp_vtl(void)
+{
+	u64 status = 0;
+	unsigned long flags;
+	struct hv_input_enable_vp_vtl *hvin = NULL;
+
+	hvin = *this_cpu_ptr(hyperv_pcpu_input_arg);
+	hvin->partition_id = HV_PARTITION_ID_SELF;
+	hvin->vp_index = HV_VP_INDEX_SELF;
+	hvin->target_vtl = 1;
+	hvin->reserved_z0 = 0;
+	hvin->reserved_z1 = 0;
+
+	hv_vsm_arch_init_vp_context(&hvin->vp_vtl_context);
+
+	local_irq_save(flags);
+
+	status = hv_do_hypercall(HVCALL_ENABLE_VP_VTL, hvin, NULL);
+
+	local_irq_restore(flags);
+
+	return (int) (status & HV_HYPERCALL_RESULT_MASK);
+}
+
+static int __init hv_vsm_load_secure_kernel(void)
+{
+	/*
+	 * Till we combine the skloader and kernel into one binary, we have to load them separately
+	 * ToDo: Load them as one binary
+	 */
+	loff_t size_skloader, size_sk;
+	char *skloader_buf = NULL, *sk_buf = NULL;
+	int err;
+
+	// Find the size of skloader and sk
+	size_skloader = vfs_llseek(sk_loader, 0, SEEK_END);
+	size_sk = vfs_llseek(sk, 0, SEEK_END);
+
+	// Seek back to the beginning of the file
+	vfs_llseek(sk_loader, 0, SEEK_SET);
+	vfs_llseek(sk, 0, SEEK_SET);
+
+	// Allocate memory for the buffer
+	skloader_buf = kvmalloc(size_skloader, GFP_KERNEL);
+	if (!skloader_buf) {
+		pr_err("%s: Unable to allocate memory for copying secure kernel\n", __func__);
+		return -ENOMEM;
+	}
+	sk_buf = kvmalloc(size_sk, GFP_KERNEL);
+	if (!sk_buf) {
+		pr_err("%s: Unable to allocate memory for copying secure kernel\n", __func__);
+		kvfree(skloader_buf);
+		return -ENOMEM;
+	}
+
+	// Read from the file into the buffer
+	err = kernel_read(sk_loader, skloader_buf, size_skloader, &sk_loader->f_pos);
+	if (err != size_skloader) {
+		pr_err("%s Unable to read skloader.bin file\n", __func__);
+		kvfree(skloader_buf);
+		kvfree(sk_buf);
+		return -1;
+	}
+	err = kernel_read(sk, sk_buf, size_sk, &sk->f_pos);
+	if (err != size_sk) {
+		pr_err("%s Unable to read vmlinux.bin file\n", __func__);
+		kvfree(skloader_buf);
+		kvfree(sk_buf);
+		return -1;
+	}
+
+	memcpy(vsm_skm_va, skloader_buf, size_skloader);
+	memcpy(vsm_skm_va + (2 * 1024 * 1024), sk_buf, size_sk);
+	kvfree(skloader_buf);
+	kvfree(sk_buf);
+	return 0;
+}
+
+int __init hv_vsm_enable_vtl1(void)
+{
+	cpumask_var_t mask;
+	unsigned int boot_cpu;
+	u16 partition_enabled_vtl_set = 0, partition_mbec_enabled_vtl_set = 0, vp_enabled_vtl_set = 0;
+	u8 partition_max_vtl, active_mbec_enabled = 0;
+	int ret = 0;
+
+	if (!vsm_arch_has_vsm_access()) {
+		pr_err("%s: Arch does not support VSM\n", __func__);
+		return -ENOTSUPP;
+	}
+	if (hv_vsm_reserve_sk_mem()) {
+		pr_err("%s: Could not initialize memory for secure kernel\n", __func__);
+		return -ENOMEM;
+	}
+
+	sk_loader = filp_open("/usr/lib/firmware/skloader.bin", O_RDONLY, 0);
+	if (IS_ERR(sk_loader)) {
+		pr_err("%s: File usr/lib/firmware/skloader.bin not found\n", __func__);
+		ret = -ENOENT;
+		goto free_mem;
+	}
+	sk = filp_open("/usr/lib/firmware/vmlinux.bin", O_RDONLY, 0);
+	if (IS_ERR(sk)) {
+		pr_err("%s: File usr/lib/firmware/vmlinux.bin not found\n", __func__);
+		ret = -ENOENT;
+		goto close_file;
+	}
+
+	ret = hv_vsm_get_code_page_offsets();
+	if (ret) {
+		pr_err("%s: Unbable to retrieve vsm page offsets\n", __func__);
+		goto close_files;
+	}
+
+	/*
+	 * Copy the current cpu mask and pin rest of the running code to boot cpu.
+	 * Important since we want boot cpu of VTL0 to be the boot cpu for VTL1.
+	 * ToDo: Check if copying and restoring current->cpus_mask is enough
+	 * ToDo: Verify the assumption that cpumask_first(cpu_online_mask) is
+	 * the boot cpu
+	 */
+	if (!alloc_cpumask_var(&mask, GFP_KERNEL)) {
+		pr_err("%s: Could not allocate cpumask", __func__);
+		ret = -ENOMEM;
+		goto close_files;
+	}
+
+	cpumask_copy(mask, &current->cpus_mask);
+	boot_cpu = cpumask_first(cpu_online_mask);
+	set_cpus_allowed_ptr(current, cpumask_of(boot_cpu));
+
+	/* Check and enable VTL1 at the partition level */
+	ret = hv_vsm_get_partition_status(&partition_enabled_vtl_set, &partition_max_vtl, &partition_mbec_enabled_vtl_set);
+	if (ret)
+		goto out;
+
+	if (partition_max_vtl < HV_VTL1) {
+		pr_err("%s: VTL1 is not supported", __func__);
+		ret = -EINVAL;
+		goto out;
+	}
+	if (partition_enabled_vtl_set & HV_VTL1_ENABLE_BIT) {
+		pr_info("%s: Partition VTL1 is already enabled\n", __func__);
+	} else {
+		ret = hv_vsm_enable_partition_vtl();
+		if (ret) {
+			pr_err("%s: Enabling Partition VTL1 failed with status 0x%x\n", __func__, ret);
+			ret = -EINVAL;
+			goto out;
+		}
+		hv_vsm_get_partition_status(&partition_enabled_vtl_set, &partition_max_vtl, &partition_mbec_enabled_vtl_set);
+		if (!(partition_enabled_vtl_set & HV_VTL1_ENABLE_BIT)) {
+			pr_err("%s: Tried Enabling Partition VTL 1 and still failed", __func__);
+			ret = -EINVAL;
+			goto out;
+		}
+		if (!partition_mbec_enabled_vtl_set) {
+			pr_err("%s: Tried Enabling Partition MBEC and failed", __func__);
+			ret = -EINVAL;
+			goto out;
+		}
+	}
+
+	/* Check and enable VTL1 for the primary virtual processor */
+	ret = hv_vsm_get_vp_status(&vp_enabled_vtl_set, &active_mbec_enabled);
+	if (ret)
+		goto out;
+
+	if (vp_enabled_vtl_set & HV_VTL1_ENABLE_BIT) {
+		pr_info("%s: VP VTL1 is already enabled\n", __func__);
+	} else {
+		ret = hv_vsm_enable_vp_vtl();
+		if (ret) {
+			pr_err("%s: Enabling VP VTL1 failed with status 0x%x\n", __func__, ret);
+			/* ToDo: Should we disable VTL1 at partition level in this case */
+			ret = -EINVAL;
+			goto out;
+		}
+		hv_vsm_get_vp_status(&vp_enabled_vtl_set, &active_mbec_enabled);
+		if (!(vp_enabled_vtl_set & HV_VTL1_ENABLE_BIT)) {
+			pr_err("%s: Tried Enabling VP VTL 1 and still failed", __func__);
+			ret = -EINVAL;
+			goto out;
+		}
+	}
+
+	ret = hv_vsm_load_secure_kernel();
+
+	if (ret)
+		goto out;
+
+	/*
+	 * Kick start vtl1 boot on primary cpu. There is currently no way to exit
+	 * gracefully if this boot is not successful. In case of a failure, primary cpu
+	 * will not return from vtl1 and system will hang.
+	 */
+	hv_vsm_boot_vtl1();
+	hv_vsm_boot_success = true;
+
+out:
+	set_cpus_allowed_ptr(current, mask);
+	free_cpumask_var(mask);
+close_files:
+	filp_close(sk, NULL);
+close_file:
+	filp_close(sk_loader, NULL);
+free_mem:
+	vunmap(vsm_skm_va);
+	vsm_skm_pa = 0;
+	return ret;
+}
+#else
+int __init hv_vsm_enable_vtl1(void)
+{
+	return 0;
+}
+#endif
+
+static int __init hv_vsm_boot_init(void)
+{
+	hv_vsm_enable_vtl1();
+
+    return 0;
+}
+
+module_init(hv_vsm_boot_init);
+MODULE_DESCRIPTION("Hyper-V VSM Boot VTL0 Driver");
+MODULE_LICENSE("GPL");
diff --git a/drivers/hv/hv_vsm_boot.h b/drivers/hv/hv_vsm_boot.h
new file mode 100644
index 000000000000..69b304b0babb
--- /dev/null
+++ b/drivers/hv/hv_vsm_boot.h
@@ -0,0 +1,383 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (c) 2023, Microsoft Corporation.
+ *
+ * Author:
+ *   
+ */
+
+#ifndef _HV_VSM_BOOT_H
+#define _HV_VSM_BOOT_H
+
+/* ToDo : Clean this file and move appropriate stuff into vsm.h and hv_vsm_boot.c. Delete this file */
+
+#include <asm-generic/memory_model.h>
+#include <linux/mm.h>
+
+#ifndef u128
+#define u128 __uint128_t
+#endif  // u128
+
+#define VSM_FIRST_CODE_PAGE    0
+#define VSM_ISRS_CODE_PAGE     2816
+#define VSM_GDT_PAGE           4081
+#define VSM_IDT_PAGE           4082
+#define VSM_TSS_PAGE           4083
+#define VSM_PML4E_PAGE         4084
+#define VSM_PDPE_PAGE          4085
+#define VSM_PDE_PAGE           4086
+#define VSM_PTE_0_PAGE         4087
+#define VSM_KERNEL_STACK_PAGE  4095  // 4Kb stack
+
+/*
+ * Initial memory that will be mapped for secure kernel.
+ * Secure Kernel memory can be larger than this.
+ */
+#define SK_INITIAL_MAP_SIZE	(16 * 1024 * 1024)
+
+/* Defines the page size */
+#define VSM_PAGE_SHIFT  12
+
+/* Computed page size */
+#define VSM_PAGE_SIZE  (((uint32_t)1) << VSM_PAGE_SHIFT)
+
+/* Number of entries in a page table (all levels) */
+#define VSM_ENTRIES_PER_PT	512
+
+#define PAGE_AT(addr, idx) ((addr) + (idx) * VSM_PAGE_SIZE)
+
+/* Compute the address of the next page with the given base */
+#define NEXT_PAGE(addr) PAGE_AT((addr), 1)
+
+/* Locations of configuration bits in page table entries (all levels) */
+#define VSM_PAGE_BIT_PRESENT	0
+#define VSM_PAGE_BIT_RW			1
+#define VSM_PAGE_BIT_USER		2
+#define VSM_PAGE_BIT_PWT		3
+#define VSM_PAGE_BIT_PCD		4
+#define VSM_PAGE_BIT_ACCESSED	5
+#define VSM_PAGE_BIT_DIRTY		6
+#define VSM_PAGE_BIT_PAT		7
+#define VSM_PAGE_BIT_GLOBAL		8
+#define VSM_PAGE_BIT_NX			63
+
+/* Shifts to compute page table mapping (See AMD APM Vol 2, 5.3) */
+#define VSM_PD_TABLE_SHIFT      21
+#define VSM_PDP_TABLE_SHIFT     30
+#define VSM_PML4_TABLE_SHIFT    39
+
+/* Computed values for each configuration bit */
+#define VSM_PAGE_PRESENT	(1 << VSM_PAGE_BIT_PRESENT)
+#define VSM_PAGE_RW			(1 << VSM_PAGE_BIT_RW)
+#define VSM_PAGE_USER		(1 << VSM_PAGE_BIT_USER)
+#define VSM_PAGE_PWT		(1 << VSM_PAGE_BIT_PWT)
+#define VSM_PAGE_PCD		(1 << VSM_PAGE_BIT_PCD)
+#define VSM_PAGE_ACCESSED	(1 << VSM_PAGE_BIT_ACCESSED)
+#define VSM_PAGE_DIRTY		(1 << VSM_PAGE_BIT_DIRTY)
+#define VSM_PAGE_PAT		(1 << VSM_PAGE_BIT_PAT)
+#define VSM_PAGE_GLOBAL		(1 << VSM_PAGE_BIT_GLOBAL)
+#define VSM_PAGE_NX		(1 << VSM_PAGE_BIT_NX)
+
+/* Useful combinations of bit configurations */
+#define VSM_PAGE_OPTEE \
+	(VSM_PAGE_PRESENT | VSM_PAGE_RW)
+#define VSM_PAGE_PTE_OPTEE \
+	(VSM_PAGE_OPTEE | VSM_PAGE_ACCESSED | VSM_PAGE_DIRTY)
+
+#define HV_VTL1_ENABLE_BIT	BIT(1)
+#define HV_VTL1			0x1
+
+/* Compute the VA for a given PA for initial VTL1 loading. Assumes identity mapping */
+#define VSM_VA_FROM_PA(pa) (pa)
+
+/* 
+ * Built in phys_to_virt converts kernel logical addresses to PAs.
+ * These convert kernel virtual address to PAs and vice versa.
+ */
+#define VSM_NON_LOGICAL_PHYS_TO_VIRT(pa) ((pa) - vsm_skm_pa + vsm_skm_va)
+#define VSM_NON_LOGICAL_VIRT_TO_PHYS(va) ((va) - vsm_skm_va + vsm_skm_pa)
+
+/* Given VA, get index into the page table at a given level */
+#define VSM_GET_PML4_INDEX_FROM_VA(va) (((va) >> VSM_PML4_TABLE_SHIFT) & 0x1FF)
+#define VSM_GET_PDP_INDEX_FROM_VA(va) (((va) >> VSM_PDP_TABLE_SHIFT) & 0x1FF)
+#define VSM_GET_PD_INDEX_FROM_VA(va) (((va) >> VSM_PD_TABLE_SHIFT) & 0x1FF)
+
+#define CR0_PE				0x00000001		// protection enable
+#define CR0_MP				0x00000002		// math present
+#define CR0_EM				0x00000004		// emulate math coprocessor
+#define CR0_TS				0x00000008		// task switched
+#define CR0_ET				0x00000010		// extension type (80387)
+#define CR0_NE				0x00000020		// numeric error
+#define CR0_WP				0x00010000		// write protect
+#define CR0_AM				0x00040000		// alignment mask
+#define CR0_NW				0x20000000		// not write-through
+#define CR0_CD				0x40000000		// cache disable
+#define CR0_PG				0x80000000		// paging
+
+#define CR4_VME				0x00000001		// V86 mode extensions
+#define CR4_PVI				0x00000002		// Protected mode virtual interrupts
+#define CR4_TSD				0x00000004		// Time stamp disable
+#define CR4_DE				0x00000008		// Debugging Extensions
+#define CR4_PSE				0x00000010		// Page size extensions
+#define CR4_PAE				0x00000020		// Physical address extensions
+#define CR4_MCE				0x00000040		// Machine check enable
+#define CR4_PGE				0x00000080		// Page global enable
+#define CR4_PCE				0x00000100		// Performance monitor counter enable
+#define CR4_FXSR			0x00000200		// FXSR used by OS
+#define CR4_XMMEXCPT		0x00000400		// XMMI used by OS
+#define CR4_UMIP			0x00000800		// User Mode Instruction Prevention (UMIP) enable
+#define CR4_LA57			0x00001000		// 5-level paging enable (57 bit linear address)
+#define CR4_VMXE			0x00002000		// VMX enable
+#define CR4_SMXE			0x00004000		// SMX enable
+#define CR4_RDWRFSGSBASE	0x00010000		// RDWR FSGS Base enable = bit 16
+#define CR4_PCIDE			0x00020000		// PCID enable
+#define CR4_XSAVE			0x00040000		// XSAVE/XRSTOR enable
+#define CR4_SMEP			0x00100000		// SMEP enable
+#define CR4_SMAP			0x00200000		// SMAP enable
+#define CR4_CET				0x00800000		// CET enable
+
+#define MSR_SCE				0x00000001		// system call enable
+#define MSR_LME				0x00000100		// long mode enable
+#define MSR_LMA				0x00000400		// long mode active
+#define MSR_NXE				0x00000800		// no execute enable
+#define MSR_SVME			0x00001000		// secure virtual machine enable
+#define MSR_FFXSR			0x00004000		// fast floating save/restore
+
+#define MSR_PAT				0x277			// page attributes table
+
+/* Intel SDM Vol 3A, 11.12.2 (IA32_PAT MSR) */
+enum {
+	PAT_UC = 0,        /* uncached */
+	PAT_WC = 1,        /* Write combining */
+	PAT_WT = 4,        /* Write Through */
+	PAT_WP = 5,        /* Write Protected */
+	PAT_WB = 6,        /* Write Back (default) */
+	PAT_UC_MINUS = 7,  /* UC, but can be overridden by MTRR */
+};
+
+/* Compute a given PAT entry for a given caching mode name */
+#define VSM_PAT(x, y)	((u64)PAT_ ## y << ((x) * 8))
+
+/* The type field for an interrupt gate */
+#define VSM_GATE_TYPE_INT	((u64)0xE)
+
+/* AMD APM Vol 2, Table 4.6 */
+#define VSM_SYSTEM_SEGMENT_TYPE_LDT				0x2
+#define VSM_SYSTEM_SEGMENT_TYPE_TSS				0x9
+#define VSM_SYSTEM_SEGMENT_TYPE_BUSY_TSS		0xB
+#define VSM_SYSTEM_SEGMENT_TYPE_CALL_GATE		0xC
+#define VSM_SYSTEM_SEGMENT_TYPE_INTERRUPT_GATE	0xE
+#define VSM_SYSTEM_SEGMENT_TYPE_TRAP_GATE		0xF
+
+/* Intel SDM Vol 3A, Table 3-1 */
+#define VSM_CODE_SEGMENT_TYPE_EXECUTE_READ_ACCESSED		0xB
+#define VSM_DATA_SEGMENT_TYPE_READ_WRITE_ACCESSED		0x3
+
+/* Format of a Code Segment (long mode) */
+union vsm_code_seg_desc {
+	u64 as_u64;
+
+	struct {
+		u64 lo : 40;
+		u64 flags_lo : 8;
+		u64 flags_hi : 8;
+		u64 hi : 8;
+	} bytes __packed;
+
+	struct {
+		u64 seglim_lo : 16;
+		u64 addr_lo : 24;
+		u64 a : 1;
+		u64 r : 1;
+		u64 c : 1;
+		u64 mbo1 : 1;
+		u64 mbo2 : 1;
+		u64 dpl : 2;
+		u64 p : 1;
+		u64 seglim_hi : 4;
+		u64 avl : 1;
+		u64 l : 1;
+		u64 d : 1;
+		u64 g : 1;
+		u64 addr_hi : 8;
+	} __packed;
+};
+
+/* Format of a Data Segment (long mode) */
+union vsm_data_seg_desc {
+	u64 as_u64;
+
+	struct {
+		u64 lo : 40;
+		u64 flags_lo : 8;
+		u64 flags_hi : 8;
+		u64 hi : 8;
+	} bytes __packed;
+
+	struct {
+		u64 seglim_lo : 16;
+		u64 addr_lo : 24;
+		u64 a : 1;
+		u64 w : 1;
+		u64 e : 1;
+		u64 mbz1 : 1;
+		u64 mbo1 : 1;
+		u64 dpl : 2;
+		u64 p : 1;
+		u64 seglim_hi : 4;
+		u64 avl : 1;
+		u64 ign1 : 1;
+		u64 db : 1;
+		u64 g : 1;
+		u64 addr_hi : 8;
+	} __packed;
+};
+
+/* Format of a System Segment (long mode) */
+union vsm_sys_seg_desc {
+	struct {
+		u64 lo : 40;
+		u64 flags_lo : 8;
+		u64 flags_hi : 8;
+		u64 mid;
+		u8 hi;
+	} __packed bytes; // Packed attribute must be first.
+
+	struct {
+		u64 seglim_lo : 16;
+		u64 addr_lo : 24;
+		u64 type : 4;
+		u64 mbz1 : 1;
+		u64 dpl : 2;
+		u64 p : 1;
+		u64 seglim_hi : 4;
+		u64 avl : 1;
+		u64 ign1 : 2;
+		u64 g : 1;
+		u64 addr_hi : 40;
+		u32 ign2;
+	} __packed;
+};
+
+/* Format of a Call-Gate Segment (long mode) */
+union vsm_call_gate_seg_desc {
+	struct {
+		u64 toff_lo : 16;
+		u64 tsel : 16;
+		u64 ign1 : 8;
+		u64 type : 4;
+		u64 mbz1 : 1;
+		u64 dpl : 2;
+		u64 p : 1;
+		u64 toff_hi : 48;
+		u64 ign2 : 8;
+		u64 mbz2 : 5;
+		u64 ign3 : 19;
+	} __packed;
+};
+
+/* Format of a Interrupt- and Trap-Gate Segment (long mode) */
+union vsm_int_trap_gate_seg_desc {
+	struct {
+		u64 toff_lo : 16;
+		u64 tsel : 16;
+		u64 ist : 3;
+		u64 ign1 : 5;
+		u64 type : 4;
+		u64 mbz1 : 1;
+		u64 dpl : 2;
+		u64 p : 1;
+		u64 toff_hi : 48;
+		u64 ign2 : 32;
+	} __packed;
+};
+
+/* Format of the Task State Segment (long mode) */
+union vsm_tss {
+	u32 reserved1;
+
+	u64 rsp0;
+	u64 rsp1;
+	u64 rsp2;
+
+	u64 reserved2;
+
+	u64 ist[7];
+
+	u64 reserved3;
+	u8  reserved4;
+
+	u8 io_bitmap_base;
+} __packed;
+
+/* A type for the Global Descriptor Table (GDT) */
+typedef void vsm_gdt_t;
+
+/* Make Code Segment Descriptor */
+#define MAKE_CSD(_seglim_lo, _addr_lo, _a, _r, _c, _dpl, _p, _seglim_hi, _avl, _l, _d, _g, _addr_hi) \
+	(union vsm_code_seg_desc) {		\
+		.seglim_lo = (_seglim_lo),	\
+		.addr_lo = (_addr_lo),		\
+		.a = (_a),					\
+		.r = (_r),					\
+		.c = (_c),					\
+		.mbo1 = 1,					\
+		.mbo2 = 1,					\
+		.dpl = (_dpl),				\
+		.p = (_p),					\
+		.seglim_hi = (_seglim_hi),	\
+		.avl = (_avl),				\
+		.l = (_l),					\
+		.d = (_d),					\
+		.g = (_g)					\
+	}
+
+/* Make Code Segment Descriptor (long mode) */
+#define MAKE_CSD_LM(_c, _dpl, _p, _avl) \
+	MAKE_CSD(0, 0, 0, 0, (_c), (_dpl), (_p), 0, (_avl), 1, 0, 0, 0)
+
+/* Make Data Segment Descriptor */
+#define MAKE_DSD(_seglim_lo, _addr_lo, _a, _w, _e, _dpl, _p, _seglim_hi, _avl, _db, _g, _addr_hi) \
+	(union vsm_data_seg_desc)  {	\
+		.seglim_lo = (_seglim_lo),	\
+		.addr_lo = (_addr_lo),		\
+		.a = (_a),					\
+		.w = (_w),					\
+		.e = (_e),					\
+		.mbz1 = 0,					\
+		.mbo1 = 1,					\
+		.dpl = (_dpl),				\
+		.p = (_p),					\
+		.seglim_hi = (_seglim_hi),	\
+		.avl = (_avl),				\
+		.ign1 = 0,					\
+		.db = (_db),				\
+		.g = (_g),					\
+		.addr_hi = (_addr_hi)		\
+	}
+
+/* Make Data Segment Descriptor (long mode) */
+#define MAKE_DSD_LM(_p, _avl) \
+	MAKE_DSD(0, 0, 0, 0, 0, 0, (_p), 0, (_avl), 0, 0, 0)
+
+/* Make System Segment Descriptor  */
+#define MAKE_SSD(_seglim_lo, _addr_lo, _type, _dpl, _p, _seglim_hi, _avl, _g, _addr_hi) \
+	(union vsm_sys_seg_desc) {		\
+		.seglim_lo = (_seglim_lo),	\
+		.addr_lo = (_addr_lo),		\
+		.type = (_type),			\
+		.mbz1 = 0,					\
+		.dpl = (_dpl),				\
+		.p = (_p),					\
+		.seglim_hi = (_seglim_hi),	\
+		.avl = (_avl),				\
+		.g = (_g),					\
+		.addr_hi = (_addr_hi),		\
+		.ign2 = 0					\
+	};
+
+/* Make a Segment Selector */
+#define MAKE_SELECTOR(rpl, ti, si) \
+	(u16)(((si) << 3) | ((ti) << 2) | (rpl))
+
+#endif /* _HV_VSM_BOOT_H */
diff --git a/include/asm-generic/hyperv-tlfs.h b/include/asm-generic/hyperv-tlfs.h
index fdac4a1714ec..62273ba09113 100644
--- a/include/asm-generic/hyperv-tlfs.h
+++ b/include/asm-generic/hyperv-tlfs.h
@@ -89,6 +89,8 @@
 #define HV_ACCESS_STATS				BIT(8)
 #define HV_DEBUGGING				BIT(11)
 #define HV_CPU_MANAGEMENT			BIT(12)
+#define HV_ACCESS_VSM				BIT(16)
+#define HV_ACCESS_VP_REGS			BIT(17)
 #define HV_ENABLE_EXTENDED_HYPERCALLS		BIT(20)
 #define HV_ISOLATION				BIT(22)
 
@@ -149,6 +151,8 @@ union hv_reference_tsc_msr {
 #define HVCALL_ENABLE_VP_VTL			0x000f
 #define HVCALL_NOTIFY_LONG_SPIN_WAIT		0x0008
 #define HVCALL_SEND_IPI				0x000b
+#define HVCALL_ENABLE_PARTITION_VTL		0x000d
+#define HVCALL_ENABLE_VP_VTL			0x000f
 #define HVCALL_FLUSH_VIRTUAL_ADDRESS_SPACE_EX	0x0013
 #define HVCALL_FLUSH_VIRTUAL_ADDRESS_LIST_EX	0x0014
 #define HVCALL_SEND_IPI_EX			0x0015
-- 
2.42.0



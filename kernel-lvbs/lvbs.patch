From 165f596d6930d55fa7418f59ae91cf481504339a Mon Sep 17 00:00:00 2001
From: Anna Trikalinou <atrikalinou@microsoft.com>
Date: Tue, 20 Dec 2022 22:07:02 -0800
Subject: [PATCH 01/22] Add a new command line argument and ioresource for
 secure kernel memory

Add new securekernel kernel command line argument. Add ioresource entry
to reserve a chunk of memory for secure kernel. Implement the logic to
parse the command line and reserve appropriate sized memory during early
boot.
New command line structure :
	securekernel=<size>K/M/G@<addr> or
	securekernel=<size>K/M/G

Signed-off-by: Anna Trikalinou <atrikalinou@microsoft.com>
Signed-off-by: Thara Gopinath <tgopinath@microsoft.com>
---
 drivers/hv/Kconfig               |   9 ++
 drivers/hv/Makefile              |   1 +
 drivers/hv/hv_vsm_securekernel.c | 186 +++++++++++++++++++++++++++++++
 3 files changed, 196 insertions(+)
 create mode 100644 drivers/hv/hv_vsm_securekernel.c

diff --git a/drivers/hv/Kconfig b/drivers/hv/Kconfig
index b16c7701da19..37f84cc0bf91 100644
--- a/drivers/hv/Kconfig
+++ b/drivers/hv/Kconfig
@@ -57,4 +57,13 @@ config HYPERV_BALLOON
 
 source "drivers/hv/dxgkrnl/Kconfig"
 
+config HYPERV_VSM
+	tristate "Microsoft Hyper-V VSM driver"
+	depends on HYPERV
+	help
+	  Select this option to enable Hyper-V Virtual Secure Mode.
+	  Enabling this option will load a secure kernel in VTL1 and
+	  establish an interface between VTL0 and VTL1 to request for
+	  VSM services.
+
 endmenu
diff --git a/drivers/hv/Makefile b/drivers/hv/Makefile
index aa1cbdb5d0d2..d6811bb9580c 100644
--- a/drivers/hv/Makefile
+++ b/drivers/hv/Makefile
@@ -15,3 +15,4 @@ hv_utils-y := hv_util.o hv_kvp.o hv_snapshot.o hv_fcopy.o hv_utils_transport.o
 
 # Code that must be built-in
 obj-$(subst m,y,$(CONFIG_HYPERV)) += hv_common.o
+obj-$(subst m,y,$(CONFIG_HYPERV_VSM)) += hv_vsm_securekernel.o
diff --git a/drivers/hv/hv_vsm_securekernel.c b/drivers/hv/hv_vsm_securekernel.c
new file mode 100644
index 000000000000..d1ff1842adf3
--- /dev/null
+++ b/drivers/hv/hv_vsm_securekernel.c
@@ -0,0 +1,186 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (c) 2024, Microsoft Corporation.
+ *
+ * Author:
+ *
+ */
+
+#include <linux/memblock.h>
+
+/* Define Memory Reservation for Secure Kernel */
+#define SECKERNEL_ALIGN			SZ_2M
+#define SECKERNEL_ADDR_MAX		(max_low_pfn_mapped << PAGE_SHIFT)
+#define SECKERNEL_BASE_SIZE		(16 * 1024 * 1024)
+#define SECKERNEL_PERCPU_SIZE		(2 * 1024 * 1024)
+
+struct resource sk_res = {
+	.name  = "vsm",
+	.start = 0,
+	.end   = 0,
+	.flags = IORESOURCE_BUSY | IORESOURCE_SYSTEM_RAM,
+	.desc  = IORES_DESC_RESERVED
+};
+
+/*
+ * That function parses "simple" securekernel command lines like
+ *
+ *	securekernel=size[@offset]
+ *
+ * It returns 0 on success and -EINVAL on failure.
+ */
+static int __init parse_securekernel_simple(char *cmdline,
+					    unsigned long long *securekernel_size,
+					    unsigned long long *securekernel_base)
+{
+	char *cur = cmdline;
+
+	*securekernel_size = memparse(cmdline, &cur);
+	if (cmdline == cur) {
+		pr_warn("securekernel: memory value expected\n");
+		return -EINVAL;
+	}
+
+	if (*cur == '@') {
+		*securekernel_base = memparse(cur + 1, &cur);
+	} else if (*cur != ' ' && *cur != '\0') {
+		pr_warn("securekernel: unrecognized char: %c\n", *cur);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static __init char *get_last_securekernel(char *cmdline, const char *name)
+{
+	char *p = cmdline, *sk_cmdline = NULL;
+
+	/* find securekernel and use the last one if there are more */
+	p = strstr(p, name);
+	while (p) {
+		sk_cmdline = p;
+		p = strstr(p + 1, name);
+	}
+
+	if (!sk_cmdline)
+		return NULL;
+
+	return sk_cmdline;
+}
+
+static int __init __parse_securekernel(char *cmdline,
+				       unsigned long long system_ram,
+				       unsigned long long *securekernel_size,
+				       unsigned long long *securekernel_base,
+				       const char *name)
+{
+	char *sk_cmdline;
+
+	if (!securekernel_size || !securekernel_base)
+		return -EINVAL;
+
+	*securekernel_size = 0;
+	*securekernel_base = 0;
+
+	sk_cmdline = get_last_securekernel(cmdline, name);
+
+	if (!sk_cmdline)
+		return -EINVAL;
+
+	sk_cmdline += strlen(name);
+
+	return parse_securekernel_simple(sk_cmdline, securekernel_size, securekernel_base);
+}
+
+/*
+ * That function is the entry point for command line parsing and should be
+ * called from the arch-specific code.
+ */
+int __init parse_securekernel(char *cmdline,
+			      unsigned long long system_ram,
+			      unsigned long long *securekernel_size,
+			      unsigned long long *securekernel_base)
+{
+	return __parse_securekernel(cmdline, system_ram, securekernel_size, securekernel_base,
+					"securekernel=");
+}
+
+/*
+ * Add a dummy early_param handler to mark securekernel= as a known command line
+ * parameter and suppress incorrect warnings in init/main.c.
+ */
+static int __init parse_securekernel_dummy(char *arg)
+{
+	return 0;
+}
+early_param("securekernel", parse_securekernel_dummy);
+
+static __init int hv_vsm_seckernel_mem_init(void)
+{
+	unsigned long long securekernel_size = 0, securekernel_base = 0;
+	int ret;
+
+	/*
+	 * Reserve Secure Kernel memory.
+	 * Check command line first, if secure kernel memory was defined
+	 */
+	ret = parse_securekernel(boot_command_line, SECKERNEL_ADDR_MAX, &securekernel_size,
+				 &securekernel_base);
+
+	if (ret != 0 || securekernel_size < SECKERNEL_BASE_SIZE) {
+		pr_info("%s: securekernel cmd line not defined. Falling back to default.\n",
+			__func__);
+
+		/* Estimate amount of memory needed for Secure Kernel */
+		securekernel_size = SECKERNEL_BASE_SIZE +
+					num_possible_cpus() * SECKERNEL_PERCPU_SIZE;
+		securekernel_base = 0;
+	}
+
+	/* If securekernel_base was specified from command line,
+	 * try to reserve memory starting from that address
+	 */
+	if (securekernel_base) {
+		unsigned long long start, end;
+
+		end = securekernel_base + securekernel_size;
+		if (end >  SECKERNEL_ADDR_MAX || end < securekernel_base) {
+			pr_warn("%s: Invalid Securekernel base address %llx. Falling back to default.\n",
+				__func__, securekernel_base);
+			securekernel_base = 0;
+		} else {
+			start = memblock_phys_alloc_range(securekernel_size, SECKERNEL_ALIGN,
+							  securekernel_base,
+							  securekernel_base + securekernel_size);
+			if (start != securekernel_base) {
+				pr_warn("%s: memory reservation @ %llx failed-memory is in use\n",
+					__func__, securekernel_base);
+				pr_warn("%s:Falling back to default mem allocation\n", __func__);
+				securekernel_base = 0;
+			}
+		}
+	}
+	/* Default: Find the base address automatically */
+	if (!securekernel_base) {
+		securekernel_base = memblock_phys_alloc_range(securekernel_size, SECKERNEL_ALIGN,
+							      0, SECKERNEL_ADDR_MAX);
+		if (!securekernel_base) {
+			pr_err("%s: Securekernel reservation failed-VSM will not be enabled.\n",
+			       __func__);
+			return -EINVAL;
+		}
+	}
+
+	pr_info("Reserving %ldMB of memory at 0x%llx(%ld MB) for securekernel(System RAM:%ldMB)\n",
+		(unsigned long)(securekernel_size >> 20),
+		securekernel_base,
+		(unsigned long)(securekernel_base >> 20),
+		(unsigned long)(memblock_phys_mem_size() >> 20));
+
+	sk_res.start = securekernel_base;
+	sk_res.end   = securekernel_base + securekernel_size - 1;
+	insert_resource(&iomem_resource, &sk_res);
+
+	return 0;
+}
+early_initcall(hv_vsm_seckernel_mem_init);
-- 
2.42.0


From dc247c29d1fb897c9393435e59d2a5f919a47526 Mon Sep 17 00:00:00 2001
From: Thara Gopinath <tgopinath@microsoft.com>
Date: Thu, 23 Feb 2023 13:28:15 +0000
Subject: [PATCH 02/22] Enable VSM awareness in efi os indications variable

Without this we cannot enable VTL1 in linux kernel.

Signed-off-by: Thara Gopinath <tgopinath@microsoft.com>
---
 drivers/firmware/efi/libstub/x86-stub.c | 28 +++++++++++++++++++++++++
 1 file changed, 28 insertions(+)

diff --git a/drivers/firmware/efi/libstub/x86-stub.c b/drivers/firmware/efi/libstub/x86-stub.c
index e4ae3db727ef..23ef5f4d492d 100644
--- a/drivers/firmware/efi/libstub/x86-stub.c
+++ b/drivers/firmware/efi/libstub/x86-stub.c
@@ -22,6 +22,10 @@
 #include "x86-stub.h"
 
 extern char _bss[], _ebss[];
+#define HYPERV_PRIVATE_EFI_NAMESPACE_GUID \
+       EFI_GUID(0x610b9e98, 0xc6f6, 0x47f8, 0x8b, 0x47, 0x2d, 0x2d, 0xa0, 0xd5, 0x2a, 0x91)
+
+static const efi_char16_t efi_HvPrivOsloaderIndications_name[] = L"OsLoaderIndications";
 
 const efi_system_table_t *efi_system_table;
 const efi_dxe_services_table_t *efi_dxe_table;
@@ -718,6 +722,27 @@ static efi_status_t exit_boot_func(struct efi_boot_memmap *map,
 	return EFI_SUCCESS;
 }
 
+static void efi_set_hv_os_indications(void)
+{
+	efi_guid_t guid = HYPERV_PRIVATE_EFI_NAMESPACE_GUID;
+	efi_status_t status;
+	unsigned long size;
+	u32 attr, val;
+
+	size = sizeof(val);
+	status = get_efi_var(efi_HvPrivOsloaderIndications_name, &guid, &attr, &size, &val);
+        if (status != EFI_SUCCESS) {
+		efi_err("Could not read Hyper-V OsLoaderIndications\n");
+		return;
+	}
+	val |= 1;
+	set_efi_var(efi_HvPrivOsloaderIndications_name, &guid, attr, size, &val);
+        if (status != EFI_SUCCESS) {
+		efi_err("Could not set Hyper-V OsLoaderIndications to indicate VSM support \n");
+		return;
+	}
+}
+
 static efi_status_t exit_boot(struct boot_params *boot_params, void *handle)
 {
 	struct setup_data *e820ext = NULL;
@@ -732,6 +757,9 @@ static efi_status_t exit_boot(struct boot_params *boot_params, void *handle)
 	if (status != EFI_SUCCESS)
 		return status;
 
+	/* Indicate to bootloader that we will be enabling VTL1 before exiting boot services */
+	efi_set_hv_os_indications();
+
 	/* Might as well exit boot services now */
 	status = efi_exit_boot_services(handle, &priv, exit_boot_func);
 	if (status != EFI_SUCCESS)
-- 
2.42.0


From 722674ea461352fa813ff68c0b1476b262271cf6 Mon Sep 17 00:00:00 2001
From: Thara Gopinath <tgopinath@microsoft.com>
Date: Mon, 24 Jun 2024 07:05:06 -0400
Subject: [PATCH 03/22] Enable VTL1 and boot primary cpu in VTL1

Bring up VTL1 partition. Load the secure loader and secure kernel and
allow primary/boot cpu to boot in VTL1.

Co-developed-by: Angelina Vu <angelinavu@microsoft.com>
Signed-off-by: Angelina Vu <angelinavu@microsoft.com>
Signed-off-by: Thara Gopinath <tgopinath@microsoft.com>
---
 arch/x86/include/asm/hyperv-tlfs.h | 101 ++++
 drivers/hv/Kconfig                 |   5 +
 drivers/hv/Makefile                |   2 +-
 drivers/hv/hv_common.c             |  10 +-
 drivers/hv/hv_vsm.h                |  33 +
 drivers/hv/hv_vsm_boot.c           | 935 +++++++++++++++++++++++++++++
 drivers/hv/hv_vsm_boot.h           | 383 ++++++++++++
 include/asm-generic/hyperv-tlfs.h  |   4 +
 8 files changed, 1466 insertions(+), 7 deletions(-)
 create mode 100644 drivers/hv/hv_vsm.h
 create mode 100644 drivers/hv/hv_vsm_boot.c
 create mode 100644 drivers/hv/hv_vsm_boot.h

diff --git a/arch/x86/include/asm/hyperv-tlfs.h b/arch/x86/include/asm/hyperv-tlfs.h
index 2ff26f53cd62..28d110087227 100644
--- a/arch/x86/include/asm/hyperv-tlfs.h
+++ b/arch/x86/include/asm/hyperv-tlfs.h
@@ -799,6 +799,107 @@ struct hv_get_vp_from_apic_id_in {
 	u32 apic_ids[];
 } __packed;
 
+/* Types for the EnablePartitionVtl hypercall */
+union hv_enable_partition_vtl_flags {
+	u8 as_u8;
+
+	struct {
+		u8 enable_mbec : 1;
+		u8 reserved : 7;
+	};
+};
+
+struct hv_input_enable_partition_vtl {
+	u64 partition_id;
+	u8 target_vtl;
+	union hv_enable_partition_vtl_flags flags;
+	u16 reserved16_z;
+	u32 reserved32_z;
+} __packed;
+
+enum hv_register_name {
+	HvRegisterVsmCodePageOffsets = 0x000d0002,
+	HvRegisterVsmVpStatus = 0x000d0003,
+	HvRegisterVsmPartitionStatus = 0x000d0004,
+	HvRegisterVsmCapabilities = 0x000d0006,
+};
+
+union hv_register_value {
+	u64 as_u64;
+	u32 as_u32;
+	u16 as_u16;
+	u8 as_u8;
+};
+
+union hv_register_vsm_partition_status {
+	u64 as_u64;
+
+	struct {
+		u64 enabled_vtl_set : 16;
+		u64 max_vtl : 4;
+		u64 mbec_enabled_vtl_set: 16;
+		u64 reserved_z : 28;
+	};
+};
+
+struct hv_input_get_vp_registers {
+	u64 partition_id;
+	u32 vp_index;
+	union hv_input_vtl input_vtl;
+	u8 reserved8_z;
+	u16 reserved16_z;
+	__aligned(8) enum hv_register_name names[1];
+};
+
+union hv_register_vsm_vp_status {
+	u64 as_u64;
+
+	struct {
+		u64 active_vtl : 4;
+		u64 active_mbec_enabled : 1;
+		u64 reserved_z0 : 11;
+		u64 enabled_vtl_set : 16;
+		u64 reserved_z1 : 32;
+	};
+};
+
+/* Types for the EnableVpVtl hypercall */
+struct hv_initial_vp_context {
+	u64 rip;
+	u64 rsp;
+	u64 rflags;
+
+	/* Segment selector registers together with their hidden state */
+	struct hv_x64_segment_register cs;
+	struct hv_x64_segment_register ds;
+	struct hv_x64_segment_register es;
+	struct hv_x64_segment_register fs;
+	struct hv_x64_segment_register gs;
+	struct hv_x64_segment_register ss;
+	struct hv_x64_segment_register tr;
+	struct hv_x64_segment_register ldtr;
+
+	/* Global and Interrupt Descriptor tables */
+	struct hv_x64_table_register idtr;
+	struct hv_x64_table_register gdtr;
+
+	/* Control registers and MSRs */
+	u64 efer;
+	u64 cr0;
+	u64 cr3;
+	u64 cr4;
+	u64 msr_cr_pat;
+};
+
+struct hv_input_enable_vp_vtl {
+	u64 partition_id;
+	u32 vp_index;
+	u8 target_vtl;
+	u8 reserved_z0;
+	u16 reserved_z1;
+	struct hv_initial_vp_context vp_vtl_context;
+};
+
 #include <asm-generic/hyperv-tlfs.h>
 
 #endif
diff --git a/drivers/hv/Kconfig b/drivers/hv/Kconfig
index 37f84cc0bf91..650be40fb55b 100644
--- a/drivers/hv/Kconfig
+++ b/drivers/hv/Kconfig
@@ -66,4 +66,9 @@ config HYPERV_VSM
 	  establish an interface between VTL0 and VTL1 to request for
 	  VSM services.
 
+config HYPERV_VSM_DEBUG
+	bool
+	depends on HYPERV_VSM
+	default n
+
 endmenu
diff --git a/drivers/hv/Makefile b/drivers/hv/Makefile
index d6811bb9580c..44efca6b2993 100644
--- a/drivers/hv/Makefile
+++ b/drivers/hv/Makefile
@@ -15,4 +15,4 @@ hv_utils-y := hv_util.o hv_kvp.o hv_snapshot.o hv_fcopy.o hv_utils_transport.o
 
 # Code that must be built-in
 obj-$(subst m,y,$(CONFIG_HYPERV)) += hv_common.o
-obj-$(subst m,y,$(CONFIG_HYPERV_VSM)) += hv_vsm_securekernel.o
+obj-$(subst m,y,$(CONFIG_HYPERV_VSM)) += hv_vsm_securekernel.o hv_vsm_boot.o
diff --git a/drivers/hv/hv_common.c b/drivers/hv/hv_common.c
index ccad7bca3fd3..9db40de8cf9d 100644
--- a/drivers/hv/hv_common.c
+++ b/drivers/hv/hv_common.c
@@ -329,11 +329,8 @@ int __init hv_common_init(void)
 	hyperv_pcpu_input_arg = alloc_percpu(void  *);
 	BUG_ON(!hyperv_pcpu_input_arg);
 
-	/* Allocate the per-CPU state for output arg for root */
-	if (hv_root_partition) {
-		hyperv_pcpu_output_arg = alloc_percpu(void *);
-		BUG_ON(!hyperv_pcpu_output_arg);
-	}
+	hyperv_pcpu_output_arg = alloc_percpu(void *);
+	BUG_ON(!hyperv_pcpu_output_arg);
 
 	hv_vp_index = kmalloc_array(num_possible_cpus(), sizeof(*hv_vp_index),
 				    GFP_KERNEL);
@@ -373,11 +370,12 @@ int hv_common_cpu_init(unsigned int cpu)
 	 * allocated if this CPU was previously online and then taken offline
 	 */
 	if (!*inputarg) {
+		pgcount = 2;
 		mem = kmalloc(pgcount * HV_HYP_PAGE_SIZE, flags);
 		if (!mem)
 			return -ENOMEM;
 
-		if (hv_root_partition) {
+		if (pgcount > 1) {
 			outputarg = (void **)this_cpu_ptr(hyperv_pcpu_output_arg);
 			*outputarg = (char *)mem + HV_HYP_PAGE_SIZE;
 		}
diff --git a/drivers/hv/hv_vsm.h b/drivers/hv/hv_vsm.h
new file mode 100644
index 000000000000..5c14034fb482
--- /dev/null
+++ b/drivers/hv/hv_vsm.h
@@ -0,0 +1,33 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (c) 2023, Microsoft Corporation.
+ *
+ * Author:
+ *
+ */
+
+#ifndef _HV_VSM_H
+#define _HV_VSM_H
+
+extern bool hv_vsm_boot_success;
+extern bool hv_vsm_mbec_enabled;
+extern union hv_register_vsm_code_page_offsets vsm_code_page_offsets;
+
+struct vtlcall_param {
+	u64	a0;
+	u64	a1;
+	u64	a2;
+	u64	a3;
+} __packed;
+
+union hv_register_vsm_code_page_offsets {
+	u64 as_uint64;
+
+	struct {
+		u64 vtl_call_offset : 12;
+		u64 vtl_return_offset : 12;
+		u64 reserved_z : 40;
+	};
+} __packed;
+
+#endif /* _HV_VSM_H */
diff --git a/drivers/hv/hv_vsm_boot.c b/drivers/hv/hv_vsm_boot.c
new file mode 100644
index 000000000000..99a383d86a0a
--- /dev/null
+++ b/drivers/hv/hv_vsm_boot.c
@@ -0,0 +1,935 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * VSM boot framework that enables VTL1, loads secure kernel
+ * and boots VTL1.
+ *
+ * Copyright (c) 2023, Microsoft Corporation.
+ *
+ */
+
+#include <asm/hyperv-tlfs.h>
+#include <asm/mshyperv.h>
+#include <asm/e820/api.h>
+#include <linux/hyperv.h>
+#include <linux/module.h>
+#include <linux/file.h>
+#include <linux/fs.h>
+#include <linux/cpumask.h>
+
+#include "hv_vsm_boot.h"
+#include "hv_vsm.h"
+
+extern struct resource sk_res;
+static struct file *sk_loader, *sk;
+static phys_addr_t vsm_skm_pa;
+static void *vsm_skm_va;
+union hv_register_vsm_code_page_offsets vsm_code_page_offsets;
+
+bool hv_vsm_boot_success = false;
+bool hv_vsm_mbec_enabled = true;
+
+#ifdef CONFIG_HYPERV_VSM
+
+static int vsm_arch_has_vsm_access(void)
+{
+	if (!(ms_hyperv.features & HV_MSR_SYNIC_AVAILABLE))
+		return false;
+	if (!(ms_hyperv.priv_high & HV_ACCESS_VSM))
+		return false;
+	if (!(ms_hyperv.priv_high & HV_ACCESS_VP_REGS))
+		return false;
+	return true;
+}
+
+static int hv_vsm_get_code_page_offsets(void)
+{
+	u64 status;
+	unsigned long flags;
+	struct hv_input_get_vp_registers *hvin = NULL;
+	union hv_register_value *hvout = NULL;
+
+	hvin = *this_cpu_ptr(hyperv_pcpu_input_arg);
+	hvout = *this_cpu_ptr(hyperv_pcpu_output_arg);
+
+	hvin->partition_id = HV_PARTITION_ID_SELF;
+	hvin->vp_index = HV_VP_INDEX_SELF;
+	hvin->input_vtl.as_uint8 = 0;
+	hvin->reserved8_z = 0;
+	hvin->reserved16_z = 0;
+	hvin->names[0] = HvRegisterVsmCodePageOffsets;
+
+	local_irq_save(flags);
+	status = hv_do_rep_hypercall(HVCALL_GET_VP_REGISTERS, 1, 0, hvin, hvout);
+	local_irq_restore(flags);
+
+	if (hv_result_success(status)) {
+		vsm_code_page_offsets.as_uint64 = hvout->as_u64;
+		return 0;
+	} else {
+		return -EFAULT;
+	}
+}
+
+static int hv_vsm_get_partition_status(u16 *enabled_vtl_set, u8 *max_vtl, u16 *mbec_enabled_vtl_set)
+{
+	u64 status;
+	unsigned long flags;
+	struct hv_input_get_vp_registers *hvin = NULL;
+	union hv_register_value *hvout = NULL;
+	union hv_register_vsm_partition_status vsm_partition_status = { 0 };
+
+	local_irq_save(flags);
+
+	hvin = *this_cpu_ptr(hyperv_pcpu_input_arg);
+	hvout = *this_cpu_ptr(hyperv_pcpu_output_arg);
+
+	hvin->partition_id = HV_PARTITION_ID_SELF;
+	hvin->vp_index = HV_VP_INDEX_SELF;
+	hvin->input_vtl.as_uint8 = 0;
+	hvin->reserved8_z = 0;
+	hvin->reserved16_z = 0;
+	hvin->names[0] = HvRegisterVsmPartitionStatus;
+
+	status = hv_do_rep_hypercall(HVCALL_GET_VP_REGISTERS, 1, 0, hvin, hvout);
+	local_irq_restore(flags);
+
+	if (!hv_result_success(status))
+		return -EFAULT;
+
+	vsm_partition_status = (union hv_register_vsm_partition_status)hvout->as_u64;
+	*enabled_vtl_set = vsm_partition_status.enabled_vtl_set;
+	*max_vtl = vsm_partition_status.max_vtl;
+	*mbec_enabled_vtl_set = vsm_partition_status.mbec_enabled_vtl_set;
+	return 0;
+}
+
+static int hv_vsm_get_vp_status(u16 *enabled_vtl_set, u8 *active_mbec_enabled)
+{
+	u64 status;
+	unsigned long flags;
+	struct hv_input_get_vp_registers *hvin = NULL;
+	union hv_register_value *hvout = NULL;
+	union hv_register_vsm_vp_status vsm_vp_status = { 0 };
+
+	hvin = *this_cpu_ptr(hyperv_pcpu_input_arg);
+	hvout = *this_cpu_ptr(hyperv_pcpu_output_arg);
+
+	hvin->partition_id = HV_PARTITION_ID_SELF;
+	hvin->vp_index = HV_VP_INDEX_SELF;
+	hvin->input_vtl.as_uint8 = 0;
+	hvin->reserved8_z = 0;
+	hvin->reserved16_z = 0;
+	hvin->names[0] = HvRegisterVsmVpStatus;
+
+	local_irq_save(flags);
+	status = hv_do_rep_hypercall(HVCALL_GET_VP_REGISTERS, 1, 0, hvin, hvout);
+	local_irq_restore(flags);
+
+	if (!hv_result_success(status))
+		return -EFAULT;
+
+	vsm_vp_status = (union hv_register_vsm_vp_status)hvout->as_u64;
+	*enabled_vtl_set = vsm_vp_status.enabled_vtl_set;
+	*active_mbec_enabled = vsm_vp_status.active_mbec_enabled;
+
+	return 0;
+}
+
+#ifdef CONFIG_HYPERV_VSM_DEBUG
+/* Walks page tables, starting at the PML4 (top level). */
+static void hv_vsm_dump_pt(u64 root, int lvl)
+{
+	int n;
+
+	u64 entry_pa;
+	u64 *entry_va;
+	u64 next_pa;
+
+	if (lvl == 5)
+		return;
+
+	for (n = 0; n < VSM_ENTRIES_PER_PT; n++) {
+		entry_pa = root + (n * sizeof(u64));
+		entry_va = VSM_NON_LOGICAL_PHYS_TO_VIRT(entry_pa);
+
+		if (*entry_va)
+			pr_info("\t\t Entry: %i/%i - 0x%llx\n", n, lvl,
+				*entry_va);
+
+		if (*entry_va & VSM_PAGE_PRESENT) {
+			next_pa = *entry_va & VSM_PAGE_BASE_ADDR_MASK;
+			hv_vsm_dump_pt(next_pa, lvl + 1);
+		}
+	}
+}
+
+static void hv_vsm_dump_secure_kernel_memory(void)
+{
+	pr_info("%s: Dumping Secure Kernel Memory\n", __func__);
+	print_hex_dump(KERN_INFO, "\t", DUMP_PREFIX_ADDRESS, 32, 4,
+		(void *)vsm_skm_va, 1024, 0);
+}
+#endif
+
+static __init void __hv_vsm_init_vtlcall(struct vtlcall_param *args)
+{
+	u64 hcall_addr;
+
+	hcall_addr = (u64)((u8 *)hv_hypercall_pg + vsm_code_page_offsets.vtl_call_offset);
+	register u64 hypercall_addr asm("rax") = hcall_addr;
+
+	asm __volatile__ (	\
+	/* Keep copies of all the registers */
+		"pushq	%%rdi\n"
+		"pushq	%%rsi\n"
+		"pushq	%%rdx\n"
+		"pushq	%%rbx\n"
+		"pushq	%%rcx\n"
+		"pushq	%%rax\n"
+		"pushq	%%r8\n"
+		"pushq	%%r9\n"
+		"pushq	%%r10\n"
+		"pushq	%%r11\n"
+		"pushq	%%r12\n"
+		"pushq	%%r13\n"
+		"pushq	%%r14\n"
+		"pushq	%%r15\n"
+	/*
+	 * The vtlcall_param structure is in rdi, which is modified below, so copy it into a
+	 * register that stays constant in the instructon block immediately following.
+	 */
+		"movq	%1, %%r12\n"
+
+	/* Copy values from vtlcall_param structure into registers used to communicate with VTL1 */
+		"movq	0x00(%%r12), %%rdi\n"
+		"movq	0x08(%%r12), %%rsi\n"
+		"movq	0x10(%%r12), %%rdx\n"
+		"movq	0x18(%%r12), %%r8\n"
+	/* Make rcx 0 */
+		"xorl	%%ecx, %%ecx\n"
+	/* VTL call */
+		CALL_NOSPEC
+	/* Restore r12 with vtlcall_param after VTL call */
+		"movq	104(%%rsp), %%r12\n"
+
+	/* Copy values from registers used to communicate with VTL1 into vtlcall_param structure */
+		"movq	%%rdi,  0x00(%%r12)\n"
+		"movq	%%rsi,  0x08(%%r12)\n"
+		"movq	%%rdx,  0x10(%%r12)\n"
+		"movq	%%r8,  0x18(%%r12)\n"
+
+	/* Restore all modified registers */
+		"popq	%%r15\n"
+		"popq	%%r14\n"
+		"popq	%%r13\n"
+		"popq	%%r12\n"
+		"popq	%%r11\n"
+		"popq	%%r10\n"
+		"popq	%%r9\n"
+		"popq	%%r8\n"
+		"popq	%%rax\n"
+		"popq	%%rcx\n"
+		"popq	%%rbx\n"
+		"popq	%%rdx\n"
+		"popq	%%rsi\n"
+		"popq	%%rdi\n"
+		: ASM_CALL_CONSTRAINT
+		: "D"(args), THUNK_TARGET(hypercall_addr)
+		: "cc", "memory");
+}
+
+__init int hv_vsm_init_vtlcall(struct vtlcall_param *args)
+{
+	unsigned long flags = 0;
+	u64 cr2;
+
+	local_irq_save(flags);
+	kernel_fpu_begin_mask(0);
+	cr2 = native_read_cr2();
+	__hv_vsm_init_vtlcall(args);
+	native_write_cr2(cr2);
+	kernel_fpu_end();
+	local_irq_restore(flags);
+
+	return (int)args->a3;
+}
+
+static __init void hv_vsm_boot_vtl1(void)
+{
+	struct vtlcall_param args = {0};
+	u16 vp_enabled_vtl_set = 0;
+	u8 active_mbec_enabled = 0;
+
+	args.a0 = num_possible_cpus();
+	args.a1 = sk_res.start;
+	args.a2 = sk_res.end + 1;
+	args.a3 = max_pfn << PAGE_SHIFT;
+
+	hv_vsm_init_vtlcall(&args);
+
+	hv_vsm_get_vp_status(&vp_enabled_vtl_set, &active_mbec_enabled);
+	if (!active_mbec_enabled) {
+		pr_err("Failed to enable MBEC for VP0\n");
+		hv_vsm_mbec_enabled = false;
+	}
+}
+
+static int __init hv_vsm_enable_partition_vtl(void)
+{
+	u64 status = 0;
+	unsigned long flags;
+	struct hv_input_enable_partition_vtl *hvin = NULL;
+
+	local_irq_save(flags);
+
+	hvin = *this_cpu_ptr(hyperv_pcpu_input_arg);
+	hvin->partition_id = HV_PARTITION_ID_SELF;
+	hvin->target_vtl = 1;
+	hvin->flags.enable_mbec = 1;
+	hvin->flags.reserved = 0;
+	hvin->reserved16_z = 0;
+	hvin->reserved32_z = 0;
+
+	status = hv_do_hypercall(HVCALL_ENABLE_PARTITION_VTL, hvin, NULL);
+
+	if (status)
+		pr_err("%s: Enable Partition VTL failed. status=0x%x\n", __func__, hv_result(status));
+
+	local_irq_restore(flags);
+
+	return hv_result(status);
+}
+
+static int __init hv_vsm_reserve_sk_mem(void)
+{
+	void *va_start;
+	struct page **page, **pages;
+	unsigned long long paddr, sk_size;
+	unsigned long size;
+	int i, npages;
+
+	if (!sk_res.start) {
+		pr_err("%s: No memory reserved in cmdline for secure kernel", __func__);
+		return -ENOMEM;
+	}
+	vsm_skm_pa = sk_res.start;
+	vsm_skm_va = 0;
+
+	/* Allocate an array of struct page pointers  */
+	sk_size = sk_res.end - sk_res.start + 1;
+	npages = sk_size >> PAGE_SHIFT;
+	size = sizeof(struct page *) * npages;
+	pages = vmalloc(size);
+
+	if (!pages) {
+		pr_err("%s: Allocating array of struct page pointers failed (Size: %lu)\n",
+				__func__, size);
+		return -ENOMEM;
+	}
+
+	/* Convert each page frame number to struct page.
+	   Memory was allocated using memblock_phys_alloc_range() during boot */
+	page = pages;
+	paddr = vsm_skm_pa;
+	for (i = 0; i < npages; i++) {
+		*page++ = pfn_to_page(paddr >> PAGE_SHIFT);
+		paddr += PAGE_SIZE;
+	}
+
+	/* Map Secure Kernel physical memory into kernel virtual address space */
+	va_start = vmap(pages, npages, VM_MAP, PAGE_KERNEL);
+
+	if (!va_start) {
+		pr_err("%s: Memory mapping failed\n", __func__);
+		vfree(pages);
+		return -ENOMEM; 
+	}
+
+	vsm_skm_va = va_start;
+
+	pr_info("%s: secure kernel PA=0x%lx, VA=0x%lx\n",
+			__func__, (unsigned long)vsm_skm_pa, (unsigned long)vsm_skm_va);
+
+	memset(vsm_skm_va, 0, sk_size);
+	vfree(pages);
+	return 0;
+}
+
+static void __init hv_vsm_init_cpu(struct hv_initial_vp_context *vp_ctx)
+{
+	/* Offset rip by any secure kernel header length */
+	vp_ctx->rip = (u64)VSM_VA_FROM_PA(PAGE_AT(vsm_skm_pa,
+		VSM_FIRST_CODE_PAGE));
+
+	vp_ctx->cr0 =
+		CR0_PG |	// Paging
+		CR0_WP |	// Write Protect
+		CR0_NE |	// Numeric Error
+		CR0_ET |	// Extension Type
+		CR0_MP |	// Math Present
+		CR0_PE;		// Protection Enable
+
+	vp_ctx->cr4 =
+		CR4_PSE	|	// Page Size Extensions
+		CR4_PGE	|	// Page Global Enable
+		CR4_PAE;	// Physical Address Extensions
+
+	vp_ctx->efer =
+		MSR_LMA |	// Long Mode Active
+		MSR_LME |	// Long Mode Enable
+		MSR_NXE |	// No Execute Enable
+		MSR_SCE;	// System Call Enable
+
+	/*
+	 * Intel CPUs fail if the architectural read-as-one bit 1 of RFLAGS is not
+	 * set. See Intel SDM Vol 3C, 26.3.1.4 (RFLAGS).
+	 *
+	 * TODO: Has Hyper-V implemented setting this automatically?
+	 */
+	vp_ctx->rflags = 0b10;
+
+	vp_ctx->msr_cr_pat =
+		VSM_PAT(0, WB)       | VSM_PAT(1, WT) |
+		VSM_PAT(2, UC_MINUS) | VSM_PAT(3, UC) |
+		VSM_PAT(4, WB)       | VSM_PAT(5, WT) |
+		VSM_PAT(6, UC_MINUS) | VSM_PAT(7, UC);
+
+#ifdef CONFIG_HYPERV_VSM_DEBUG
+	pr_info("%s : Printing Initial VP Registers..\n", __func__);
+	pr_info("\t\t RIP: 0x%llx\n", vp_ctx->rip);
+	pr_info("\t\t CR0: 0x%llx\n", vp_ctx->cr0);
+	pr_info("\t\t CR4: 0x%llx\n", vp_ctx->cr4);
+	pr_info("\t\t EFER: 0x%llx\n", vp_ctx->efer);
+	pr_info("\t\t RFLAGS: 0x%llx\n", vp_ctx->rflags);
+	pr_info("\t\t CR PAT: 0x%llx\n", vp_ctx->msr_cr_pat);
+#endif
+}
+
+static void __init hv_vsm_init_gdt(struct hv_initial_vp_context *vp_ctx)
+{
+	phys_addr_t gdt_pa;
+	phys_addr_t tss_pa;
+	phys_addr_t kstack_pa;
+
+	union vsm_code_seg_desc cseg;
+	union vsm_data_seg_desc dseg;
+	union vsm_sys_seg_desc sseg;
+	union vsm_tss *tss;
+
+	u64 tss_sk_va;
+
+	size_t cseg_sz = sizeof(cseg);
+	size_t dseg_sz = sizeof(dseg);
+	size_t sseg_sz = sizeof(sseg);
+	size_t tss_sz = sizeof(*tss);
+
+	size_t gdt_offset = 0;
+
+	/* Get a page for the GDT */
+	gdt_pa = PAGE_AT(vsm_skm_pa, VSM_GDT_PAGE);
+	/* Get a page for the TSS */
+	tss_pa = PAGE_AT(vsm_skm_pa, VSM_TSS_PAGE);
+	tss = VSM_NON_LOGICAL_PHYS_TO_VIRT(tss_pa);
+	/* Get a page for the secure kernel initial stack */
+	kstack_pa = PAGE_AT(vsm_skm_pa, VSM_KERNEL_STACK_PAGE);
+
+	/* Make and add the NULL descriptor to the GDT */
+	cseg = MAKE_CSD_LM(0, 0, 0, 0);
+	memcpy(VSM_NON_LOGICAL_PHYS_TO_VIRT(gdt_pa + gdt_offset), &cseg, cseg_sz);
+	gdt_offset += cseg_sz;
+
+	/* Make and add a code segment descriptor to the GDT */
+	cseg = MAKE_CSD_LM(0, 0, 1, 0);
+	memcpy(VSM_NON_LOGICAL_PHYS_TO_VIRT(gdt_pa + gdt_offset), &cseg, cseg_sz);
+	gdt_offset += cseg_sz;
+
+	/* Make and add a data segment descriptor to the GDT */
+	dseg = MAKE_DSD_LM(1, 0);
+	memcpy(VSM_NON_LOGICAL_PHYS_TO_VIRT(gdt_pa + gdt_offset), &dseg, dseg_sz);
+	gdt_offset += dseg_sz;
+
+	/* Compute the VA that secure kernele will see for the TSS */
+	tss_sk_va = VSM_VA_FROM_PA(tss_pa);
+
+	/* Make and add a system segment descriptor for the TSS in the GDT */
+	sseg = MAKE_SSD(
+		tss_sz,
+		tss_sk_va,
+		VSM_SYSTEM_SEGMENT_TYPE_TSS,
+		0,
+		1,
+		0,
+		0,
+		0,
+		tss_sk_va >> 24);
+	memcpy(VSM_NON_LOGICAL_PHYS_TO_VIRT(gdt_pa + gdt_offset), &sseg, sseg_sz);
+	gdt_offset += sseg_sz;
+
+#ifdef CONFIG_HYPERV_VSM_DEBUG
+	pr_info("%s: Printing GDT..\n", __func__);
+	pr_info("\t\t 0: 0x%llx\n", (u64)0);
+	pr_info("\t\t 1: 0x%llx\n", *(u64 *)&cseg);
+	pr_info("\t\t 2: 0x%llx\n", *(u64 *)&dseg);
+	pr_info("\t\t 3: 0x%llx\n", *(u64 *)&sseg);
+#endif
+	/* Set up the GDT register */
+	vp_ctx->gdtr.base = VSM_VA_FROM_PA(gdt_pa);
+	vp_ctx->gdtr.limit = gdt_offset - 1;
+
+#ifdef CONFIG_HYPERV_VSM_DEBUG
+	pr_info("%s Printing GDTR..\n", __func__);
+	pr_info("\t\t Base:  0x%llx\n", vp_ctx->gdtr.base);
+	pr_info("\t\t Limit: 0x%x\n",   vp_ctx->gdtr.limit);
+#endif
+	/* Set the code segment (CS) selector */
+	vp_ctx->cs.base = 0;
+	vp_ctx->cs.limit = 0;
+	vp_ctx->cs.selector = 1 << 3;
+	vp_ctx->cs.segment_type = VSM_CODE_SEGMENT_TYPE_EXECUTE_READ_ACCESSED;
+	vp_ctx->cs.non_system_segment = 1;
+	vp_ctx->cs.descriptor_privilege_level = 0;
+	vp_ctx->cs.present = 1;
+	vp_ctx->cs.reserved = 0;
+	vp_ctx->cs.available = 0;
+	vp_ctx->cs._long = 1;
+	vp_ctx->cs._default = 0;
+	vp_ctx->cs.granularity = 0;
+
+#ifdef CONFIG_HYPERV_VSM_DEBUG
+	pr_info("%s Printing CS...\n", __func__);
+	pr_info("\t\t Sel:   0x%x\n",   vp_ctx->cs.selector);
+	pr_info("\t\t Base:  0x%llx\n", vp_ctx->cs.base);
+	pr_info("\t\t Limit: 0x%x\n",   vp_ctx->cs.limit);
+	pr_info("\t\t Attrs: 0x%x\n",   vp_ctx->cs.attributes);
+#endif
+
+	/* Set the data segment (DS) selector */
+	vp_ctx->ds.base = 0;
+	vp_ctx->ds.limit = 0;
+	vp_ctx->ds.selector = 2 << 3;
+	vp_ctx->ds.segment_type = VSM_DATA_SEGMENT_TYPE_READ_WRITE_ACCESSED;
+	vp_ctx->ds.non_system_segment = 1;
+	vp_ctx->ds.descriptor_privilege_level = 0;
+	vp_ctx->ds.present = 1;
+	vp_ctx->ds.reserved = 0;
+	vp_ctx->ds.available = 0;
+	vp_ctx->ds._long = 0;
+	vp_ctx->ds._default = 1;
+	vp_ctx->ds.granularity = 0;
+
+	/* Set the ES, FS and GS to be the same as DS, for now */
+	vp_ctx->es = vp_ctx->ds;
+	vp_ctx->fs = vp_ctx->ds;
+	vp_ctx->gs = vp_ctx->ds;
+
+#ifdef CONFIG_HYPERV_VSM_DEBUG
+	pr_info("%s: Printing DS/ES/FS/GS...\n");
+	pr_info("\t\t Sel:   0x%x\n",   vp_ctx->ds.selector);
+	pr_info("\t\t Base:  0x%llx\n", vp_ctx->ds.base);
+	pr_info("\t\t Limit: 0x%x\n",   vp_ctx->ds.limit);
+	pr_info("\t\t Attrs: 0x%x\n",   vp_ctx->ds.attributes);
+#endif
+
+	/* Set the stack selector to 0 (unused in long mode) */
+	vp_ctx->ss.selector = 0;
+
+	/* Set the initial stack pointer for the kernel to point to bottom of kernel stack */
+	tss->rsp0 = VSM_VA_FROM_PA(kstack_pa) + VSM_PAGE_SIZE - 1;
+	vp_ctx->rsp = tss->rsp0;
+
+#ifdef CONFIG_HYPERV_VSM_DEBUG
+	pr_info("%s: Printing TSS...\n", __func__);
+	pr_info("\t\t RSP0: 0x%llx\n", tss->rsp0);
+#endif
+
+	/* Set the task register selector  */
+	vp_ctx->tr.base = tss_sk_va;
+	vp_ctx->tr.limit = tss_sz - 1;
+	vp_ctx->tr.selector = 3 << 3;
+	vp_ctx->tr.segment_type = VSM_SYSTEM_SEGMENT_TYPE_BUSY_TSS;
+	vp_ctx->tr.non_system_segment = 0;
+	vp_ctx->tr.descriptor_privilege_level = 0;
+	vp_ctx->tr.present = 1;
+	vp_ctx->tr.reserved = 0;
+	vp_ctx->tr.available = 0;
+	vp_ctx->tr._long = 0;
+	vp_ctx->tr._default = 0;
+	vp_ctx->tr.granularity = 0;
+
+#ifdef CONFIG_HYPERV_VSM_DEBUG
+	pr_info("%s: Printing TR...\n", __func__);
+	pr_info("\t\t Sel:   0x%x\n",   vp_ctx->tr.selector);
+	pr_info("\t\t Base:  0x%llx\n", vp_ctx->tr.base);
+	pr_info("\t\t Limit: 0x%x\n",   vp_ctx->tr.limit);
+	pr_info("\t\t Attrs: 0x%x\n",   vp_ctx->tr.attributes);
+#endif
+}
+
+/* ToDo: Evaluate whether IDT need to be populated */
+static void __init hv_vsm_init_idt(struct hv_initial_vp_context *vp_ctx)
+{
+	phys_addr_t idt_pa;
+	u128 seg;
+	u64 target;
+	u16 target_lo;
+	u128 target_hi;
+	u64 type;
+	u16 selector;
+	size_t seg_sz = sizeof(seg);
+	int n;
+
+	idt_pa = PAGE_AT(vsm_skm_pa, VSM_IDT_PAGE);
+
+	/* Fill in the IDT to point to the ISR stubs */
+	selector = 1 << 3;
+	for (n = 0; n <= 21; n++) {
+		type = VSM_GATE_TYPE_INT;
+		target = (u64)VSM_VA_FROM_PA(PAGE_AT(vsm_skm_pa,
+			VSM_ISRS_CODE_PAGE) + (n * 8));
+
+		target_lo = (u16)target;
+		target_hi = (u128)target << 32;
+
+		seg = target_lo | target_hi | ((u64)selector << 16) |
+			(VSM_GATE_TYPE_INT << 40) | ((u64)1 << 47);
+		
+		memcpy(VSM_NON_LOGICAL_PHYS_TO_VIRT(idt_pa + (n * seg_sz)), &seg, seg_sz);
+	}
+
+	/* Set the IDT register */
+	vp_ctx->idtr.base = VSM_VA_FROM_PA(idt_pa);
+	vp_ctx->idtr.limit = (seg_sz * 22) - 1;
+}
+
+static void __init hv_vsm_fill_pte_tables(u64 *pde, int pd_index, int num_pte_tables)
+{
+	/* 
+	 * ToDo: Make a generic solution that can take any PA start and size and adjust
+	 * the number of page tables and determine where to start filling them. 
+	 */
+	u16 i, j;
+	phys_addr_t pte_pa;
+	u64 *pte;
+
+	/* Fill page tables with entries */
+	for (i = 0; i < num_pte_tables; i++) {
+		pte_pa = PAGE_AT(vsm_skm_pa, VSM_PTE_0_PAGE + i);
+		pte = VSM_NON_LOGICAL_PHYS_TO_VIRT(pte_pa);
+		*(pde + pd_index + i) = pte_pa | VSM_PAGE_PTE_OPTEE;
+		for (j = 0; j < VSM_ENTRIES_PER_PT; j++) {
+			*(pte + j) =
+				(vsm_skm_pa + ((j + (i * VSM_ENTRIES_PER_PT)) * VSM_PAGE_SIZE)) |
+					VSM_PAGE_PTE_OPTEE;
+		}
+	}
+}
+
+static void __init hv_vsm_init_page_tables(struct hv_initial_vp_context *vp_ctx)
+{
+	u64 pml4_index;
+	u64 pdp_index;
+	u64 pd_index;
+	phys_addr_t pml4e_pa;
+	phys_addr_t pdpe_pa;
+	phys_addr_t pde_pa;
+	u64 *pml4e;
+	u64 *pdpe;
+	u64 *pde;
+	int num_pte_tables;
+	
+	/* Get offset to know where to start mapping. Note vsm_skm_pa is the VA for OP-TEE */
+	pml4_index = VSM_GET_PML4_INDEX_FROM_VA(vsm_skm_pa);
+	pdp_index = VSM_GET_PDP_INDEX_FROM_VA(vsm_skm_pa);
+	pd_index = VSM_GET_PD_INDEX_FROM_VA(vsm_skm_pa);
+
+#ifdef CONFIG_HYPERV_VSM_DEBUG
+	pr_info("%s: pml4_index = 0x%llx, pdp_index = 0x%llx, pd_index=0x%llx\n",
+			__func__, pml4_index, pdp_index, pd_index);
+#endif
+
+	pml4e_pa = PAGE_AT(vsm_skm_pa, VSM_PML4E_PAGE);
+	pdpe_pa = PAGE_AT(vsm_skm_pa, VSM_PDPE_PAGE);
+	pde_pa = PAGE_AT(vsm_skm_pa, VSM_PDE_PAGE);
+
+	pml4e = VSM_NON_LOGICAL_PHYS_TO_VIRT(pml4e_pa);
+	pdpe = VSM_NON_LOGICAL_PHYS_TO_VIRT(pdpe_pa);
+	pde = VSM_NON_LOGICAL_PHYS_TO_VIRT(pde_pa);
+
+	/* N.B.: Adding '+ 1' to a pointer moves the underlying value forward by 8 bytes! */
+	*(pml4e + pml4_index) = pdpe_pa | VSM_PAGE_OPTEE;
+	*(pdpe + pdp_index) = pde_pa | VSM_PAGE_OPTEE;
+
+	/* Initial page tables map only the first SK_INITIAL_MAP_SIZE size of memory.
+	 * This memory will be used for the Secure Loader.
+	 */
+	num_pte_tables = (SK_INITIAL_MAP_SIZE / VSM_PAGE_SIZE) / VSM_ENTRIES_PER_PT;
+	hv_vsm_fill_pte_tables(pde, pd_index, num_pte_tables);
+
+	vp_ctx->cr3 = pml4e_pa;
+
+#ifdef CONFIG_HYPERV_VSM_DEBUG
+	pr_info("%s: Physical Range..\n", __func__);
+	pr_info("\t\t Start: 0x%llx\n", vsm_skm_pa);
+	pr_info("\t\t End:   0x%llx\n", vsm_skm_pa + SK_INITIAL_MAP_SIZE - 1);
+	pr_info("%s: Page Table Physical Addresses\n", __func__);
+	pr_info("\t\t PML4:   0x%llx\n", pml4e_pa);
+	pr_info("\t\t PDPE:   0x%llx\n", pdpe_pa);
+	pr_info("\t\t PDE:    0x%llx\n", pde_pa);
+	pr_info("\t\t PTE 0: 0x%llx\n", PAGE_AT(vsm_skm_pa, VSM_PTE_0_PAGE));
+	pr_info("\t\t PTE 1: 0x%llx\n", PAGE_AT(vsm_skm_pa, VSM_PTE_0_PAGE + 1));
+	pr_info("\t\t PTE 2: 0x%llx\n", PAGE_AT(vsm_skm_pa, VSM_PTE_0_PAGE + 2));
+	pr_info("\t\t PTE 3: 0x%llx\n", PAGE_AT(vsm_skm_pa, VSM_PTE_0_PAGE + 3));
+	pr_info("%s: Page Table Dump\n", __func__);
+	pr_info("\t\t Entry: Idx/Lvl - Raw Value\n");
+	hv_vsm_dump_pt(pml4e_pa, 1);
+#endif
+}
+
+static void __init hv_vsm_arch_init_vp_context(struct hv_initial_vp_context *vp_ctx)
+{
+	/* The CPU expects these structures to be properly laid out. */
+	compiletime_assert(sizeof(union vsm_code_seg_desc) == 8,
+		"Code Segment Descriptor is not 8 bytes.");
+	compiletime_assert(sizeof(union vsm_data_seg_desc) == 8,
+		"Code Segment Descriptor is not 8 bytes.");
+	compiletime_assert(sizeof(union vsm_sys_seg_desc) == 16,
+		"System Segment Descriptor is not 16 bytes.");
+	compiletime_assert(sizeof(union vsm_call_gate_seg_desc) == 16,
+		"Call-Gate Segment Descriptor is not 16 bytes.");
+	compiletime_assert(sizeof(union vsm_int_trap_gate_seg_desc) == 16,
+		"Interrupt-/Trap-Gate Segment Descriptor is not 16 bytes.");
+
+	hv_vsm_init_cpu(vp_ctx);
+	hv_vsm_init_gdt(vp_ctx);
+	hv_vsm_init_idt(vp_ctx);
+	hv_vsm_init_page_tables(vp_ctx);
+}
+
+static int __init hv_vsm_enable_vp_vtl(void)
+{
+	u64 status = 0;
+	unsigned long flags;
+	struct hv_input_enable_vp_vtl *hvin = NULL;
+
+	hvin = *this_cpu_ptr(hyperv_pcpu_input_arg);
+	hvin->partition_id = HV_PARTITION_ID_SELF;
+	hvin->vp_index = HV_VP_INDEX_SELF;
+	hvin->target_vtl = 1;
+	hvin->reserved_z0 = 0;
+	hvin->reserved_z1 = 0;
+
+	hv_vsm_arch_init_vp_context(&hvin->vp_vtl_context);
+
+	local_irq_save(flags);
+
+	status = hv_do_hypercall(HVCALL_ENABLE_VP_VTL, hvin, NULL);
+
+	local_irq_restore(flags);
+
+	return (int) (status & HV_HYPERCALL_RESULT_MASK);
+}
+
+static int __init hv_vsm_load_secure_kernel(void)
+{
+	/*
+	 * Till we combine the skloader and kernel into one binary, we have to load them separately
+	 * ToDo: Load them as one binary
+	 */
+	loff_t size_skloader, size_sk;
+	char *skloader_buf = NULL, *sk_buf = NULL;
+	int err;
+
+	// Find the size of skloader and sk
+	size_skloader = vfs_llseek(sk_loader, 0, SEEK_END);
+	size_sk = vfs_llseek(sk, 0, SEEK_END);
+
+	// Seek back to the beginning of the file
+	vfs_llseek(sk_loader, 0, SEEK_SET);
+	vfs_llseek(sk, 0, SEEK_SET);
+
+	// Allocate memory for the buffer
+	skloader_buf = kvmalloc(size_skloader, GFP_KERNEL);
+	if (!skloader_buf) {
+		pr_err("%s: Unable to allocate memory for copying secure kernel\n", __func__);
+		return -ENOMEM;
+	}
+	sk_buf = kvmalloc(size_sk, GFP_KERNEL);
+	if (!sk_buf) {
+		pr_err("%s: Unable to allocate memory for copying secure kernel\n", __func__);
+		kvfree(skloader_buf);
+		return -ENOMEM;
+	}
+
+	// Read from the file into the buffer
+	err = kernel_read(sk_loader, skloader_buf, size_skloader, &sk_loader->f_pos);
+	if (err != size_skloader) {
+		pr_err("%s Unable to read skloader.bin file\n", __func__);
+		kvfree(skloader_buf);
+		kvfree(sk_buf);
+		return -1;
+	}
+	err = kernel_read(sk, sk_buf, size_sk, &sk->f_pos);
+	if (err != size_sk) {
+		pr_err("%s Unable to read vmlinux.bin file\n", __func__);
+		kvfree(skloader_buf);
+		kvfree(sk_buf);
+		return -1;
+	}
+
+	memcpy(vsm_skm_va, skloader_buf, size_skloader);
+	memcpy(vsm_skm_va + (2 * 1024 * 1024), sk_buf, size_sk);
+	kvfree(skloader_buf);
+	kvfree(sk_buf);
+	return 0;
+}
+
+int __init hv_vsm_enable_vtl1(void)
+{
+	cpumask_var_t mask;
+	unsigned int boot_cpu;
+	u16 partition_enabled_vtl_set = 0, partition_mbec_enabled_vtl_set = 0, vp_enabled_vtl_set = 0;
+	u8 partition_max_vtl, active_mbec_enabled = 0;
+	int ret = 0;
+
+	if (!vsm_arch_has_vsm_access()) {
+		pr_err("%s: Arch does not support VSM\n", __func__);
+		return -ENOTSUPP;
+	}
+	if (hv_vsm_reserve_sk_mem()) {
+		pr_err("%s: Could not initialize memory for secure kernel\n", __func__);
+		return -ENOMEM;
+	}
+
+	sk_loader = filp_open("/usr/lib/firmware/skloader.bin", O_RDONLY, 0);
+	if (IS_ERR(sk_loader)) {
+		pr_err("%s: File usr/lib/firmware/skloader.bin not found\n", __func__);
+		ret = -ENOENT;
+		goto free_mem;
+	}
+	sk = filp_open("/usr/lib/firmware/vmlinux.bin", O_RDONLY, 0);
+	if (IS_ERR(sk)) {
+		pr_err("%s: File usr/lib/firmware/vmlinux.bin not found\n", __func__);
+		ret = -ENOENT;
+		goto close_file;
+	}
+
+	ret = hv_vsm_get_code_page_offsets();
+	if (ret) {
+		pr_err("%s: Unbable to retrieve vsm page offsets\n", __func__);
+		goto close_files;
+	}
+
+	/*
+	 * Copy the current cpu mask and pin rest of the running code to boot cpu.
+	 * Important since we want boot cpu of VTL0 to be the boot cpu for VTL1.
+	 * ToDo: Check if copying and restoring current->cpus_mask is enough
+	 * ToDo: Verify the assumption that cpumask_first(cpu_online_mask) is
+	 * the boot cpu
+	 */
+	if (!alloc_cpumask_var(&mask, GFP_KERNEL)) {
+		pr_err("%s: Could not allocate cpumask", __func__);
+		ret = -ENOMEM;
+		goto close_files;
+	}
+
+	cpumask_copy(mask, &current->cpus_mask);
+	boot_cpu = cpumask_first(cpu_online_mask);
+	set_cpus_allowed_ptr(current, cpumask_of(boot_cpu));
+
+	/* Check and enable VTL1 at the partition level */
+	ret = hv_vsm_get_partition_status(&partition_enabled_vtl_set, &partition_max_vtl, &partition_mbec_enabled_vtl_set);
+	if (ret)
+		goto out;
+
+	if (partition_max_vtl < HV_VTL1) {
+		pr_err("%s: VTL1 is not supported", __func__);
+		ret = -EINVAL;
+		goto out;
+	}
+	if (partition_enabled_vtl_set & HV_VTL1_ENABLE_BIT) {
+		pr_info("%s: Partition VTL1 is already enabled\n", __func__);
+	} else {
+		ret = hv_vsm_enable_partition_vtl();
+		if (ret) {
+			pr_err("%s: Enabling Partition VTL1 failed with status 0x%x\n", __func__, ret);
+			ret = -EINVAL;
+			goto out;
+		}
+		hv_vsm_get_partition_status(&partition_enabled_vtl_set, &partition_max_vtl, &partition_mbec_enabled_vtl_set);
+		if (!(partition_enabled_vtl_set & HV_VTL1_ENABLE_BIT)) {
+			pr_err("%s: Tried Enabling Partition VTL 1 and still failed", __func__);
+			ret = -EINVAL;
+			goto out;
+		}
+		if (!partition_mbec_enabled_vtl_set) {
+			pr_err("%s: Tried Enabling Partition MBEC and failed", __func__);
+			ret = -EINVAL;
+			goto out;
+		}
+	}
+
+	/* Check and enable VTL1 for the primary virtual processor */
+	ret = hv_vsm_get_vp_status(&vp_enabled_vtl_set, &active_mbec_enabled);
+	if (ret)
+		goto out;
+
+	if (vp_enabled_vtl_set & HV_VTL1_ENABLE_BIT) {
+		pr_info("%s: VP VTL1 is already enabled\n", __func__);
+	} else {
+		ret = hv_vsm_enable_vp_vtl();
+		if (ret) {
+			pr_err("%s: Enabling VP VTL1 failed with status 0x%x\n", __func__, ret);
+			/* ToDo: Should we disable VTL1 at partition level in this case */
+			ret = -EINVAL;
+			goto out;
+		}
+		hv_vsm_get_vp_status(&vp_enabled_vtl_set, &active_mbec_enabled);
+		if (!(vp_enabled_vtl_set & HV_VTL1_ENABLE_BIT)) {
+			pr_err("%s: Tried Enabling VP VTL 1 and still failed", __func__);
+			ret = -EINVAL;
+			goto out;
+		}
+	}
+
+	ret = hv_vsm_load_secure_kernel();
+
+	if (ret)
+		goto out;
+
+	/*
+	 * Kick start vtl1 boot on primary cpu. There is currently no way to exit
+	 * gracefully if this boot is not successful. In case of a failure, primary cpu
+	 * will not return from vtl1 and system will hang.
+	 */
+	hv_vsm_boot_vtl1();
+	hv_vsm_boot_success = true;
+
+out:
+	set_cpus_allowed_ptr(current, mask);
+	free_cpumask_var(mask);
+close_files:
+	filp_close(sk, NULL);
+close_file:
+	filp_close(sk_loader, NULL);
+free_mem:
+	vunmap(vsm_skm_va);
+	vsm_skm_pa = 0;
+	return ret;
+}
+#else
+int __init hv_vsm_enable_vtl1(void)
+{
+	return 0;
+}
+#endif
+
+static int __init hv_vsm_boot_init(void)
+{
+	hv_vsm_enable_vtl1();
+
+    return 0;
+}
+
+module_init(hv_vsm_boot_init);
+MODULE_DESCRIPTION("Hyper-V VSM Boot VTL0 Driver");
+MODULE_LICENSE("GPL");
diff --git a/drivers/hv/hv_vsm_boot.h b/drivers/hv/hv_vsm_boot.h
new file mode 100644
index 000000000000..69b304b0babb
--- /dev/null
+++ b/drivers/hv/hv_vsm_boot.h
@@ -0,0 +1,383 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (c) 2023, Microsoft Corporation.
+ *
+ * Author:
+ *   
+ */
+
+#ifndef _HV_VSM_BOOT_H
+#define _HV_VSM_BOOT_H
+
+/* ToDo : Clean this file and move appropriate stuff into vsm.h and hv_vsm_boot.c. Delete this file */
+
+#include <asm-generic/memory_model.h>
+#include <linux/mm.h>
+
+#ifndef u128
+#define u128 __uint128_t
+#endif  // u128
+
+#define VSM_FIRST_CODE_PAGE    0
+#define VSM_ISRS_CODE_PAGE     2816
+#define VSM_GDT_PAGE           4081
+#define VSM_IDT_PAGE           4082
+#define VSM_TSS_PAGE           4083
+#define VSM_PML4E_PAGE         4084
+#define VSM_PDPE_PAGE          4085
+#define VSM_PDE_PAGE           4086
+#define VSM_PTE_0_PAGE         4087
+#define VSM_KERNEL_STACK_PAGE  4095  // 4Kb stack
+
+/*
+ * Initial memory that will be mapped for secure kernel.
+ * Secure Kernel memory can be larger than this.
+ */
+#define SK_INITIAL_MAP_SIZE	(16 * 1024 * 1024)
+
+/* Defines the page size */
+#define VSM_PAGE_SHIFT  12
+
+/* Computed page size */
+#define VSM_PAGE_SIZE  (((uint32_t)1) << VSM_PAGE_SHIFT)
+
+/* Number of entries in a page table (all levels) */
+#define VSM_ENTRIES_PER_PT	512
+
+#define PAGE_AT(addr, idx) ((addr) + (idx) * VSM_PAGE_SIZE)
+
+/* Compute the address of the next page with the given base */
+#define NEXT_PAGE(addr) PAGE_AT((addr), 1)
+
+/* Locations of configuration bits in page table entries (all levels) */
+#define VSM_PAGE_BIT_PRESENT	0
+#define VSM_PAGE_BIT_RW			1
+#define VSM_PAGE_BIT_USER		2
+#define VSM_PAGE_BIT_PWT		3
+#define VSM_PAGE_BIT_PCD		4
+#define VSM_PAGE_BIT_ACCESSED	5
+#define VSM_PAGE_BIT_DIRTY		6
+#define VSM_PAGE_BIT_PAT		7
+#define VSM_PAGE_BIT_GLOBAL		8
+#define VSM_PAGE_BIT_NX			63
+
+/* Shifts to compute page table mapping (See AMD APM Vol 2, 5.3) */
+#define VSM_PD_TABLE_SHIFT      21
+#define VSM_PDP_TABLE_SHIFT     30
+#define VSM_PML4_TABLE_SHIFT    39
+
+/* Computed values for each configuration bit */
+#define VSM_PAGE_PRESENT	(1 << VSM_PAGE_BIT_PRESENT)
+#define VSM_PAGE_RW			(1 << VSM_PAGE_BIT_RW)
+#define VSM_PAGE_USER		(1 << VSM_PAGE_BIT_USER)
+#define VSM_PAGE_PWT		(1 << VSM_PAGE_BIT_PWT)
+#define VSM_PAGE_PCD		(1 << VSM_PAGE_BIT_PCD)
+#define VSM_PAGE_ACCESSED	(1 << VSM_PAGE_BIT_ACCESSED)
+#define VSM_PAGE_DIRTY		(1 << VSM_PAGE_BIT_DIRTY)
+#define VSM_PAGE_PAT		(1 << VSM_PAGE_BIT_PAT)
+#define VSM_PAGE_GLOBAL		(1 << VSM_PAGE_BIT_GLOBAL)
+#define VSM_PAGE_NX		(1 << VSM_PAGE_BIT_NX)
+
+/* Useful combinations of bit configurations */
+#define VSM_PAGE_OPTEE \
+	(VSM_PAGE_PRESENT | VSM_PAGE_RW)
+#define VSM_PAGE_PTE_OPTEE \
+	(VSM_PAGE_OPTEE | VSM_PAGE_ACCESSED | VSM_PAGE_DIRTY)
+
+#define HV_VTL1_ENABLE_BIT	BIT(1)
+#define HV_VTL1			0x1
+
+/* Compute the VA for a given PA for initial VTL1 loading. Assumes identity mapping */
+#define VSM_VA_FROM_PA(pa) (pa)
+
+/* 
+ * Built in phys_to_virt converts kernel logical addresses to PAs.
+ * These convert kernel virtual address to PAs and vice versa.
+ */
+#define VSM_NON_LOGICAL_PHYS_TO_VIRT(pa) ((pa) - vsm_skm_pa + vsm_skm_va)
+#define VSM_NON_LOGICAL_VIRT_TO_PHYS(va) ((va) - vsm_skm_va + vsm_skm_pa)
+
+/* Given VA, get index into the page table at a given level */
+#define VSM_GET_PML4_INDEX_FROM_VA(va) (((va) >> VSM_PML4_TABLE_SHIFT) & 0x1FF)
+#define VSM_GET_PDP_INDEX_FROM_VA(va) (((va) >> VSM_PDP_TABLE_SHIFT) & 0x1FF)
+#define VSM_GET_PD_INDEX_FROM_VA(va) (((va) >> VSM_PD_TABLE_SHIFT) & 0x1FF)
+
+#define CR0_PE				0x00000001		// protection enable
+#define CR0_MP				0x00000002		// math present
+#define CR0_EM				0x00000004		// emulate math coprocessor
+#define CR0_TS				0x00000008		// task switched
+#define CR0_ET				0x00000010		// extension type (80387)
+#define CR0_NE				0x00000020		// numeric error
+#define CR0_WP				0x00010000		// write protect
+#define CR0_AM				0x00040000		// alignment mask
+#define CR0_NW				0x20000000		// not write-through
+#define CR0_CD				0x40000000		// cache disable
+#define CR0_PG				0x80000000		// paging
+
+#define CR4_VME				0x00000001		// V86 mode extensions
+#define CR4_PVI				0x00000002		// Protected mode virtual interrupts
+#define CR4_TSD				0x00000004		// Time stamp disable
+#define CR4_DE				0x00000008		// Debugging Extensions
+#define CR4_PSE				0x00000010		// Page size extensions
+#define CR4_PAE				0x00000020		// Physical address extensions
+#define CR4_MCE				0x00000040		// Machine check enable
+#define CR4_PGE				0x00000080		// Page global enable
+#define CR4_PCE				0x00000100		// Performance monitor counter enable
+#define CR4_FXSR			0x00000200		// FXSR used by OS
+#define CR4_XMMEXCPT		0x00000400		// XMMI used by OS
+#define CR4_UMIP			0x00000800		// User Mode Instruction Prevention (UMIP) enable
+#define CR4_LA57			0x00001000		// 5-level paging enable (57 bit linear address)
+#define CR4_VMXE			0x00002000		// VMX enable
+#define CR4_SMXE			0x00004000		// SMX enable
+#define CR4_RDWRFSGSBASE	0x00010000		// RDWR FSGS Base enable = bit 16
+#define CR4_PCIDE			0x00020000		// PCID enable
+#define CR4_XSAVE			0x00040000		// XSAVE/XRSTOR enable
+#define CR4_SMEP			0x00100000		// SMEP enable
+#define CR4_SMAP			0x00200000		// SMAP enable
+#define CR4_CET				0x00800000		// CET enable
+
+#define MSR_SCE				0x00000001		// system call enable
+#define MSR_LME				0x00000100		// long mode enable
+#define MSR_LMA				0x00000400		// long mode active
+#define MSR_NXE				0x00000800		// no execute enable
+#define MSR_SVME			0x00001000		// secure virtual machine enable
+#define MSR_FFXSR			0x00004000		// fast floating save/restore
+
+#define MSR_PAT				0x277			// page attributes table
+
+/* Intel SDM Vol 3A, 11.12.2 (IA32_PAT MSR) */
+enum {
+	PAT_UC = 0,        /* uncached */
+	PAT_WC = 1,        /* Write combining */
+	PAT_WT = 4,        /* Write Through */
+	PAT_WP = 5,        /* Write Protected */
+	PAT_WB = 6,        /* Write Back (default) */
+	PAT_UC_MINUS = 7,  /* UC, but can be overridden by MTRR */
+};
+
+/* Compute a given PAT entry for a given caching mode name */
+#define VSM_PAT(x, y)	((u64)PAT_ ## y << ((x) * 8))
+
+/* The type field for an interrupt gate */
+#define VSM_GATE_TYPE_INT	((u64)0xE)
+
+/* AMD APM Vol 2, Table 4.6 */
+#define VSM_SYSTEM_SEGMENT_TYPE_LDT				0x2
+#define VSM_SYSTEM_SEGMENT_TYPE_TSS				0x9
+#define VSM_SYSTEM_SEGMENT_TYPE_BUSY_TSS		0xB
+#define VSM_SYSTEM_SEGMENT_TYPE_CALL_GATE		0xC
+#define VSM_SYSTEM_SEGMENT_TYPE_INTERRUPT_GATE	0xE
+#define VSM_SYSTEM_SEGMENT_TYPE_TRAP_GATE		0xF
+
+/* Intel SDM Vol 3A, Table 3-1 */
+#define VSM_CODE_SEGMENT_TYPE_EXECUTE_READ_ACCESSED		0xB
+#define VSM_DATA_SEGMENT_TYPE_READ_WRITE_ACCESSED		0x3
+
+/* Format of a Code Segment (long mode) */
+union vsm_code_seg_desc {
+	u64 as_u64;
+
+	struct {
+		u64 lo : 40;
+		u64 flags_lo : 8;
+		u64 flags_hi : 8;
+		u64 hi : 8;
+	} bytes __packed;
+
+	struct {
+		u64 seglim_lo : 16;
+		u64 addr_lo : 24;
+		u64 a : 1;
+		u64 r : 1;
+		u64 c : 1;
+		u64 mbo1 : 1;
+		u64 mbo2 : 1;
+		u64 dpl : 2;
+		u64 p : 1;
+		u64 seglim_hi : 4;
+		u64 avl : 1;
+		u64 l : 1;
+		u64 d : 1;
+		u64 g : 1;
+		u64 addr_hi : 8;
+	} __packed;
+};
+
+/* Format of a Data Segment (long mode) */
+union vsm_data_seg_desc {
+	u64 as_u64;
+
+	struct {
+		u64 lo : 40;
+		u64 flags_lo : 8;
+		u64 flags_hi : 8;
+		u64 hi : 8;
+	} bytes __packed;
+
+	struct {
+		u64 seglim_lo : 16;
+		u64 addr_lo : 24;
+		u64 a : 1;
+		u64 w : 1;
+		u64 e : 1;
+		u64 mbz1 : 1;
+		u64 mbo1 : 1;
+		u64 dpl : 2;
+		u64 p : 1;
+		u64 seglim_hi : 4;
+		u64 avl : 1;
+		u64 ign1 : 1;
+		u64 db : 1;
+		u64 g : 1;
+		u64 addr_hi : 8;
+	} __packed;
+};
+
+/* Format of a System Segment (long mode) */
+union vsm_sys_seg_desc {
+	struct {
+		u64 lo : 40;
+		u64 flags_lo : 8;
+		u64 flags_hi : 8;
+		u64 mid;
+		u8 hi;
+	} __packed bytes; // Packed attribute must be first.
+
+	struct {
+		u64 seglim_lo : 16;
+		u64 addr_lo : 24;
+		u64 type : 4;
+		u64 mbz1 : 1;
+		u64 dpl : 2;
+		u64 p : 1;
+		u64 seglim_hi : 4;
+		u64 avl : 1;
+		u64 ign1 : 2;
+		u64 g : 1;
+		u64 addr_hi : 40;
+		u32 ign2;
+	} __packed;
+};
+
+/* Format of a Call-Gate Segment (long mode) */
+union vsm_call_gate_seg_desc {
+	struct {
+		u64 toff_lo : 16;
+		u64 tsel : 16;
+		u64 ign1 : 8;
+		u64 type : 4;
+		u64 mbz1 : 1;
+		u64 dpl : 2;
+		u64 p : 1;
+		u64 toff_hi : 48;
+		u64 ign2 : 8;
+		u64 mbz2 : 5;
+		u64 ign3 : 19;
+	} __packed;
+};
+
+/* Format of a Interrupt- and Trap-Gate Segment (long mode) */
+union vsm_int_trap_gate_seg_desc {
+	struct {
+		u64 toff_lo : 16;
+		u64 tsel : 16;
+		u64 ist : 3;
+		u64 ign1 : 5;
+		u64 type : 4;
+		u64 mbz1 : 1;
+		u64 dpl : 2;
+		u64 p : 1;
+		u64 toff_hi : 48;
+		u64 ign2 : 32;
+	} __packed;
+};
+
+/* Format of the Task State Segment (long mode) */
+union vsm_tss {
+	u32 reserved1;
+
+	u64 rsp0;
+	u64 rsp1;
+	u64 rsp2;
+
+	u64 reserved2;
+
+	u64 ist[7];
+
+	u64 reserved3;
+	u8  reserved4;
+
+	u8 io_bitmap_base;
+} __packed;
+
+/* A type for the Global Descriptor Table (GDT) */
+typedef void vsm_gdt_t;
+
+/* Make Code Segment Descriptor */
+#define MAKE_CSD(_seglim_lo, _addr_lo, _a, _r, _c, _dpl, _p, _seglim_hi, _avl, _l, _d, _g, _addr_hi) \
+	(union vsm_code_seg_desc) {		\
+		.seglim_lo = (_seglim_lo),	\
+		.addr_lo = (_addr_lo),		\
+		.a = (_a),					\
+		.r = (_r),					\
+		.c = (_c),					\
+		.mbo1 = 1,					\
+		.mbo2 = 1,					\
+		.dpl = (_dpl),				\
+		.p = (_p),					\
+		.seglim_hi = (_seglim_hi),	\
+		.avl = (_avl),				\
+		.l = (_l),					\
+		.d = (_d),					\
+		.g = (_g)					\
+	}
+
+/* Make Code Segment Descriptor (long mode) */
+#define MAKE_CSD_LM(_c, _dpl, _p, _avl) \
+	MAKE_CSD(0, 0, 0, 0, (_c), (_dpl), (_p), 0, (_avl), 1, 0, 0, 0)
+
+/* Make Data Segment Descriptor */
+#define MAKE_DSD(_seglim_lo, _addr_lo, _a, _w, _e, _dpl, _p, _seglim_hi, _avl, _db, _g, _addr_hi) \
+	(union vsm_data_seg_desc)  {	\
+		.seglim_lo = (_seglim_lo),	\
+		.addr_lo = (_addr_lo),		\
+		.a = (_a),					\
+		.w = (_w),					\
+		.e = (_e),					\
+		.mbz1 = 0,					\
+		.mbo1 = 1,					\
+		.dpl = (_dpl),				\
+		.p = (_p),					\
+		.seglim_hi = (_seglim_hi),	\
+		.avl = (_avl),				\
+		.ign1 = 0,					\
+		.db = (_db),				\
+		.g = (_g),					\
+		.addr_hi = (_addr_hi)		\
+	}
+
+/* Make Data Segment Descriptor (long mode) */
+#define MAKE_DSD_LM(_p, _avl) \
+	MAKE_DSD(0, 0, 0, 0, 0, 0, (_p), 0, (_avl), 0, 0, 0)
+
+/* Make System Segment Descriptor  */
+#define MAKE_SSD(_seglim_lo, _addr_lo, _type, _dpl, _p, _seglim_hi, _avl, _g, _addr_hi) \
+	(union vsm_sys_seg_desc) {		\
+		.seglim_lo = (_seglim_lo),	\
+		.addr_lo = (_addr_lo),		\
+		.type = (_type),			\
+		.mbz1 = 0,					\
+		.dpl = (_dpl),				\
+		.p = (_p),					\
+		.seglim_hi = (_seglim_hi),	\
+		.avl = (_avl),				\
+		.g = (_g),					\
+		.addr_hi = (_addr_hi),		\
+		.ign2 = 0					\
+	};
+
+/* Make a Segment Selector */
+#define MAKE_SELECTOR(rpl, ti, si) \
+	(u16)(((si) << 3) | ((ti) << 2) | (rpl))
+
+#endif /* _HV_VSM_BOOT_H */
diff --git a/include/asm-generic/hyperv-tlfs.h b/include/asm-generic/hyperv-tlfs.h
index fdac4a1714ec..62273ba09113 100644
--- a/include/asm-generic/hyperv-tlfs.h
+++ b/include/asm-generic/hyperv-tlfs.h
@@ -89,6 +89,8 @@
 #define HV_ACCESS_STATS				BIT(8)
 #define HV_DEBUGGING				BIT(11)
 #define HV_CPU_MANAGEMENT			BIT(12)
+#define HV_ACCESS_VSM				BIT(16)
+#define HV_ACCESS_VP_REGS			BIT(17)
 #define HV_ENABLE_EXTENDED_HYPERCALLS		BIT(20)
 #define HV_ISOLATION				BIT(22)
 
@@ -149,6 +151,8 @@ union hv_reference_tsc_msr {
 #define HVCALL_ENABLE_VP_VTL			0x000f
 #define HVCALL_NOTIFY_LONG_SPIN_WAIT		0x0008
 #define HVCALL_SEND_IPI				0x000b
+#define HVCALL_ENABLE_PARTITION_VTL		0x000d
+#define HVCALL_ENABLE_VP_VTL			0x000f
 #define HVCALL_FLUSH_VIRTUAL_ADDRESS_SPACE_EX	0x0013
 #define HVCALL_FLUSH_VIRTUAL_ADDRESS_LIST_EX	0x0014
 #define HVCALL_SEND_IPI_EX			0x0015
-- 
2.42.0


From e9bbec80b94f15113413dc90a5b9510c95328cf9 Mon Sep 17 00:00:00 2001
From: Anna Trikalinou <atrikalinou@microsoft.com>
Date: Mon, 6 Nov 2023 21:02:03 +0000
Subject: [PATCH 04/22] Boot Secondary VPs

Add up to 63 Virtual Application/Secondary Processors to VTL1.

This includes 2 vtlcalls:
- `VSM_VTL_CALL_FUNC_ID_ENABLE_APS_VTL`: This function will use VTL0's
  cpu_present_mask to make CPU's present in VTL1. It will also enable VTL1
for each CPU.
- `VSM_VTL_CALL_FUNC_ID_BOOT_APS`: This function will use VTL0's
  cpu_online_mask and will boot each CPU that is set.

A key part of bringing up Application Processor `x` (APx)  is that it is
done in part by the Boot CPU and in part by APx. The Boot CPU in VTL1
initiates the Boot Process of APx and at some point it needs to notify APx
in VTL0 that it can now transition to VTL1 to continue its part of the boot
process. This is done by using the `boot_signal` shared page between VTL0
and VTL1. Each secondary cpu will invokes the he vtl1 boot thread for the
next online cpu there by not needing all the cpus to be in a holding
pattern at boot.

Co-developed-by: Thara Gopinath <tgopinath@microsoft.com>
Signed-off-by: Thara Gopinath <tgopinath@microsoft.com>
Signed-off-by: Anna Trikalinou <atrikalinou@microsoft.com>
---
 drivers/hv/hv_vsm.h      |   3 +
 drivers/hv/hv_vsm_boot.c | 192 ++++++++++++++++++++++++++++++++++++++-
 2 files changed, 190 insertions(+), 5 deletions(-)

diff --git a/drivers/hv/hv_vsm.h b/drivers/hv/hv_vsm.h
index 5c14034fb482..4376a1aa3356 100644
--- a/drivers/hv/hv_vsm.h
+++ b/drivers/hv/hv_vsm.h
@@ -9,6 +9,9 @@
 #ifndef _HV_VSM_H
 #define _HV_VSM_H
 
+#define VSM_VTL_CALL_FUNC_ID_ENABLE_APS_VTL	0x1FFE0
+#define VSM_VTL_CALL_FUNC_ID_BOOT_APS		0x1FFE1
+
 extern bool hv_vsm_boot_success;
 extern bool hv_vsm_mbec_enabled;
 extern union hv_register_vsm_code_page_offsets vsm_code_page_offsets;
diff --git a/drivers/hv/hv_vsm_boot.c b/drivers/hv/hv_vsm_boot.c
index 99a383d86a0a..fd32e5d18c71 100644
--- a/drivers/hv/hv_vsm_boot.c
+++ b/drivers/hv/hv_vsm_boot.c
@@ -12,18 +12,25 @@
 #include <asm/e820/api.h>
 #include <linux/hyperv.h>
 #include <linux/module.h>
+#include <linux/kthread.h>
 #include <linux/file.h>
 #include <linux/fs.h>
+#include <linux/slab.h>
 #include <linux/cpumask.h>
 
 #include "hv_vsm_boot.h"
 #include "hv_vsm.h"
 
+#define VSM_BOOT_SIGNAL	0xDC
+
 extern struct resource sk_res;
+
 static struct file *sk_loader, *sk;
+static struct page *boot_signal_page, *cpu_online_page, *cpu_present_page;
 static phys_addr_t vsm_skm_pa;
 static void *vsm_skm_va;
 union hv_register_vsm_code_page_offsets vsm_code_page_offsets;
+static u8 *boot_signal;
 
 bool hv_vsm_boot_success = false;
 bool hv_vsm_mbec_enabled = true;
@@ -125,8 +132,10 @@ static int hv_vsm_get_vp_status(u16 *enabled_vtl_set, u8 *active_mbec_enabled)
 	status = hv_do_rep_hypercall(HVCALL_GET_VP_REGISTERS, 1, 0, hvin, hvout);
 	local_irq_restore(flags);
 
-	if (!hv_result_success(status))
+	if (!hv_result_success(status)) {
+		pr_err("%s failed with code %llu\n", __func__, status);
 		return -EFAULT;
+	}
 
 	vsm_vp_status = (union hv_register_vsm_vp_status)hvout->as_u64;
 	*enabled_vtl_set = vsm_vp_status.enabled_vtl_set;
@@ -274,6 +283,172 @@ static __init void hv_vsm_boot_vtl1(void)
 	}
 }
 
+static u64 hv_vsm_establish_shared_page(struct page **page)
+{
+	void *va;
+
+	*page = alloc_page(GFP_KERNEL);
+
+	if (!(*page)) {
+		pr_err("%s: Unable to establish VTL0-VTL1 shared page\n", __func__);
+		return -ENOMEM;
+	}
+
+	va = page_address(*page);
+	memset(va, 0, PAGE_SIZE);
+
+	return page_to_pfn(*page);
+}
+
+static __init int hv_vsm_enable_ap_vtl(void)
+{
+	struct vtlcall_param args = {0};
+	u64 cpu_present_mask_pfn;
+	void *va;
+	int ret = 0;
+
+	/* Allocate Present Cpumask Page & Copy cpu_present_mask */
+	cpu_present_mask_pfn = hv_vsm_establish_shared_page(&cpu_present_page);
+
+	if (cpu_present_mask_pfn < 0)
+		return -ENOMEM;
+
+	va = page_address(cpu_present_page);
+	cpumask_copy((struct cpumask *)va, cpu_present_mask);
+
+	args.a0 = VSM_VTL_CALL_FUNC_ID_ENABLE_APS_VTL;
+	args.a1 = cpu_present_mask_pfn;
+
+	ret = hv_vsm_init_vtlcall(&args);
+
+	if (ret)
+		pr_err("%s: Failed to enable VTL1 for APs. Error %d", __func__, ret);
+
+	__free_page(cpu_present_page);
+	return ret;
+}
+
+struct task_struct **ap_thread;
+static int hv_vsm_boot_sec_vp_thread_fn(void *unused)
+{
+	struct vtlcall_param args = {0};
+	unsigned long flags = 0;
+	int cpu = smp_processor_id(), next_cpu;
+	u16 vp_enabled_vtl_set = 0;
+	u8 active_mbec_enabled = 0;
+
+	/* TODO: Remove once we allow >64 CPUs in Secure Kernel */
+	if (cpu > 63) {
+		pr_err("CPU%d: Secure Kernel currently supports CPUID <= 63.", smp_processor_id());
+		return -EINVAL;
+	}
+
+	pr_info("%s: cpu%d entering vtl1 boot thread\n", __func__, cpu);
+	local_irq_save(flags);
+	while (READ_ONCE(boot_signal[cpu]) != VSM_BOOT_SIGNAL) {
+		if (kthread_should_stop()) {
+			local_irq_restore(flags);
+			goto out;
+		}
+	}
+
+	local_irq_restore(flags);
+	hv_vsm_init_vtlcall(&args);
+out:
+	next_cpu = cpumask_next(cpu, cpu_online_mask);
+	if (next_cpu > 0 && next_cpu < nr_cpu_ids) {
+		wake_up_process(ap_thread[next_cpu]);
+		pr_info("%s: cpu%d exiting vtl1 boot thread. Waking up cpu%d\n",
+			__func__, cpu, next_cpu);
+	}
+
+	hv_vsm_get_vp_status(&vp_enabled_vtl_set, &active_mbec_enabled);
+	if (!active_mbec_enabled) {
+		pr_err("Failed to enable MBEC for VP%d\n", cpu);
+		hv_vsm_mbec_enabled = false;
+	}
+	return 0;
+}
+
+static __init int hv_vsm_boot_ap_vtl(void)
+{
+	struct vtlcall_param args = {0};
+	void *va;
+	u64 boot_signal_pfn, cpu_online_mask_pfn;
+	unsigned int cpu, cur_cpu = smp_processor_id(), vsm_cpus = num_possible_cpus(), next_cpu;
+	int ret;
+
+	/* Allocate & Initialize Boot Signal Page */
+	boot_signal_pfn = hv_vsm_establish_shared_page(&boot_signal_page);
+
+	if (boot_signal_pfn < 0)
+		return -ENOMEM;
+
+	va = page_address(boot_signal_page);
+	boot_signal = (u8 *)va;
+	boot_signal[0] = VSM_BOOT_SIGNAL;
+
+	/* Allocate Online Cpumask Page & Copy cpu_online_mask */
+	cpu_online_mask_pfn = hv_vsm_establish_shared_page(&cpu_online_page);
+
+	if (cpu_online_mask_pfn < 0) {
+		ret = -ENOMEM;
+		goto free_bootsignal;
+	}
+
+	va = page_address(cpu_online_page);
+	cpumask_copy((struct cpumask *)va, cpu_online_mask);
+
+	/* Create per-CPU threads to do vtlcall and complete per-CPU hotplug boot in VTL1 */
+	ap_thread = kmalloc_array(vsm_cpus, sizeof(*ap_thread), GFP_KERNEL);
+
+	if (!ap_thread) {
+		ret = -ENOMEM;
+		goto free_sharedpages;
+	}
+
+	memset(ap_thread, 0, sizeof(*ap_thread) * vsm_cpus);
+
+	for_each_online_cpu(cpu) {
+		if (cpu == cur_cpu)
+			continue;
+		ap_thread[cpu] = kthread_create(hv_vsm_boot_sec_vp_thread_fn, NULL, "ap_thread");
+
+		if (IS_ERR(ap_thread[cpu])) {
+			ret = PTR_ERR(ap_thread[cpu]);
+			goto out;
+		}
+
+		kthread_bind(ap_thread[cpu], cpu);
+		sched_set_fifo(ap_thread[cpu]);
+	}
+
+	next_cpu = cpumask_next(cur_cpu, cpu_online_mask);
+	if (next_cpu >= nr_cpu_ids)
+		goto out;
+
+	wake_up_process(ap_thread[next_cpu]);
+	args.a0 = VSM_VTL_CALL_FUNC_ID_BOOT_APS;
+	args.a1 = cpu_online_mask_pfn;
+	args.a2 = boot_signal_pfn;
+
+	ret = hv_vsm_init_vtlcall(&args);
+
+	if (ret)
+		pr_err("%s: Failed to boot APs for VTL1. Error %d", __func__, ret);
+out:
+	for_each_online_cpu(cpu) {
+		if (ap_thread[cpu])
+			kthread_stop(ap_thread[cpu]);
+	}
+	kfree(ap_thread);
+free_sharedpages:
+	__free_page(cpu_online_page);
+free_bootsignal:
+	__free_page(boot_signal_page);
+	return ret;
+}
+
 static int __init hv_vsm_enable_partition_vtl(void)
 {
 	u64 status = 0;
@@ -902,11 +1077,20 @@ int __init hv_vsm_enable_vtl1(void)
 	 * will not return from vtl1 and system will hang.
 	 */
 	hv_vsm_boot_vtl1();
-	hv_vsm_boot_success = true;
 
+	/* Enable VTL1 for secondary processots */
+	ret = hv_vsm_enable_ap_vtl();
+	if (ret)
+		goto out;
+
+	/* Boot secondary processors in VTL1 */
+	ret = hv_vsm_boot_ap_vtl();
+	if (!ret)
+		hv_vsm_boot_success = true;
 out:
 	set_cpus_allowed_ptr(current, mask);
 	free_cpumask_var(mask);
+
 close_files:
 	filp_close(sk, NULL);
 close_file:
@@ -925,9 +1109,7 @@ int __init hv_vsm_enable_vtl1(void)
 
 static int __init hv_vsm_boot_init(void)
 {
-	hv_vsm_enable_vtl1();
-
-    return 0;
+	return hv_vsm_enable_vtl1();
 }
 
 module_init(hv_vsm_boot_init);
-- 
2.42.0


From 58603410c62f7b7939c121eac7fe9202e826cfd0 Mon Sep 17 00:00:00 2001
From: Anna Trikalinou <atrikalinou@microsoft.com>
Date: Fri, 15 Dec 2023 23:47:38 +0000
Subject: [PATCH 05/22] Initialize VSM after VTL0 initcalls

Same as: https://dev.azure.com/lsg-linux-core/Linux-VBS/_git/mariner/pullrequest/113

Signed-off-by: Anna Trikalinou <atrikalinou@microsoft.com>
Signed-off-by: Thara Gopinath <tgopinath@microsoft.com>
---
 drivers/hv/hv_vsm_boot.c | 31 ++++---------------------------
 include/linux/vsm.h      | 23 +++++++++++++++++++++++
 init/main.c              |  2 ++
 3 files changed, 29 insertions(+), 27 deletions(-)
 create mode 100644 include/linux/vsm.h

diff --git a/drivers/hv/hv_vsm_boot.c b/drivers/hv/hv_vsm_boot.c
index fd32e5d18c71..cd25dad1eeaf 100644
--- a/drivers/hv/hv_vsm_boot.c
+++ b/drivers/hv/hv_vsm_boot.c
@@ -12,6 +12,7 @@
 #include <asm/e820/api.h>
 #include <linux/hyperv.h>
 #include <linux/module.h>
+#include <linux/memblock.h>
 #include <linux/kthread.h>
 #include <linux/file.h>
 #include <linux/fs.h>
@@ -958,7 +959,7 @@ static int __init hv_vsm_load_secure_kernel(void)
 	return 0;
 }
 
-int __init hv_vsm_enable_vtl1(void)
+void __init vsm_init(void)
 {
 	cpumask_var_t mask;
 	unsigned int boot_cpu;
@@ -968,23 +969,21 @@ int __init hv_vsm_enable_vtl1(void)
 
 	if (!vsm_arch_has_vsm_access()) {
 		pr_err("%s: Arch does not support VSM\n", __func__);
-		return -ENOTSUPP;
+		return;
 	}
 	if (hv_vsm_reserve_sk_mem()) {
 		pr_err("%s: Could not initialize memory for secure kernel\n", __func__);
-		return -ENOMEM;
+		return;
 	}
 
 	sk_loader = filp_open("/usr/lib/firmware/skloader.bin", O_RDONLY, 0);
 	if (IS_ERR(sk_loader)) {
 		pr_err("%s: File usr/lib/firmware/skloader.bin not found\n", __func__);
-		ret = -ENOENT;
 		goto free_mem;
 	}
 	sk = filp_open("/usr/lib/firmware/vmlinux.bin", O_RDONLY, 0);
 	if (IS_ERR(sk)) {
 		pr_err("%s: File usr/lib/firmware/vmlinux.bin not found\n", __func__);
-		ret = -ENOENT;
 		goto close_file;
 	}
 
@@ -1003,7 +1002,6 @@ int __init hv_vsm_enable_vtl1(void)
 	 */
 	if (!alloc_cpumask_var(&mask, GFP_KERNEL)) {
 		pr_err("%s: Could not allocate cpumask", __func__);
-		ret = -ENOMEM;
 		goto close_files;
 	}
 
@@ -1018,7 +1016,6 @@ int __init hv_vsm_enable_vtl1(void)
 
 	if (partition_max_vtl < HV_VTL1) {
 		pr_err("%s: VTL1 is not supported", __func__);
-		ret = -EINVAL;
 		goto out;
 	}
 	if (partition_enabled_vtl_set & HV_VTL1_ENABLE_BIT) {
@@ -1027,18 +1024,15 @@ int __init hv_vsm_enable_vtl1(void)
 		ret = hv_vsm_enable_partition_vtl();
 		if (ret) {
 			pr_err("%s: Enabling Partition VTL1 failed with status 0x%x\n", __func__, ret);
-			ret = -EINVAL;
 			goto out;
 		}
 		hv_vsm_get_partition_status(&partition_enabled_vtl_set, &partition_max_vtl, &partition_mbec_enabled_vtl_set);
 		if (!(partition_enabled_vtl_set & HV_VTL1_ENABLE_BIT)) {
 			pr_err("%s: Tried Enabling Partition VTL 1 and still failed", __func__);
-			ret = -EINVAL;
 			goto out;
 		}
 		if (!partition_mbec_enabled_vtl_set) {
 			pr_err("%s: Tried Enabling Partition MBEC and failed", __func__);
-			ret = -EINVAL;
 			goto out;
 		}
 	}
@@ -1055,13 +1049,11 @@ int __init hv_vsm_enable_vtl1(void)
 		if (ret) {
 			pr_err("%s: Enabling VP VTL1 failed with status 0x%x\n", __func__, ret);
 			/* ToDo: Should we disable VTL1 at partition level in this case */
-			ret = -EINVAL;
 			goto out;
 		}
 		hv_vsm_get_vp_status(&vp_enabled_vtl_set, &active_mbec_enabled);
 		if (!(vp_enabled_vtl_set & HV_VTL1_ENABLE_BIT)) {
 			pr_err("%s: Tried Enabling VP VTL 1 and still failed", __func__);
-			ret = -EINVAL;
 			goto out;
 		}
 	}
@@ -1098,20 +1090,5 @@ int __init hv_vsm_enable_vtl1(void)
 free_mem:
 	vunmap(vsm_skm_va);
 	vsm_skm_pa = 0;
-	return ret;
-}
-#else
-int __init hv_vsm_enable_vtl1(void)
-{
-	return 0;
 }
 #endif
-
-static int __init hv_vsm_boot_init(void)
-{
-	return hv_vsm_enable_vtl1();
-}
-
-module_init(hv_vsm_boot_init);
-MODULE_DESCRIPTION("Hyper-V VSM Boot VTL0 Driver");
-MODULE_LICENSE("GPL");
diff --git a/include/linux/vsm.h b/include/linux/vsm.h
new file mode 100644
index 000000000000..357e09b7dab8
--- /dev/null
+++ b/include/linux/vsm.h
@@ -0,0 +1,23 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * VSM - Headers
+ *
+ * Copyright © 2023 Microsoft Corporation
+ */
+
+#ifndef __VSM_H__
+#define __VSM_H__
+
+#ifdef CONFIG_HYPERV_VSM
+
+void __init vsm_init(void);
+
+#else /* !CONFIG_HYPERV_VSM */
+
+static inline void vsm_init(void)
+{
+}
+
+#endif /* CONFIG_HYPERV_VSM */
+
+#endif /* __VSM_H__ */
diff --git a/init/main.c b/init/main.c
index b25c779e93ac..1edfd61e1446 100644
--- a/init/main.c
+++ b/init/main.c
@@ -101,6 +101,7 @@
 #include <linux/stackdepot.h>
 #include <linux/randomize_kstack.h>
 #include <net/net_namespace.h>
+#include <linux/vsm.h>
 
 #include <asm/io.h>
 #include <asm/setup.h>
@@ -1552,6 +1553,7 @@ static noinline void __init kernel_init_freeable(void)
 	page_alloc_init_late();
 
 	do_basic_setup();
+	vsm_init();
 
 	kunit_run_all_tests();
 
-- 
2.42.0


From 5bfbd8d700e27ddc8b919933817b66175ea464cd Mon Sep 17 00:00:00 2001
From: Anna Trikalinou <atrikalinou@microsoft.com>
Date: Sat, 23 Dec 2023 01:51:54 +0000
Subject: [PATCH 06/22] Merged PR 109: Verify signatures of Secure Loader and
 Secure Kernel

Generate & verify the signature of Secure Loader and Secure Kernel.

I'm using SHA256 with RSA encryption, which is the standard and what Mariner is using for their own certificate.

**One time setup**
- Install necessary packages
`sudo apt-get install openssl`
- Generate your own public/private key pair. Make sure that you have compiled the Linux kernel at least once before this step, so that you have the file ./certs/x509.genkey
`cd <linux_root_dir>/certs/`
`sudo openssl req -new -nodes -utf8 -sha256 -days 36500 -batch -x509 -config x509.genkey -outform PEM -out mykey.pem -keyout mykey_priv.key`
- (Optional) `sudo openssl rsa -in mykey_priv.key -pubout > mykey_pub.key`
- Edit lvbs_defconfig to add the new certificate
CONFIG_SYSTEM_TRUSTED_KEYS="certs/mykey.pem"
- Generate the signature files for the first time
- Add the signature files to mkinitramfs generation
`sudo ./Microsoft/add_sk_to_initramfs.sh <path_to_out_dir>`

**Sign files**
`cd <linux_root_dir>`
`sudo ./scripts/sign-file -dp sha256 ./certs/mykey_priv.key ./certs/mykey.pem ./<path_to_skloader>/skloader.bin`
`sudo ./scripts/sign-file -dp sha256 ./certs/mykey_priv.key ./certs/mykey.pem ./<path_to_sk>/vmlinux.bin`
This will generate ./<path_to_skloader>/skloader.bin.p7s and ./<path_to_sk>/vmlinux.bin.p7s
Copy both .p7s files to out folder, which is used by add_sk_to_initramfs.sh script.

Every time skloader or vmlinux are changed, the new signature for the corresponding file needs to be generated and added to `out` directory.
---
 drivers/hv/Kconfig       |  11 +++
 drivers/hv/hv_vsm_boot.c | 146 ++++++++++++++++++++++++++++++++++-----
 2 files changed, 139 insertions(+), 18 deletions(-)

diff --git a/drivers/hv/Kconfig b/drivers/hv/Kconfig
index 650be40fb55b..dc00760cfb1e 100644
--- a/drivers/hv/Kconfig
+++ b/drivers/hv/Kconfig
@@ -71,4 +71,15 @@ config HYPERV_VSM_DEBUG
 	depends on HYPERV_VSM
 	default n
 
+config HYPERV_VSM_DISABLE_IMG_VERIFY
+	bool "Disable Image Verification for VSM binaries"
+	depends on HYPERV_VSM
+	help
+	  Select this option to disable VSM Secure Boot.
+	  Enabling this option will disable verifying the integrity and
+	  authenticity of the VSM secure kernel and loader. Enable this
+	  option only when debugging VSM. If this option is set, the
+	  system can be vulnerable to an attacker changing/replacing the
+	  secure kernel and/or loader.
+
 endmenu
diff --git a/drivers/hv/hv_vsm_boot.c b/drivers/hv/hv_vsm_boot.c
index cd25dad1eeaf..d8ddeb858142 100644
--- a/drivers/hv/hv_vsm_boot.c
+++ b/drivers/hv/hv_vsm_boot.c
@@ -18,6 +18,8 @@
 #include <linux/fs.h>
 #include <linux/slab.h>
 #include <linux/cpumask.h>
+#include <linux/verification.h>
+#include <crypto/pkcs7.h>
 
 #include "hv_vsm_boot.h"
 #include "hv_vsm.h"
@@ -26,7 +28,7 @@
 
 extern struct resource sk_res;
 
-static struct file *sk_loader, *sk;
+static struct file *sk_loader, *sk_loader_sig, *sk, *sk_sig;
 static struct page *boot_signal_page, *cpu_online_page, *cpu_present_page;
 static phys_addr_t vsm_skm_pa;
 static void *vsm_skm_va;
@@ -905,6 +907,30 @@ static int __init hv_vsm_enable_vp_vtl(void)
 	return (int) (status & HV_HYPERCALL_RESULT_MASK);
 }
 
+#ifndef CONFIG_HYPERV_VSM_DISABLE_IMG_VERIFY
+static int verify_vsm_signature(char *buffer, unsigned int buff_size, char *signature,
+								unsigned int sig_size)
+{
+	int ret = 0;
+	struct pkcs7_message *pkcs7;
+
+	if (!buffer || !signature)
+		return -EINVAL;
+	pkcs7 = pkcs7_parse_message(signature, sig_size);
+	if (IS_ERR(pkcs7)) {
+		pr_err("%s: pkcs7_parse_message failed. Error code: %ld", __func__, PTR_ERR(pkcs7));
+		return PTR_ERR(pkcs7);
+	}
+	ret = verify_pkcs7_signature(buffer, buff_size, signature, sig_size, NULL,
+								 VERIFYING_UNSPECIFIED_SIGNATURE, NULL, NULL);
+	if (ret) {
+		pr_err("%s: verify_pkcs7_signature failed. Error code: %d", __func__, ret);
+		return ret;
+	}
+	return ret;
+}
+#endif
+
 static int __init hv_vsm_load_secure_kernel(void)
 {
 	/*
@@ -913,16 +939,30 @@ static int __init hv_vsm_load_secure_kernel(void)
 	 */
 	loff_t size_skloader, size_sk;
 	char *skloader_buf = NULL, *sk_buf = NULL;
-	int err;
+	int ret = 0;
+#ifndef CONFIG_HYPERV_VSM_DISABLE_IMG_VERIFY
+	loff_t size_skloader_sig, size_sk_sig;
+	char *skloader_sig_buf = NULL, *sk_sig_buf = NULL;
+#endif
 
 	// Find the size of skloader and sk
 	size_skloader = vfs_llseek(sk_loader, 0, SEEK_END);
 	size_sk = vfs_llseek(sk, 0, SEEK_END);
 
+#ifndef CONFIG_HYPERV_VSM_DISABLE_IMG_VERIFY
+	size_skloader_sig = vfs_llseek(sk_loader_sig, 0, SEEK_END);
+	size_sk_sig = vfs_llseek(sk_sig, 0, SEEK_END);
+#endif
+
 	// Seek back to the beginning of the file
 	vfs_llseek(sk_loader, 0, SEEK_SET);
 	vfs_llseek(sk, 0, SEEK_SET);
 
+#ifndef CONFIG_HYPERV_VSM_DISABLE_IMG_VERIFY
+	vfs_llseek(sk_loader_sig, 0, SEEK_SET);
+	vfs_llseek(sk_sig, 0, SEEK_SET);
+#endif
+
 	// Allocate memory for the buffer
 	skloader_buf = kvmalloc(size_skloader, GFP_KERNEL);
 	if (!skloader_buf) {
@@ -932,31 +972,80 @@ static int __init hv_vsm_load_secure_kernel(void)
 	sk_buf = kvmalloc(size_sk, GFP_KERNEL);
 	if (!sk_buf) {
 		pr_err("%s: Unable to allocate memory for copying secure kernel\n", __func__);
-		kvfree(skloader_buf);
-		return -ENOMEM;
+		ret = -ENOMEM;
+		goto free_skl;
+	}
+
+#ifndef CONFIG_HYPERV_VSM_DISABLE_IMG_VERIFY
+	skloader_sig_buf = kvmalloc(size_skloader_sig, GFP_KERNEL);
+	if (!skloader_sig_buf) {
+		pr_err("%s: Unable to allocate memory for copying secure kernel\n", __func__);
+		ret = -ENOMEM;
+		goto free_sk;
+	}
+	sk_sig_buf = kvmalloc(size_sk_sig, GFP_KERNEL);
+	if (!sk_sig_buf) {
+		pr_err("%s: Unable to allocate memory for copying secure kernel\n", __func__);
+		ret = -ENOMEM;
+		goto free_skl_sig;
 	}
+#endif
 
 	// Read from the file into the buffer
-	err = kernel_read(sk_loader, skloader_buf, size_skloader, &sk_loader->f_pos);
-	if (err != size_skloader) {
+	ret = kernel_read(sk_loader, skloader_buf, size_skloader, &sk_loader->f_pos);
+	if (ret != size_skloader) {
 		pr_err("%s Unable to read skloader.bin file\n", __func__);
-		kvfree(skloader_buf);
-		kvfree(sk_buf);
-		return -1;
+		goto free_bufs;
 	}
-	err = kernel_read(sk, sk_buf, size_sk, &sk->f_pos);
-	if (err != size_sk) {
+	ret = kernel_read(sk, sk_buf, size_sk, &sk->f_pos);
+	if (ret != size_sk) {
 		pr_err("%s Unable to read vmlinux.bin file\n", __func__);
-		kvfree(skloader_buf);
-		kvfree(sk_buf);
-		return -1;
+		goto free_bufs;
+	}
+
+#ifndef CONFIG_HYPERV_VSM_DISABLE_IMG_VERIFY
+	ret = kernel_read(sk_loader_sig, skloader_sig_buf, size_skloader_sig,
+					  &sk_loader_sig->f_pos);
+	if (ret != size_skloader_sig) {
+		pr_err("%s Unable to read skloader.bin.p7s file\n", __func__);
+		goto free_bufs;
+	}
+	ret = kernel_read(sk_sig, sk_sig_buf, size_sk_sig, &sk_sig->f_pos);
+	if (ret != size_sk_sig) {
+		pr_err("%s Unable to read vmlinux.bin.p7s file\n", __func__);
+		goto free_bufs;
+	}
+
+	ret = verify_vsm_signature(skloader_buf, size_skloader, skloader_sig_buf, size_skloader_sig);
+
+	if (ret) {
+		pr_err("%s: Failed to verify Secure Loader signature.", __func__);
+		goto free_bufs;
+	}
+
+	ret = verify_vsm_signature(sk_buf, size_sk, sk_sig_buf, size_sk_sig);
+
+	if (ret) {
+		pr_err("%s: Failed to verify Secure Kernel signature.", __func__);
+		goto free_bufs;
 	}
+#endif
 
 	memcpy(vsm_skm_va, skloader_buf, size_skloader);
 	memcpy(vsm_skm_va + (2 * 1024 * 1024), sk_buf, size_sk);
-	kvfree(skloader_buf);
+	ret = 0;
+
+free_bufs:
+#ifndef CONFIG_HYPERV_VSM_DISABLE_IMG_VERIFY
+	kvfree(sk_sig_buf);
+free_skl_sig:
+	kvfree(skloader_sig_buf);
+free_sk:
+#endif
 	kvfree(sk_buf);
-	return 0;
+free_skl:
+	kvfree(skloader_buf);
+	return ret;
 }
 
 void __init vsm_init(void)
@@ -984,9 +1073,24 @@ void __init vsm_init(void)
 	sk = filp_open("/usr/lib/firmware/vmlinux.bin", O_RDONLY, 0);
 	if (IS_ERR(sk)) {
 		pr_err("%s: File usr/lib/firmware/vmlinux.bin not found\n", __func__);
-		goto close_file;
+		ret = -ENOENT;
+		goto close_skl_file;
 	}
 
+#ifndef CONFIG_HYPERV_VSM_DISABLE_IMG_VERIFY
+	sk_loader_sig = filp_open("/usr/lib/firmware/skloader.bin.p7s", O_RDONLY, 0);
+	if (IS_ERR(sk_loader_sig)) {
+		pr_err("%s: File usr/lib/firmware/skloader.bin.p7s not found\n", __func__);
+		ret = -ENOENT;
+		goto close_sk_file;
+	}
+	sk_sig = filp_open("/usr/lib/firmware/vmlinux.bin.p7s", O_RDONLY, 0);
+	if (IS_ERR(sk_sig)) {
+		pr_err("%s: File usr/lib/firmware/vmlinux.bin.p7s not found\n", __func__);
+		ret = -ENOENT;
+		goto close_skl_sig_file;
+	}
+#endif
 	ret = hv_vsm_get_code_page_offsets();
 	if (ret) {
 		pr_err("%s: Unbable to retrieve vsm page offsets\n", __func__);
@@ -1084,8 +1188,14 @@ void __init vsm_init(void)
 	free_cpumask_var(mask);
 
 close_files:
+#ifndef CONFIG_HYPERV_VSM_DISABLE_IMG_VERIFY
+	filp_close(sk_sig, NULL);
+close_skl_sig_file:
+	filp_close(sk_loader_sig, NULL);
+close_sk_file:
+#endif
 	filp_close(sk, NULL);
-close_file:
+close_skl_file:
 	filp_close(sk_loader, NULL);
 free_mem:
 	vunmap(vsm_skm_va);
-- 
2.42.0


From 2f5f4794334fde2acbe1c4d4e4be3d3316ab54f8 Mon Sep 17 00:00:00 2001
From: "Madhavan T. Venkataraman" <madvenka@linux.microsoft.com>
Date: Sat, 13 Apr 2024 00:59:35 -0500
Subject: [PATCH 07/22] virt: Introduce Hypervisor Enforced Kernel Integrity
 (Heki)
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Hypervisor Enforced Kernel Integrity (Heki) is a feature that will use
the hypervisor to enhance guest virtual machine security.

Implement minimal code to introduce Heki:

- Define the config variables.

- Define a kernel command line parameter "heki" to turn the feature
  on or off. By default, Heki is on.

- Define heki_early_init() and call it in start_kernel(). Currently,
  this function only prints the value of the "heki" command
  line parameter.

Co-developed-by: Mickaël Salaün <mic@digikod.net>
Signed-off-by: Mickaël Salaün <mic@digikod.net>
Signed-off-by: Madhavan T. Venkataraman <madvenka@linux.microsoft.com>
---
 Kconfig              |  2 ++
 arch/x86/Kconfig     |  1 +
 include/linux/heki.h | 31 +++++++++++++++++++++++++++++++
 init/main.c          |  3 ++-
 mm/mm_init.c         |  1 +
 virt/Makefile        |  1 +
 virt/heki/Kconfig    | 19 +++++++++++++++++++
 virt/heki/Makefile   |  3 +++
 virt/heki/common.h   | 16 ++++++++++++++++
 virt/heki/main.c     | 32 ++++++++++++++++++++++++++++++++
 10 files changed, 108 insertions(+), 1 deletion(-)
 create mode 100644 include/linux/heki.h
 create mode 100644 virt/heki/Kconfig
 create mode 100644 virt/heki/Makefile
 create mode 100644 virt/heki/common.h
 create mode 100644 virt/heki/main.c

diff --git a/Kconfig b/Kconfig
index 745bc773f567..0c844d9bcb03 100644
--- a/Kconfig
+++ b/Kconfig
@@ -29,4 +29,6 @@ source "lib/Kconfig"
 
 source "lib/Kconfig.debug"
 
+source "virt/heki/Kconfig"
+
 source "Documentation/Kconfig"
diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
index 0ca3130c6c8f..1c07d3e8f402 100644
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -35,6 +35,7 @@ config X86_64
 	select SWIOTLB
 	select ARCH_HAS_ELFCORE_COMPAT
 	select ZONE_DMA32
+	select ARCH_SUPPORTS_HEKI
 
 config FORCE_DYNAMIC_FTRACE
 	def_bool y
diff --git a/include/linux/heki.h b/include/linux/heki.h
new file mode 100644
index 000000000000..4c18d2283392
--- /dev/null
+++ b/include/linux/heki.h
@@ -0,0 +1,31 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * Hypervisor Enforced Kernel Integrity (Heki) - Definitions
+ *
+ * Copyright © 2023 Microsoft Corporation
+ */
+
+#ifndef __HEKI_H__
+#define __HEKI_H__
+
+#include <linux/types.h>
+#include <linux/cache.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/printk.h>
+
+#ifdef CONFIG_HEKI
+
+extern bool heki_enabled;
+
+void heki_early_init(void);
+
+#else /* !CONFIG_HEKI */
+
+static inline void heki_early_init(void)
+{
+}
+
+#endif /* CONFIG_HEKI */
+
+#endif /* __HEKI_H__ */
diff --git a/init/main.c b/init/main.c
index 1edfd61e1446..59f0b74374c7 100644
--- a/init/main.c
+++ b/init/main.c
@@ -102,7 +102,7 @@
 #include <linux/randomize_kstack.h>
 #include <net/net_namespace.h>
 #include <linux/vsm.h>
-
+#include <linux/heki.h>
 #include <asm/io.h>
 #include <asm/setup.h>
 #include <asm/sections.h>
@@ -1055,6 +1055,7 @@ void start_kernel(void)
 	uts_ns_init();
 	key_init();
 	security_init();
+	heki_early_init();
 	dbg_late_init();
 	net_ns_init();
 	vfs_caches_init();
diff --git a/mm/mm_init.c b/mm/mm_init.c
index 77fd04c83d04..674944329ffc 100644
--- a/mm/mm_init.c
+++ b/mm/mm_init.c
@@ -27,6 +27,7 @@
 #include <linux/swap.h>
 #include <linux/cma.h>
 #include <linux/crash_dump.h>
+#include <linux/heki.h>
 #include "internal.h"
 #include "slab.h"
 #include "shuffle.h"
diff --git a/virt/Makefile b/virt/Makefile
index 1cfea9436af9..4550dc624466 100644
--- a/virt/Makefile
+++ b/virt/Makefile
@@ -1,2 +1,3 @@
 # SPDX-License-Identifier: GPL-2.0-only
 obj-y	+= lib/
+obj-$(CONFIG_HEKI) += heki/
diff --git a/virt/heki/Kconfig b/virt/heki/Kconfig
new file mode 100644
index 000000000000..49695fff6d21
--- /dev/null
+++ b/virt/heki/Kconfig
@@ -0,0 +1,19 @@
+# SPDX-License-Identifier: GPL-2.0
+#
+# Hypervisor Enforced Kernel Integrity (Heki)
+
+config HEKI
+	bool "Hypervisor Enforced Kernel Integrity (Heki)"
+	depends on ARCH_SUPPORTS_HEKI
+	help
+	  This feature enhances guest virtual machine security by taking
+	  advantage of security features provided by the hypervisor for guests.
+	  This feature is helpful in maintaining guest virtual machine security
+	  even after the guest kernel has been compromised.
+
+config ARCH_SUPPORTS_HEKI
+	bool "Architecture support for Heki"
+	help
+	  An architecture should select this when it can successfully build
+	  and run with CONFIG_HEKI. That is, it should provide all of the
+	  architecture support required for the HEKI feature.
diff --git a/virt/heki/Makefile b/virt/heki/Makefile
new file mode 100644
index 000000000000..354e567df71c
--- /dev/null
+++ b/virt/heki/Makefile
@@ -0,0 +1,3 @@
+# SPDX-License-Identifier: GPL-2.0-only
+
+obj-y += main.o
diff --git a/virt/heki/common.h b/virt/heki/common.h
new file mode 100644
index 000000000000..edd98fc650a8
--- /dev/null
+++ b/virt/heki/common.h
@@ -0,0 +1,16 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * Hypervisor Enforced Kernel Integrity (Heki) - Common header
+ *
+ * Copyright © 2023 Microsoft Corporation
+ */
+
+#ifndef _HEKI_COMMON_H
+
+#ifdef pr_fmt
+#undef pr_fmt
+#endif
+
+#define pr_fmt(fmt) "heki-guest: " fmt
+
+#endif /* _HEKI_COMMON_H */
diff --git a/virt/heki/main.c b/virt/heki/main.c
new file mode 100644
index 000000000000..f005dd74d586
--- /dev/null
+++ b/virt/heki/main.c
@@ -0,0 +1,32 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Hypervisor Enforced Kernel Integrity (Heki) - Common code
+ *
+ * Copyright © 2023 Microsoft Corporation
+ */
+
+#include <linux/heki.h>
+
+#include "common.h"
+
+bool heki_enabled __ro_after_init = true;
+
+/*
+ * Must be called after kmem_cache_init().
+ */
+__init void heki_early_init(void)
+{
+	if (!heki_enabled) {
+		pr_warn("Heki is not enabled\n");
+		return;
+	}
+	pr_warn("Heki is enabled\n");
+}
+
+static int __init heki_parse_config(char *str)
+{
+	if (strtobool(str, &heki_enabled))
+		pr_warn("Invalid option string for heki: '%s'\n", str);
+	return 1;
+}
+__setup("heki=", heki_parse_config);
-- 
2.42.0


From 22a4f34b8c749dd437278940054326576fa8e400 Mon Sep 17 00:00:00 2001
From: "Madhavan T. Venkataraman" <madvenka@linux.microsoft.com>
Date: Thu, 6 Jun 2024 09:35:25 -0400
Subject: [PATCH 08/22] heki: Lock guest control registers at the end of guest
 kernel
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

The hypervisor needs to provide some functions to support Heki. These
form the Heki-Hypervisor API.

Define a heki_hypervisor structure to house the API functions. A
hypervisor that supports Heki must instantiate a heki_hypervisor
structure and pass it to the Heki common code. This allows the common
code to access these functions in a hypervisor-agnostic way.

The first function that is implemented is lock_crs() (lock control
registers). That is, certain flags in the control registers are pinned
so that they can never be changed for the lifetime of the guest.

Implement Heki support in the guest:

- Each supported hypervisor in x86 implements a set of functions for the
  guest kernel. Add an init_heki() function to that set.  This function
  initializes Heki-related stuff. Call init_heki() for the detected
  hypervisor in init_hypervisor_platform().

- Implement init_heki() for the guest.

- Pass the heki_hypervisor structure to Heki common code in init_heki().

Implement a heki_late_init() function and call it at the end of kernel
init. This function calls lock_crs(). In other words, control registers
of a guest are locked down at the end of guest kernel init.

Signed-off-by: Mickaël Salaün <mic@digikod.net>
Signed-off-by: Madhavan T. Venkataraman <madvenka@linux.microsoft.com>
---
 arch/x86/include/asm/x86_init.h  |  1 +
 arch/x86/kernel/cpu/hypervisor.c |  1 +
 arch/x86/kernel/x86_init.c       |  1 +
 include/linux/heki.h             | 23 +++++++++++++++++++++++
 init/main.c                      |  1 +
 virt/heki/Kconfig                |  9 ++++++++-
 virt/heki/main.c                 | 25 +++++++++++++++++++++++++
 7 files changed, 60 insertions(+), 1 deletion(-)

diff --git a/arch/x86/include/asm/x86_init.h b/arch/x86/include/asm/x86_init.h
index 0fe4e482a97b..97160e9c3baf 100644
--- a/arch/x86/include/asm/x86_init.h
+++ b/arch/x86/include/asm/x86_init.h
@@ -128,6 +128,7 @@ struct x86_hyper_init {
 	bool (*msi_ext_dest_id)(void);
 	void (*init_mem_mapping)(void);
 	void (*init_after_bootmem)(void);
+	void (*init_heki)(void);
 };
 
 /**
diff --git a/arch/x86/kernel/cpu/hypervisor.c b/arch/x86/kernel/cpu/hypervisor.c
index 553bfbfc3a1b..6085c8129e0c 100644
--- a/arch/x86/kernel/cpu/hypervisor.c
+++ b/arch/x86/kernel/cpu/hypervisor.c
@@ -106,4 +106,5 @@ void __init init_hypervisor_platform(void)
 
 	x86_hyper_type = h->type;
 	x86_init.hyper.init_platform();
+	x86_init.hyper.init_heki();
 }
diff --git a/arch/x86/kernel/x86_init.c b/arch/x86/kernel/x86_init.c
index 3f0718b4a7d2..08a159fde556 100644
--- a/arch/x86/kernel/x86_init.c
+++ b/arch/x86/kernel/x86_init.c
@@ -116,6 +116,7 @@ struct x86_init_ops x86_init __initdata = {
 		.msi_ext_dest_id	= bool_x86_init_noop,
 		.init_mem_mapping	= x86_init_noop,
 		.init_after_bootmem	= x86_init_noop,
+		.init_heki              = x86_init_noop,
 	},
 
 	.acpi = {
diff --git a/include/linux/heki.h b/include/linux/heki.h
index 4c18d2283392..9a508cba51b2 100644
--- a/include/linux/heki.h
+++ b/include/linux/heki.h
@@ -9,6 +9,7 @@
 #define __HEKI_H__
 
 #include <linux/types.h>
+#include <linux/bug.h>
 #include <linux/cache.h>
 #include <linux/init.h>
 #include <linux/kernel.h>
@@ -16,15 +17,37 @@
 
 #ifdef CONFIG_HEKI
 
+/*
+ * A hypervisor that supports Heki will instantiate this structure to
+ * provide hypervisor specific functions for Heki.
+ */
+struct heki_hypervisor {
+	/* Lock control registers. */
+	int (*lock_crs)(void);
+};
+
+/*
+ * If the active hypervisor supports Heki, it will plug its heki_hypervisor
+ * pointer into this heki structure.
+ */
+struct heki {
+	struct heki_hypervisor *hypervisor;
+};
+
+extern struct heki heki;
 extern bool heki_enabled;
 
 void heki_early_init(void);
+void heki_late_init(void);
 
 #else /* !CONFIG_HEKI */
 
 static inline void heki_early_init(void)
 {
 }
+static inline void heki_late_init(void)
+{
+}
 
 #endif /* CONFIG_HEKI */
 
diff --git a/init/main.c b/init/main.c
index 59f0b74374c7..7404b16634a5 100644
--- a/init/main.c
+++ b/init/main.c
@@ -1454,6 +1454,7 @@ static int __ref kernel_init(void *unused)
 	exit_boot_config();
 	free_initmem();
 	mark_readonly();
+	heki_late_init();
 
 	/*
 	 * Kernel mappings are now finalized - update the userspace page-table
diff --git a/virt/heki/Kconfig b/virt/heki/Kconfig
index 49695fff6d21..5ea75b595667 100644
--- a/virt/heki/Kconfig
+++ b/virt/heki/Kconfig
@@ -4,7 +4,7 @@
 
 config HEKI
 	bool "Hypervisor Enforced Kernel Integrity (Heki)"
-	depends on ARCH_SUPPORTS_HEKI
+	depends on ARCH_SUPPORTS_HEKI && HYPERVISOR_SUPPORTS_HEKI
 	help
 	  This feature enhances guest virtual machine security by taking
 	  advantage of security features provided by the hypervisor for guests.
@@ -17,3 +17,10 @@ config ARCH_SUPPORTS_HEKI
 	  An architecture should select this when it can successfully build
 	  and run with CONFIG_HEKI. That is, it should provide all of the
 	  architecture support required for the HEKI feature.
+
+config HYPERVISOR_SUPPORTS_HEKI
+	bool "Hypervisor support for Heki"
+	help
+	  A hypervisor should select this when it can successfully build
+	  and run with CONFIG_HEKI. That is, it should provide all of the
+	  hypervisor support required for the Heki feature.
diff --git a/virt/heki/main.c b/virt/heki/main.c
index f005dd74d586..ff1937e1c946 100644
--- a/virt/heki/main.c
+++ b/virt/heki/main.c
@@ -10,6 +10,7 @@
 #include "common.h"
 
 bool heki_enabled __ro_after_init = true;
+struct heki heki;
 
 /*
  * Must be called after kmem_cache_init().
@@ -21,6 +22,30 @@ __init void heki_early_init(void)
 		return;
 	}
 	pr_warn("Heki is enabled\n");
+
+	if (!heki.hypervisor) {
+		/* This happens for kernels running on bare metal as well. */
+		pr_warn("No support for Heki in the active hypervisor\n");
+		return;
+	}
+	pr_warn("Heki is supported by the active Hypervisor\n");
+}
+
+/*
+ * Must be called after mark_readonly().
+ */
+void heki_late_init(void)
+{
+	struct heki_hypervisor *hypervisor = heki.hypervisor;
+
+	if (!heki_enabled || !heki.hypervisor)
+		return;
+
+	/* Locks control registers so a compromised guest cannot change them. */
+	if (WARN_ON(hypervisor->lock_crs()))
+		return;
+
+	pr_warn("Control registers locked\n");
 }
 
 static int __init heki_parse_config(char *str)
-- 
2.42.0


From 207a8c19e9e45829ea8e1215b5538962521222bb Mon Sep 17 00:00:00 2001
From: Angelina Vu <angelinavu@microsoft.com>
Date: Thu, 6 Jun 2024 17:51:31 -0400
Subject: [PATCH 09/22] Add Hyper-V support for Heki

Create a heki_hypervisor struct for Hyper-V. Implement lock crs functions
for hyper-v to pass the appropriate arguments and call into the secure
kernel .

Signed-off-by: Angelina Vu <angelinavu@microsoft.com>
---
 arch/x86/kernel/cpu/mshyperv.c |   3 +
 drivers/hv/Kconfig             |   1 +
 drivers/hv/Makefile            |   2 +-
 drivers/hv/hv_vsm.c            | 125 +++++++++++++++++++++++++++++++++
 drivers/hv/hv_vsm.h            |   1 +
 include/linux/vsm.h            |   6 ++
 6 files changed, 137 insertions(+), 1 deletion(-)
 create mode 100644 drivers/hv/hv_vsm.c

diff --git a/arch/x86/kernel/cpu/mshyperv.c b/arch/x86/kernel/cpu/mshyperv.c
index e6bba12c759c..fe93974a33ed 100644
--- a/arch/x86/kernel/cpu/mshyperv.c
+++ b/arch/x86/kernel/cpu/mshyperv.c
@@ -18,6 +18,8 @@
 #include <linux/kexec.h>
 #include <linux/i8253.h>
 #include <linux/random.h>
+#include <linux/heki.h>
+#include <linux/vsm.h>
 #include <asm/processor.h>
 #include <asm/hypervisor.h>
 #include <asm/hyperv-tlfs.h>
@@ -640,6 +642,7 @@ const __initconst struct hypervisor_x86 x86_hyper_ms_hyperv = {
 	.init.x2apic_available	= ms_hyperv_x2apic_available,
 	.init.msi_ext_dest_id	= ms_hyperv_msi_ext_dest_id,
 	.init.init_platform	= ms_hyperv_init_platform,
+	.init.init_heki		= hv_vsm_init_heki,
 #ifdef CONFIG_AMD_MEM_ENCRYPT
 	.runtime.sev_es_hcall_prepare = hv_sev_es_hcall_prepare,
 	.runtime.sev_es_hcall_finish = hv_sev_es_hcall_finish,
diff --git a/drivers/hv/Kconfig b/drivers/hv/Kconfig
index dc00760cfb1e..8fac045891f4 100644
--- a/drivers/hv/Kconfig
+++ b/drivers/hv/Kconfig
@@ -9,6 +9,7 @@ config HYPERV
 	select PARAVIRT
 	select X86_HV_CALLBACK_VECTOR if X86
 	select OF_EARLY_FLATTREE if OF
+	select HYPERVISOR_SUPPORTS_HEKI
 	help
 	  Select this option to run Linux as a Hyper-V client operating
 	  system.
diff --git a/drivers/hv/Makefile b/drivers/hv/Makefile
index 44efca6b2993..744b896beed1 100644
--- a/drivers/hv/Makefile
+++ b/drivers/hv/Makefile
@@ -15,4 +15,4 @@ hv_utils-y := hv_util.o hv_kvp.o hv_snapshot.o hv_fcopy.o hv_utils_transport.o
 
 # Code that must be built-in
 obj-$(subst m,y,$(CONFIG_HYPERV)) += hv_common.o
-obj-$(subst m,y,$(CONFIG_HYPERV_VSM)) += hv_vsm_securekernel.o hv_vsm_boot.o
+obj-$(subst m,y,$(CONFIG_HYPERV_VSM)) += hv_vsm_securekernel.o hv_vsm_boot.o hv_vsm.o
diff --git a/drivers/hv/hv_vsm.c b/drivers/hv/hv_vsm.c
new file mode 100644
index 000000000000..4333c17f1e2a
--- /dev/null
+++ b/drivers/hv/hv_vsm.c
@@ -0,0 +1,125 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (c) 2023, Microsoft Corporation.
+ *
+ */
+
+#include <linux/types.h>
+#include <linux/cpumask.h>
+#include <linux/sched.h>
+#include <linux/vsm.h>
+#include <linux/heki.h>
+#include <asm/mshyperv.h>
+#include "hv_vsm.h"
+
+static void __hv_vsm_vtlcall(struct vtlcall_param *args)
+{
+	u64 hcall_addr;
+
+	hcall_addr = (u64)((u8 *)hv_hypercall_pg + vsm_code_page_offsets.vtl_call_offset);
+	register u64 hypercall_addr asm("rax") = hcall_addr;
+
+	asm __volatile__ (	\
+	/*
+	 * Keep copies of the registers we modify.
+	 * Everything else is saved and restored by VTL1.
+	 */
+		"pushq	%%rdi\n"
+		"pushq	%%rsi\n"
+		"pushq	%%rdx\n"
+		"pushq	%%r8\n"
+		"pushq	%%rcx\n"
+		"pushq	%%rax\n"
+	/*
+	 * The vtlcall_param structure is in rdi, which is modified below, so copy it into a
+	 * register that stays constant in the instructon block immediately following.
+	 */
+		"movq	%1, %%rcx\n"
+	/* Copy values from vtlcall_param structure into registers used to communicate with VTL1 */
+		"movq	0x00(%%rcx), %%rdi\n"
+		"movq	0x08(%%rcx), %%rsi\n"
+		"movq	0x10(%%rcx), %%rdx\n"
+		"movq	0x18(%%rcx), %%r8\n"
+	/* Make rcx 0 */
+		"xorl	%%ecx, %%ecx\n"
+	/* VTL call */
+		CALL_NOSPEC
+	/* Restore rcx to args after VTL call */
+		"movq	40(%%rsp),  %%rcx\n"
+	/* Copy values from registers used to communicate with VTL1 into vtlcall_param structure */
+		"movq	%%rdi,  0x00(%%rcx)\n"
+		"movq	%%rsi,  0x08(%%rcx)\n"
+		"movq	%%rdx,  0x10(%%rcx)\n"
+		"movq	%%r8,  0x18(%%rcx)\n"
+	/* Restore all modified registers */
+		"popq	%%rax\n"
+		"popq	%%rcx\n"
+		"popq	%%r8\n"
+		"popq	%%rdx\n"
+		"popq	%%rsi\n"
+		"popq	%%rdi\n"
+		: ASM_CALL_CONSTRAINT
+		: "D"(args), THUNK_TARGET(hypercall_addr)
+		: "cc", "memory");
+}
+
+static int hv_vsm_vtlcall(struct vtlcall_param *args)
+{
+	unsigned long flags = 0;
+
+	local_irq_save(flags);
+	__hv_vsm_vtlcall(args);
+	local_irq_restore(flags);
+
+	return (int)args->a3;
+}
+
+#ifdef CONFIG_HEKI
+
+static int hv_vsm_lock_crs(void)
+{
+	cpumask_var_t orig_mask;
+	struct vtlcall_param args = {0};
+	int cpu, ret = 0;
+
+	if (!hv_vsm_boot_success)
+		return -ENOTSUPP;
+
+	args.a0 = VSM_VTL_CALL_FUNC_ID_LOCK_REGS;
+
+	if (!alloc_cpumask_var(&orig_mask, GFP_KERNEL)) {
+		ret = -ENOMEM;
+		goto out;
+	}
+	cpumask_copy(orig_mask, &current->cpus_mask);
+	/*
+	 * ToDo: Spin off separate threads on each cpu to do this.
+	 * Should be better from a performance point of view.
+	 * Irrespective this thread should wait until all cpus have locked
+	 * the registers
+	 */
+	for_each_online_cpu(cpu) {
+		set_cpus_allowed_ptr(current, cpumask_of(cpu));
+		ret = hv_vsm_vtlcall(&args);
+		if (ret) {
+			pr_err("%s: Unable to lock registers for cpu%d..Aborting\n",
+			       __func__, cpu);
+			break;
+		}
+	}
+	set_cpus_allowed_ptr(current, orig_mask);
+	free_cpumask_var(orig_mask);
+
+out:
+	return ret;
+}
+
+static struct heki_hypervisor hyperv_heki_hypervisor = {
+	.lock_crs = hv_vsm_lock_crs,
+};
+
+void __init hv_vsm_init_heki(void)
+{
+	heki.hypervisor = &hyperv_heki_hypervisor;
+}
+#endif
diff --git a/drivers/hv/hv_vsm.h b/drivers/hv/hv_vsm.h
index 4376a1aa3356..2e5cbdc0ff05 100644
--- a/drivers/hv/hv_vsm.h
+++ b/drivers/hv/hv_vsm.h
@@ -11,6 +11,7 @@
 
 #define VSM_VTL_CALL_FUNC_ID_ENABLE_APS_VTL	0x1FFE0
 #define VSM_VTL_CALL_FUNC_ID_BOOT_APS		0x1FFE1
+#define VSM_VTL_CALL_FUNC_ID_LOCK_REGS		0x1FFE2
 
 extern bool hv_vsm_boot_success;
 extern bool hv_vsm_mbec_enabled;
diff --git a/include/linux/vsm.h b/include/linux/vsm.h
index 357e09b7dab8..4b3f651fa1d1 100644
--- a/include/linux/vsm.h
+++ b/include/linux/vsm.h
@@ -12,6 +12,12 @@
 
 void __init vsm_init(void);
 
+#ifdef CONFIG_HEKI
+void hv_vsm_init_heki(void);
+#else
+static inline void hv_vsm_init_heki(void) { }
+#endif /* CONFIG_HEKI */
+
 #else /* !CONFIG_HYPERV_VSM */
 
 static inline void vsm_init(void)
-- 
2.42.0


From 50d398dc270e520270dd1f1dbd9a47d02475d724 Mon Sep 17 00:00:00 2001
From: "Madhavan T. Venkataraman" <madvenka@linux.microsoft.com>
Date: Wed, 10 Jul 2024 05:05:46 -0500
Subject: [PATCH 10/22] heki: Make the VTL0 kernel immutable after boot

Inform VTL1 that the guest kernel has finished booting so that VTL1 can
disallow certain operations after boot. E.g. all booting apis, register
locking apis and in future requests to change ept permissions without
authentication.

Beyond this point, all changes to the EPT require some form of
authentication. E.g., signature-based authentication for validating
modules and setting permissions for module sections.

Signed-off-by: Madhavan T. Venkataraman <madvenka@linux.microsoft.com>
---
 drivers/hv/hv_vsm.c  | 12 ++++++++++++
 drivers/hv/hv_vsm.h  |  1 +
 include/linux/heki.h |  3 +++
 virt/heki/main.c     |  8 ++++++++
 4 files changed, 24 insertions(+)

diff --git a/drivers/hv/hv_vsm.c b/drivers/hv/hv_vsm.c
index 4333c17f1e2a..d7c26b32575e 100644
--- a/drivers/hv/hv_vsm.c
+++ b/drivers/hv/hv_vsm.c
@@ -114,8 +114,20 @@ static int hv_vsm_lock_crs(void)
 	return ret;
 }
 
+static int hv_vsm_signal_end_of_boot(void)
+{
+	struct vtlcall_param args = {0};
+
+	if (!hv_vsm_boot_success)
+		return -ENOTSUPP;
+
+	args.a0 = VSM_VTL_CALL_FUNC_ID_SIGNAL_END_OF_BOOT;
+	return hv_vsm_vtlcall(&args);
+}
+
 static struct heki_hypervisor hyperv_heki_hypervisor = {
 	.lock_crs = hv_vsm_lock_crs,
+	.finish_boot = hv_vsm_signal_end_of_boot,
 };
 
 void __init hv_vsm_init_heki(void)
diff --git a/drivers/hv/hv_vsm.h b/drivers/hv/hv_vsm.h
index 2e5cbdc0ff05..0e3ccbc83393 100644
--- a/drivers/hv/hv_vsm.h
+++ b/drivers/hv/hv_vsm.h
@@ -12,6 +12,7 @@
 #define VSM_VTL_CALL_FUNC_ID_ENABLE_APS_VTL	0x1FFE0
 #define VSM_VTL_CALL_FUNC_ID_BOOT_APS		0x1FFE1
 #define VSM_VTL_CALL_FUNC_ID_LOCK_REGS		0x1FFE2
+#define VSM_VTL_CALL_FUNC_ID_SIGNAL_END_OF_BOOT	0x1FFE3
 
 extern bool hv_vsm_boot_success;
 extern bool hv_vsm_mbec_enabled;
diff --git a/include/linux/heki.h b/include/linux/heki.h
index 9a508cba51b2..f428ed2c0b53 100644
--- a/include/linux/heki.h
+++ b/include/linux/heki.h
@@ -24,6 +24,9 @@
 struct heki_hypervisor {
 	/* Lock control registers. */
 	int (*lock_crs)(void);
+
+	/* Signal end of kernel boot */
+	int (*finish_boot)(void);
 };
 
 /*
diff --git a/virt/heki/main.c b/virt/heki/main.c
index ff1937e1c946..52f69e21c883 100644
--- a/virt/heki/main.c
+++ b/virt/heki/main.c
@@ -46,6 +46,14 @@ void heki_late_init(void)
 		return;
 
 	pr_warn("Control registers locked\n");
+
+	/* 
+	 * Signal end of kernel boot.
+	 * This means all boot time lvbs protections are in place and protections on
+	 * many of the resources cannot be altered now.
+	 */
+	if (hypervisor->finish_boot())
+		hypervisor->finish_boot();
 }
 
 static int __init heki_parse_config(char *str)
-- 
2.42.0


From f9c04f8ff8b33086aa2ed6c39cb2cde270882278 Mon Sep 17 00:00:00 2001
From: "Madhavan T. Venkataraman" <madvenka@linux.microsoft.com>
Date: Sun, 7 Jul 2024 11:58:01 -0500
Subject: [PATCH 11/22] heki: Implement a kernel page table walker
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

The Heki feature needs to do the following:

- Find kernel mappings.

- Determine the permissions associated with each mapping.

- Send the mappings to VTL1 for establishing EPT permissions for the
  mappings.

This way, a guest physical page can reflect only the required
permissions in the EPT.

Implement a kernel page table walker that walks all of the kernel
mappings and calls a callback function for each mapping.

Signed-off-by: Mickaël Salaün <mic@digikod.net>
Signed-off-by: Madhavan T. Venkataraman <madvenka@linux.microsoft.com>
---
 include/linux/heki.h |  16 +++++
 virt/heki/Makefile   |   1 +
 virt/heki/walk.c     | 148 +++++++++++++++++++++++++++++++++++++++++++
 3 files changed, 165 insertions(+)
 create mode 100644 virt/heki/walk.c

diff --git a/include/linux/heki.h b/include/linux/heki.h
index f428ed2c0b53..aeb9fa9e98d4 100644
--- a/include/linux/heki.h
+++ b/include/linux/heki.h
@@ -37,6 +37,22 @@ struct heki {
 	struct heki_hypervisor *hypervisor;
 };
 
+/*
+ * The kernel page table is walked to locate kernel mappings. For each
+ * mapping, a callback function is called. The table walker passes information
+ * about the mapping to the callback using this structure.
+ */
+struct heki_args {
+	/* Information passed by the table walker to the callback. */
+	unsigned long va;
+	phys_addr_t pa;
+	size_t size;
+	unsigned long flags;
+};
+
+/* Callback function called by the table walker. */
+typedef void (*heki_func_t)(struct heki_args *args);
+
 extern struct heki heki;
 extern bool heki_enabled;
 
diff --git a/virt/heki/Makefile b/virt/heki/Makefile
index 354e567df71c..a5daa4ff7a4f 100644
--- a/virt/heki/Makefile
+++ b/virt/heki/Makefile
@@ -1,3 +1,4 @@
 # SPDX-License-Identifier: GPL-2.0-only
 
 obj-y += main.o
+obj-y += walk.o
diff --git a/virt/heki/walk.c b/virt/heki/walk.c
new file mode 100644
index 000000000000..0ffae594428f
--- /dev/null
+++ b/virt/heki/walk.c
@@ -0,0 +1,148 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Hypervisor Enforced Kernel Integrity (Heki) - Kernel page table walker.
+ *
+ * Copyright © 2023 Microsoft Corporation
+ *
+ * Cf. arch/x86/mm/init_64.c
+ */
+
+#include <linux/heki.h>
+#include <linux/pgtable.h>
+
+static void heki_walk_pte(pmd_t *pmd, unsigned long va, unsigned long va_end,
+			  heki_func_t func, struct heki_args *args)
+{
+	pte_t *pte;
+	unsigned long next_va;
+
+	for (pte = pte_offset_kernel(pmd, va); va < va_end;
+	     va = next_va, pte++) {
+		next_va = (va + PAGE_SIZE) & PAGE_MASK;
+		if (next_va > va_end)
+			next_va = va_end;
+
+		if (!pte_present(*pte))
+			continue;
+
+		args->va = va;
+		args->pa = pte_pfn(*pte) << PAGE_SHIFT;
+		args->pa += va & (PAGE_SIZE - 1);
+		args->size = next_va - va;
+		args->flags = pte_flags(*pte);
+
+		func(args);
+	}
+}
+
+static void heki_walk_pmd(pud_t *pud, unsigned long va, unsigned long va_end,
+			  heki_func_t func, struct heki_args *args)
+{
+	pmd_t *pmd;
+	unsigned long next_va;
+
+	for (pmd = pmd_offset(pud, va); va < va_end; va = next_va, pmd++) {
+		next_va = pmd_addr_end(va, va_end);
+		if (next_va > va_end)
+			next_va = va_end;
+
+		if (!pmd_present(*pmd))
+			continue;
+
+		if (pmd_large(*pmd)) {
+			args->va = va;
+			args->pa = pmd_pfn(*pmd) << PAGE_SHIFT;
+			args->pa += va & (PMD_SIZE - 1);
+			args->size = next_va - va;
+			args->flags = pmd_flags(*pmd);
+
+			func(args);
+		} else {
+			heki_walk_pte(pmd, va, next_va, func, args);
+		}
+	}
+}
+
+static void heki_walk_pud(p4d_t *p4d, unsigned long va, unsigned long va_end,
+			  heki_func_t func, struct heki_args *args)
+{
+	pud_t *pud;
+	unsigned long next_va;
+
+	for (pud = pud_offset(p4d, va); va < va_end; va = next_va, pud++) {
+		next_va = pud_addr_end(va, va_end);
+		if (next_va > va_end)
+			next_va = va_end;
+
+		if (!pud_present(*pud))
+			continue;
+
+		if (pud_large(*pud)) {
+			args->va = va;
+			args->pa = pud_pfn(*pud) << PAGE_SHIFT;
+			args->pa += va & (PUD_SIZE - 1);
+			args->size = next_va - va;
+			args->flags = pud_flags(*pud);
+
+			func(args);
+		} else {
+			heki_walk_pmd(pud, va, next_va, func, args);
+		}
+	}
+}
+
+static void heki_walk_p4d(pgd_t *pgd, unsigned long va, unsigned long va_end,
+			  heki_func_t func, struct heki_args *args)
+{
+	p4d_t *p4d;
+	unsigned long next_va;
+
+	for (p4d = p4d_offset(pgd, va); va < va_end; va = next_va, p4d++) {
+		next_va = p4d_addr_end(va, va_end);
+		if (next_va > va_end)
+			next_va = va_end;
+
+		if (!p4d_present(*p4d))
+			continue;
+
+		if (p4d_large(*p4d)) {
+			args->va = va;
+			args->pa = p4d_pfn(*p4d) << PAGE_SHIFT;
+			args->pa += va & (P4D_SIZE - 1);
+			args->size = next_va - va;
+			args->flags = p4d_flags(*p4d);
+
+			func(args);
+		} else {
+			heki_walk_pud(p4d, va, next_va, func, args);
+		}
+	}
+}
+
+void heki_walk(unsigned long va, unsigned long va_end, heki_func_t func,
+	       struct heki_args *args)
+{
+	pgd_t *pgd;
+	unsigned long next_va;
+
+	for (pgd = pgd_offset_k(va); va < va_end; va = next_va, pgd++) {
+		next_va = pgd_addr_end(va, va_end);
+		if (next_va > va_end)
+			next_va = va_end;
+
+		if (!pgd_present(*pgd))
+			continue;
+
+		if (pgd_large(*pgd)) {
+			args->va = va;
+			args->pa = pgd_pfn(*pgd) << PAGE_SHIFT;
+			args->pa += va & (PGDIR_SIZE - 1);
+			args->size = next_va - va;
+			args->flags = pgd_flags(*pgd);
+
+			func(args);
+		} else {
+			heki_walk_p4d(pgd, va, next_va, func, args);
+		}
+	}
+}
-- 
2.42.0


From aa44f9a06ff57d3c8cb35c19ee42c2c033f5083c Mon Sep 17 00:00:00 2001
From: "Madhavan T. Venkataraman" <madvenka@linux.microsoft.com>
Date: Mon, 4 Mar 2024 01:08:47 -0600
Subject: [PATCH 12/22] heki: x86: Remove features that need modifiable text

HEKI currently does not address text modifications since that involves
setting write permissions in the EPT for text pages. This will be
addressed in the future.

This means that the following features are not currently supported in
HEKI. Disable them:

	- FTrace
	- KProbes
	- Static Calls
	- BPF and eBPF JIT
	- Jump Label optimization

However, disabling Static Calls causes some build errors on X86 because
of some missing definitions for the generic case. Include those
definitions. Also, disable RETHUNK since it has a hard dependency on
Static Calls Inline.

Finally, PARAVIRT depends on Static Calls. Remove that hard dependency
since PARAVIRT is required for guest support.

Signed-off-by: Madhavan T. Venkataraman <madvenka@linux.microsoft.com>
---
 arch/x86/Kconfig                        | 40 ++++++++++++-------------
 arch/x86/kernel/paravirt.c              |  1 +
 include/linux/static_call.h             |  2 ++
 include/linux/static_call_types.h       |  1 +
 kernel/trace/Kconfig                    |  1 +
 tools/include/linux/static_call_types.h |  1 +
 6 files changed, 26 insertions(+), 20 deletions(-)

diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
index 1c07d3e8f402..57fd391da195 100644
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -124,7 +124,7 @@ config X86
 	select ARCH_USE_QUEUED_SPINLOCKS
 	select ARCH_USE_SYM_ANNOTATIONS
 	select ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH
-	select ARCH_WANT_DEFAULT_BPF_JIT	if X86_64
+	select ARCH_WANT_DEFAULT_BPF_JIT	if X86_64 && !HEKI
 	select ARCH_WANTS_DYNAMIC_TASK_STRUCT
 	select ARCH_WANTS_NO_INSTR
 	select ARCH_WANT_GENERAL_HUGETLB
@@ -174,8 +174,8 @@ config X86
 	select HAVE_ARCH_AUDITSYSCALL
 	select HAVE_ARCH_HUGE_VMAP		if X86_64 || X86_PAE
 	select HAVE_ARCH_HUGE_VMALLOC		if X86_64
-	select HAVE_ARCH_JUMP_LABEL
-	select HAVE_ARCH_JUMP_LABEL_RELATIVE
+	select HAVE_ARCH_JUMP_LABEL		if !HEKI
+	select HAVE_ARCH_JUMP_LABEL_RELATIVE	if !HEKI
 	select HAVE_ARCH_KASAN			if X86_64
 	select HAVE_ARCH_KASAN_VMALLOC		if X86_64
 	select HAVE_ARCH_KFENCE
@@ -207,28 +207,28 @@ config X86
 	select HAVE_BUILDTIME_MCOUNT_SORT
 	select HAVE_DEBUG_KMEMLEAK
 	select HAVE_DMA_CONTIGUOUS
-	select HAVE_DYNAMIC_FTRACE
-	select HAVE_DYNAMIC_FTRACE_WITH_REGS
-	select HAVE_DYNAMIC_FTRACE_WITH_ARGS	if X86_64
-	select HAVE_DYNAMIC_FTRACE_WITH_DIRECT_CALLS
-	select HAVE_SAMPLE_FTRACE_DIRECT	if X86_64
-	select HAVE_SAMPLE_FTRACE_DIRECT_MULTI	if X86_64
-	select HAVE_EBPF_JIT
+	select HAVE_DYNAMIC_FTRACE		if !HEKI
+	select HAVE_DYNAMIC_FTRACE_WITH_REGS	if !HEKI
+	select HAVE_DYNAMIC_FTRACE_WITH_ARGS	if X86_64 && !HEKI
+	select HAVE_DYNAMIC_FTRACE_WITH_DIRECT_CALLS	if !HEKI
+	select HAVE_SAMPLE_FTRACE_DIRECT	if X86_64 && !HEKI
+	select HAVE_SAMPLE_FTRACE_DIRECT_MULTI	if X86_64 && !HEKI
+	select HAVE_EBPF_JIT			if !HEKI
 	select HAVE_EFFICIENT_UNALIGNED_ACCESS
 	select HAVE_EISA
 	select HAVE_EXIT_THREAD
 	select HAVE_FAST_GUP
 	select HAVE_FENTRY			if X86_64 || DYNAMIC_FTRACE
-	select HAVE_FTRACE_MCOUNT_RECORD
+	select HAVE_FTRACE_MCOUNT_RECORD	if !HEKI
 	select HAVE_FUNCTION_GRAPH_RETVAL	if HAVE_FUNCTION_GRAPH_TRACER
 	select HAVE_FUNCTION_GRAPH_TRACER	if X86_32 || (X86_64 && DYNAMIC_FTRACE)
-	select HAVE_FUNCTION_TRACER
+	select HAVE_FUNCTION_TRACER		if !HEKI
 	select HAVE_GCC_PLUGINS
 	select HAVE_HW_BREAKPOINT
 	select HAVE_IOREMAP_PROT
 	select HAVE_IRQ_EXIT_ON_IRQ_STACK	if X86_64
 	select HAVE_IRQ_TIME_ACCOUNTING
-	select HAVE_JUMP_LABEL_HACK		if HAVE_OBJTOOL
+	select HAVE_JUMP_LABEL_HACK		if HAVE_OBJTOOL && !HEKI
 	select HAVE_KERNEL_BZIP2
 	select HAVE_KERNEL_GZIP
 	select HAVE_KERNEL_LZ4
@@ -236,8 +236,8 @@ config X86
 	select HAVE_KERNEL_LZO
 	select HAVE_KERNEL_XZ
 	select HAVE_KERNEL_ZSTD
-	select HAVE_KPROBES
-	select HAVE_KPROBES_ON_FTRACE
+	select HAVE_KPROBES			if !HEKI
+	select HAVE_KPROBES_ON_FTRACE		if !HEKI
 	select HAVE_FUNCTION_ERROR_INJECTION
 	select HAVE_KRETPROBES
 	select HAVE_RETHOOK
@@ -269,9 +269,9 @@ config X86
 	select HAVE_SOFTIRQ_ON_OWN_STACK
 	select HAVE_STACKPROTECTOR		if CC_HAS_SANE_STACKPROTECTOR
 	select HAVE_STACK_VALIDATION		if HAVE_OBJTOOL
-	select HAVE_STATIC_CALL
-	select HAVE_STATIC_CALL_INLINE		if HAVE_OBJTOOL
-	select HAVE_PREEMPT_DYNAMIC_CALL
+	select HAVE_STATIC_CALL			if !HEKI
+	select HAVE_STATIC_CALL_INLINE		if HAVE_OBJTOOL && !HEKI
+	select HAVE_PREEMPT_DYNAMIC_CALL	if !HEKI
 	select HAVE_RSEQ
 	select HAVE_RUST			if X86_64
 	select HAVE_SYSCALL_TRACEPOINTS
@@ -304,7 +304,7 @@ config X86
 	select FUNCTION_ALIGNMENT_16B		if X86_64 || X86_ALIGNMENT_16
 	select FUNCTION_ALIGNMENT_4B
 	imply IMA_SECURE_AND_OR_TRUSTED_BOOT    if EFI
-	select HAVE_DYNAMIC_FTRACE_NO_PATCHABLE
+	select HAVE_DYNAMIC_FTRACE_NO_PATCHABLE		if !HEKI
 
 config INSTRUCTION_DECODER
 	def_bool y
@@ -781,7 +781,6 @@ if HYPERVISOR_GUEST
 
 config PARAVIRT
 	bool "Enable paravirtualization code"
-	depends on HAVE_STATIC_CALL
 	help
 	  This changes the kernel so it can modify itself when it is run
 	  under a hypervisor, potentially improving performance significantly
@@ -2458,6 +2457,7 @@ config RETPOLINE
 config RETHUNK
 	bool "Enable return-thunks"
 	depends on RETPOLINE && CC_HAS_RETURN_THUNK
+	depends on !HEKI
 	select OBJTOOL if HAVE_OBJTOOL
 	default y if X86_64
 	help
diff --git a/arch/x86/kernel/paravirt.c b/arch/x86/kernel/paravirt.c
index 97f1436c1a20..d7eecd2b892c 100644
--- a/arch/x86/kernel/paravirt.c
+++ b/arch/x86/kernel/paravirt.c
@@ -33,6 +33,7 @@
 #include <asm/tlb.h>
 #include <asm/io_bitmap.h>
 #include <asm/gsseg.h>
+#include <asm/text-patching.h>
 
 /*
  * nop stub, which must not clobber anything *including the stack* to
diff --git a/include/linux/static_call.h b/include/linux/static_call.h
index 141e6b176a1b..c2fa4da81568 100644
--- a/include/linux/static_call.h
+++ b/include/linux/static_call.h
@@ -340,6 +340,8 @@ static inline int static_call_text_reserved(void *start, void *end)
 
 #define EXPORT_STATIC_CALL(name)	EXPORT_SYMBOL(STATIC_CALL_KEY(name))
 #define EXPORT_STATIC_CALL_GPL(name)	EXPORT_SYMBOL_GPL(STATIC_CALL_KEY(name))
+#define EXPORT_STATIC_CALL_TRAMP_GPL(name)				\
+	EXPORT_SYMBOL_GPL(STATIC_CALL_KEY(name))
 
 #endif /* CONFIG_HAVE_STATIC_CALL */
 
diff --git a/include/linux/static_call_types.h b/include/linux/static_call_types.h
index 5a00b8b2cf9f..1fcf682658d6 100644
--- a/include/linux/static_call_types.h
+++ b/include/linux/static_call_types.h
@@ -97,6 +97,7 @@ struct static_call_key {
 
 #define static_call(name)						\
 	((typeof(STATIC_CALL_TRAMP(name))*)(STATIC_CALL_KEY(name).func))
+#define static_call_mod(name)	static_call(name)
 
 #endif /* CONFIG_HAVE_STATIC_CALL */
 
diff --git a/kernel/trace/Kconfig b/kernel/trace/Kconfig
index 61c541c36596..a9a06b436181 100644
--- a/kernel/trace/Kconfig
+++ b/kernel/trace/Kconfig
@@ -181,6 +181,7 @@ config TRACING_SUPPORT
 
 menuconfig FTRACE
 	bool "Tracers"
+	depends on !HEKI
 	depends on TRACING_SUPPORT
 	default y if DEBUG_KERNEL
 	help
diff --git a/tools/include/linux/static_call_types.h b/tools/include/linux/static_call_types.h
index 5a00b8b2cf9f..1fcf682658d6 100644
--- a/tools/include/linux/static_call_types.h
+++ b/tools/include/linux/static_call_types.h
@@ -97,6 +97,7 @@ struct static_call_key {
 
 #define static_call(name)						\
 	((typeof(STATIC_CALL_TRAMP(name))*)(STATIC_CALL_KEY(name).func))
+#define static_call_mod(name)	static_call(name)
 
 #endif /* CONFIG_HAVE_STATIC_CALL */
 
-- 
2.42.0


From 23e5a760139a42606b9722d78d630fd1ef2f6666 Mon Sep 17 00:00:00 2001
From: "Madhavan T. Venkataraman" <madvenka@linux.microsoft.com>
Date: Mon, 8 Jul 2024 11:33:01 -0500
Subject: [PATCH 13/22] heki: x86: Initialize permissions for all guest kernel
 pages

Walk the kernel mappings and initialize permissions for all pages that
are mapped.

Walk the direct map mappings and initialize default permissions (RW_)
for all pages that are not mapped.

In the future, these permissions will be passed on to the host to set
the EPT permissions for the pages correctly.

While walking the kernel address space, serialize page table access
with vmap, vunmap and set_memory*() primitives.

Signed-off-by: Madhavan T. Venkataraman <madvenka@linux.microsoft.com>
---
 arch/x86/mm/Makefile     |  2 ++
 arch/x86/mm/heki.c       | 68 ++++++++++++++++++++++++++++++++++++++++
 include/linux/heki.h     | 12 +++++++
 include/linux/mem_attr.h | 25 +++++++++++++++
 virt/heki/Kconfig        |  1 +
 virt/heki/Makefile       |  1 +
 virt/heki/main.c         |  4 +++
 virt/heki/protect.c      | 45 ++++++++++++++++++++++++++
 8 files changed, 158 insertions(+)
 create mode 100644 arch/x86/mm/heki.c
 create mode 100644 include/linux/mem_attr.h
 create mode 100644 virt/heki/protect.c

diff --git a/arch/x86/mm/Makefile b/arch/x86/mm/Makefile
index c80febc44cd2..2998eaac0dbb 100644
--- a/arch/x86/mm/Makefile
+++ b/arch/x86/mm/Makefile
@@ -67,3 +67,5 @@ obj-$(CONFIG_AMD_MEM_ENCRYPT)	+= mem_encrypt_amd.o
 
 obj-$(CONFIG_AMD_MEM_ENCRYPT)	+= mem_encrypt_identity.o
 obj-$(CONFIG_AMD_MEM_ENCRYPT)	+= mem_encrypt_boot.o
+
+obj-$(CONFIG_HEKI)		+= heki.o
diff --git a/arch/x86/mm/heki.c b/arch/x86/mm/heki.c
new file mode 100644
index 000000000000..17700b52369b
--- /dev/null
+++ b/arch/x86/mm/heki.c
@@ -0,0 +1,68 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Hypervisor Enforced Kernel Integrity (Heki) - Arch specific.
+ *
+ * Copyright © 2023 Microsoft Corporation
+ */
+
+#include <linux/heki.h>
+#include <linux/mem_attr.h>
+
+#ifdef pr_fmt
+#undef pr_fmt
+#endif
+
+#define pr_fmt(fmt) "heki-guest: " fmt
+
+static unsigned long kernel_va;
+static unsigned long kernel_end;
+static unsigned long direct_map_va;
+static unsigned long direct_map_end;
+
+void heki_arch_init(void)
+{
+	struct heki_args args = {};
+
+	size_t direct_map_size;
+
+	if (pgtable_l5_enabled()) {
+		kernel_va = 0xff00000000000000UL;
+		kernel_end = 0xffffffffffe00000UL;
+		direct_map_size = 0xff91000000000000UL - 0xff11000000000000UL;
+	} else {
+		kernel_va = 0xffff800000000000UL;
+		kernel_end = 0xffffffffffe00000UL;
+		direct_map_size = 0xffffc88000000000UL - 0xffff888000000000UL;
+	}
+	direct_map_va = PAGE_OFFSET;
+	direct_map_end = direct_map_va + direct_map_size;
+
+	mutex_lock(&heki.lock);
+
+	xa_init(&args.permissions);
+
+	/*
+	 * Walk all the kernel mappings and record the permissions for each
+	 * physical page. If there are multiple mappings to a page, the
+	 * permissions must be ORed.
+	 */
+	heki_init_perm(kernel_va, direct_map_va, &args);
+	heki_init_perm(direct_map_end, kernel_end, &args);
+
+	xa_destroy(&args.permissions);
+
+	mutex_unlock(&heki.lock);
+}
+
+unsigned long heki_flags_to_permissions(unsigned long flags)
+{
+	unsigned long permissions;
+
+	permissions = MEM_ATTR_READ | MEM_ATTR_EXEC;
+	if (flags & _PAGE_RW)
+		permissions |= MEM_ATTR_WRITE;
+	if (flags & _PAGE_NX)
+		permissions &= ~MEM_ATTR_EXEC;
+
+	return permissions;
+}
diff --git a/include/linux/heki.h b/include/linux/heki.h
index aeb9fa9e98d4..ad42d47d8fe4 100644
--- a/include/linux/heki.h
+++ b/include/linux/heki.h
@@ -14,6 +14,7 @@
 #include <linux/init.h>
 #include <linux/kernel.h>
 #include <linux/printk.h>
+#include <linux/xarray.h>
 
 #ifdef CONFIG_HEKI
 
@@ -34,6 +35,7 @@ struct heki_hypervisor {
  * pointer into this heki structure.
  */
 struct heki {
+	struct mutex lock;
 	struct heki_hypervisor *hypervisor;
 };
 
@@ -48,6 +50,7 @@ struct heki_args {
 	phys_addr_t pa;
 	size_t size;
 	unsigned long flags;
+	struct xarray permissions;
 };
 
 /* Callback function called by the table walker. */
@@ -58,6 +61,15 @@ extern bool heki_enabled;
 
 void heki_early_init(void);
 void heki_late_init(void);
+void heki_walk(unsigned long va, unsigned long va_end, heki_func_t func,
+	       struct heki_args *args);
+void heki_map(unsigned long va, unsigned long end);
+void heki_init_perm(unsigned long va, unsigned long end,
+		    struct heki_args *args);
+
+/* Arch-specific functions. */
+void heki_arch_init(void);
+unsigned long heki_flags_to_permissions(unsigned long flags);
 
 #else /* !CONFIG_HEKI */
 
diff --git a/include/linux/mem_attr.h b/include/linux/mem_attr.h
new file mode 100644
index 000000000000..08323c9caf54
--- /dev/null
+++ b/include/linux/mem_attr.h
@@ -0,0 +1,25 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * Guest page permissions - Definitions.
+ *
+ * Copyright © 2023 Microsoft Corporation.
+ */
+#ifndef __MEM_ATTR_H__
+#define __MEM_ATTR_H__
+
+/* clang-format off */
+
+#define MEM_ATTR_READ			BIT(0)
+#define MEM_ATTR_WRITE			BIT(1)
+#define MEM_ATTR_EXEC			BIT(2)
+#define MEM_ATTR_IMMUTABLE		BIT(3)
+
+#define MEM_ATTR_PROT ( \
+	MEM_ATTR_READ | \
+	MEM_ATTR_WRITE | \
+	MEM_ATTR_EXEC | \
+	MEM_ATTR_IMMUTABLE)
+
+/* clang-format on */
+
+#endif /* __MEM_ATTR_H__ */
diff --git a/virt/heki/Kconfig b/virt/heki/Kconfig
index 5ea75b595667..6623b4173c5e 100644
--- a/virt/heki/Kconfig
+++ b/virt/heki/Kconfig
@@ -5,6 +5,7 @@
 config HEKI
 	bool "Hypervisor Enforced Kernel Integrity (Heki)"
 	depends on ARCH_SUPPORTS_HEKI && HYPERVISOR_SUPPORTS_HEKI
+	depends on !X86_16BIT
 	help
 	  This feature enhances guest virtual machine security by taking
 	  advantage of security features provided by the hypervisor for guests.
diff --git a/virt/heki/Makefile b/virt/heki/Makefile
index a5daa4ff7a4f..9dc49588faa3 100644
--- a/virt/heki/Makefile
+++ b/virt/heki/Makefile
@@ -2,3 +2,4 @@
 
 obj-y += main.o
 obj-y += walk.o
+obj-y += protect.o
diff --git a/virt/heki/main.c b/virt/heki/main.c
index 52f69e21c883..cdd1447fc5f0 100644
--- a/virt/heki/main.c
+++ b/virt/heki/main.c
@@ -29,6 +29,8 @@ __init void heki_early_init(void)
 		return;
 	}
 	pr_warn("Heki is supported by the active Hypervisor\n");
+
+	mutex_init(&heki.lock);
 }
 
 /*
@@ -47,6 +49,8 @@ void heki_late_init(void)
 
 	pr_warn("Control registers locked\n");
 
+	heki_arch_init();
+
 	/* 
 	 * Signal end of kernel boot.
 	 * This means all boot time lvbs protections are in place and protections on
diff --git a/virt/heki/protect.c b/virt/heki/protect.c
new file mode 100644
index 000000000000..c66bd68543a5
--- /dev/null
+++ b/virt/heki/protect.c
@@ -0,0 +1,45 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Hypervisor Enforced Kernel Integrity (Heki) - Protect kernel mappings.
+ *
+ * Copyright © 2023 Microsoft Corporation
+ */
+
+#include <linux/heki.h>
+#include <linux/mem_attr.h>
+#include <linux/xarray.h>
+
+#include "common.h"
+
+static void heki_init_perm_cb(struct heki_args *args)
+{
+	unsigned long va;
+	phys_addr_t pa, pa_end;
+	unsigned long pfn, perm, cur_perm;
+
+	if (!pfn_valid(args->pa >> PAGE_SHIFT))
+		return;
+
+	perm = heki_flags_to_permissions(args->flags);
+
+	/* Walk the leaf entries and record page permissions for each page. */
+	pa_end = args->pa + args->size;
+	for (pa = args->pa, va = args->va; pa < pa_end;
+	     pa += PAGE_SIZE, va += PAGE_SIZE) {
+
+		pfn = pa >> PAGE_SHIFT;
+		cur_perm = (unsigned long) xa_load(&args->permissions, pfn);
+		if (cur_perm)
+			perm |= cur_perm;
+		xa_store(&args->permissions, pfn, (void *) perm, GFP_KERNEL);
+	}
+}
+
+/* Find the mappings in the given range and initialize permissions for them. */
+void heki_init_perm(unsigned long va, unsigned long end, struct heki_args *args)
+{
+	va = ALIGN_DOWN(va, PAGE_SIZE);
+	end = ALIGN(end, PAGE_SIZE);
+
+	heki_walk(va, end, heki_init_perm_cb, args);
+}
-- 
2.42.0


From 60998783c1e104a4c2ff290025da25380508b712 Mon Sep 17 00:00:00 2001
From: "Madhavan T. Venkataraman" <madvenka@linux.microsoft.com>
Date: Mon, 8 Jul 2024 14:14:05 -0500
Subject: [PATCH 14/22] heki: x86: Protect guest kernel memory
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Pass guest kernel pages along with their permissions to the host so
that EPT permissions can be set for them.

This is done at the end of kernel init just before kicking off the
init process. Up to this point, the kernel is trusted.

Beyond this point, any request to modify EPT permissions for guest
pages needs to be authenticated in the host. E.g., module loading and
unloading, text patching, eBPF JIT.

This support will be implemented in the future. Until then, modules
have to be built in and the other features have to be disabled.

Co-developed-by: Mickaël Salaün <mic@digikod.net>
Signed-off-by: Mickaël Salaün <mic@digikod.net>
Signed-off-by: Madhavan T. Venkataraman <madvenka@linux.microsoft.com>
---
 arch/x86/mm/heki.c   |   6 +++
 drivers/hv/hv_vsm.c  |  14 +++++
 drivers/hv/hv_vsm.h  |   1 +
 include/linux/heki.h |  46 ++++++++++++++++
 virt/heki/protect.c  | 122 +++++++++++++++++++++++++++++++++++++++++++
 5 files changed, 189 insertions(+)

diff --git a/arch/x86/mm/heki.c b/arch/x86/mm/heki.c
index 17700b52369b..635a7e5640ba 100644
--- a/arch/x86/mm/heki.c
+++ b/arch/x86/mm/heki.c
@@ -49,6 +49,12 @@ void heki_arch_init(void)
 	heki_init_perm(kernel_va, direct_map_va, &args);
 	heki_init_perm(direct_map_end, kernel_end, &args);
 
+	/*
+	 * Pass guest pages along with their permissions to the host so EPT
+	 * permissions can be set for the pages.
+	 */
+	heki_protect(direct_map_va, direct_map_end, &args);
+
 	xa_destroy(&args.permissions);
 
 	mutex_unlock(&heki.lock);
diff --git a/drivers/hv/hv_vsm.c b/drivers/hv/hv_vsm.c
index d7c26b32575e..2a2750cb5a65 100644
--- a/drivers/hv/hv_vsm.c
+++ b/drivers/hv/hv_vsm.c
@@ -125,9 +125,23 @@ static int hv_vsm_signal_end_of_boot(void)
 	return hv_vsm_vtlcall(&args);
 }
 
+static int hv_vsm_protect_memory(phys_addr_t pa, unsigned long nranges)
+{
+	struct vtlcall_param args = {0};
+
+	if (!hv_vsm_boot_success || !hv_vsm_mbec_enabled)
+		return -ENOTSUPP;
+
+	args.a0 = VSM_VTL_CALL_FUNC_ID_PROTECT_MEMORY;
+	args.a1 = pa;
+	args.a2 = nranges;
+	return hv_vsm_vtlcall(&args);
+}
+
 static struct heki_hypervisor hyperv_heki_hypervisor = {
 	.lock_crs = hv_vsm_lock_crs,
 	.finish_boot = hv_vsm_signal_end_of_boot,
+	.protect_memory = hv_vsm_protect_memory,
 };
 
 void __init hv_vsm_init_heki(void)
diff --git a/drivers/hv/hv_vsm.h b/drivers/hv/hv_vsm.h
index 0e3ccbc83393..5d25cb2b7624 100644
--- a/drivers/hv/hv_vsm.h
+++ b/drivers/hv/hv_vsm.h
@@ -13,6 +13,7 @@
 #define VSM_VTL_CALL_FUNC_ID_BOOT_APS		0x1FFE1
 #define VSM_VTL_CALL_FUNC_ID_LOCK_REGS		0x1FFE2
 #define VSM_VTL_CALL_FUNC_ID_SIGNAL_END_OF_BOOT	0x1FFE3
+#define VSM_VTL_CALL_FUNC_ID_PROTECT_MEMORY	0x1FFE4
 
 extern bool hv_vsm_boot_success;
 extern bool hv_vsm_mbec_enabled;
diff --git a/include/linux/heki.h b/include/linux/heki.h
index ad42d47d8fe4..c0ac454c9007 100644
--- a/include/linux/heki.h
+++ b/include/linux/heki.h
@@ -14,10 +14,39 @@
 #include <linux/init.h>
 #include <linux/kernel.h>
 #include <linux/printk.h>
+#include <linux/mm.h>
 #include <linux/xarray.h>
 
 #ifdef CONFIG_HEKI
 
+/*
+ * This structure contains a guest physical range and its attributes (e.g.,
+ * permissions (RWX)).
+ */
+struct heki_range {
+	unsigned long va;
+	phys_addr_t pa;
+	phys_addr_t epa;
+	unsigned long attributes;
+};
+
+/*
+ * Guest ranges are passed to the VMM or hypervisor so they can be authenticated
+ * and their permissions can be set in the host page table. When an array of
+ * these is passed to the Hypervisor or VMM, the array must be in physically
+ * contiguous memory.
+ *
+ * This struct occupies one page. In each page, an array of guest ranges can
+ * be passed. A guest request to the VMM/Hypervisor may contain a list of
+ * these structs (linked by "next_pa").
+ */
+struct heki_page {
+	struct heki_page *next;
+	phys_addr_t next_pa;
+	unsigned long nranges;
+	struct heki_range ranges[];
+};
+
 /*
  * A hypervisor that supports Heki will instantiate this structure to
  * provide hypervisor specific functions for Heki.
@@ -28,6 +57,9 @@ struct heki_hypervisor {
 
 	/* Signal end of kernel boot */
 	int (*finish_boot)(void);
+
+	/* Protect guest memory */
+	int (*protect_memory)(phys_addr_t pa, unsigned long nranges);
 };
 
 /*
@@ -51,6 +83,16 @@ struct heki_args {
 	size_t size;
 	unsigned long flags;
 	struct xarray permissions;
+
+	/* attributes passed to heki_add_pa_range(). */
+	unsigned long attributes;
+
+	/* Page list is built by the callback. */
+	struct heki_page *head;
+	struct heki_page *tail;
+	struct heki_range *cur;
+	unsigned long nranges;
+	phys_addr_t head_pa;
 };
 
 /* Callback function called by the table walker. */
@@ -66,6 +108,10 @@ void heki_walk(unsigned long va, unsigned long va_end, heki_func_t func,
 void heki_map(unsigned long va, unsigned long end);
 void heki_init_perm(unsigned long va, unsigned long end,
 		    struct heki_args *args);
+void heki_protect(unsigned long va, unsigned long end, struct heki_args *args);
+void heki_add_range(struct heki_args *args, unsigned long va,
+		    phys_addr_t pa, phys_addr_t epa);
+void heki_cleanup_args(struct heki_args *args);
 
 /* Arch-specific functions. */
 void heki_arch_init(void);
diff --git a/virt/heki/protect.c b/virt/heki/protect.c
index c66bd68543a5..8d8db3ed282e 100644
--- a/virt/heki/protect.c
+++ b/virt/heki/protect.c
@@ -11,6 +11,8 @@
 
 #include "common.h"
 
+static void heki_apply_permissions(struct heki_args *args);
+
 static void heki_init_perm_cb(struct heki_args *args)
 {
 	unsigned long va;
@@ -43,3 +45,123 @@ void heki_init_perm(unsigned long va, unsigned long end, struct heki_args *args)
 
 	heki_walk(va, end, heki_init_perm_cb, args);
 }
+
+static void heki_protect_cb(struct heki_args *args)
+{
+	unsigned long va;
+	phys_addr_t pa, pa_end;
+	unsigned long pfn, perm, cur_perm;
+
+	if (!pfn_valid(args->pa >> PAGE_SHIFT))
+		return;
+
+	perm = heki_flags_to_permissions(args->flags);
+
+	/* Walk the leaf entries and record page permissions for each page. */
+	pa_end = args->pa + args->size;
+	for (pa = args->pa, va = args->va; pa < pa_end;
+	     pa += PAGE_SIZE, va += PAGE_SIZE) {
+
+		pfn = pa >> PAGE_SHIFT;
+		cur_perm = (unsigned long) xa_load(&args->permissions, pfn);
+
+		args->attributes = cur_perm | perm;
+		heki_add_range(args, va, pa, pa + PAGE_SIZE);
+	}
+}
+
+/* Protect guest memory in the host page table. */
+void heki_protect(unsigned long va, unsigned long end, struct heki_args *args)
+{
+	va = ALIGN_DOWN(va, PAGE_SIZE);
+	end = ALIGN(end, PAGE_SIZE);
+
+	heki_walk(va, end, heki_protect_cb, args);
+	heki_apply_permissions(args);
+}
+
+/*
+ * Build a list of guest pages with their permissions. This list will be
+ * passed to the VMM/Hypervisor to set these permissions in the host page
+ * table.
+ */
+void heki_add_range(struct heki_args *args, unsigned long va,
+		    phys_addr_t pa, phys_addr_t epa)
+{
+	struct heki_page *list = args->head;
+	struct heki_range *cur = args->cur;
+	struct heki_range *range;
+	u64 max_ranges;
+	struct page *page;
+
+	max_ranges = (PAGE_SIZE - sizeof(*list)) / sizeof(*range);
+
+	if (cur && cur->epa == pa && cur->attributes == args->attributes) {
+		cur->epa = epa;
+		return;
+	}
+
+	if (!list || list->nranges == max_ranges) {
+		page = alloc_page(GFP_KERNEL);
+		if (WARN_ON_ONCE(!page))
+			return;
+
+		list = page_address(page);
+		list->nranges = 0;
+		list->next = NULL;
+		list->next_pa = 0;
+
+		if (args->head) {
+			args->tail->next = list;
+			args->tail->next_pa = page_to_pfn(page) << PAGE_SHIFT;
+		} else {
+			args->head = list;
+			args->head_pa = page_to_pfn(page) << PAGE_SHIFT;
+		}
+		args->tail = list;
+	}
+
+	range = &list->ranges[list->nranges];
+	range->va = va;
+	range->pa = pa;
+	range->epa = epa;
+	range->attributes = args->attributes;
+	args->cur = range;
+	list->nranges++;
+	args->nranges++;
+}
+
+void heki_cleanup_args(struct heki_args *args)
+{
+	struct heki_page *list = args->head;
+	phys_addr_t list_pa = args->head_pa;
+	struct page *page;
+
+	/* Free all the pages in the page list. */
+	while (list) {
+		page = pfn_to_page(list_pa >> PAGE_SHIFT);
+		list_pa = list->next_pa;
+		list = list->next;
+		__free_pages(page, 0);
+	}
+}
+
+static void heki_apply_permissions(struct heki_args *args)
+{
+	struct heki_hypervisor *hypervisor = heki.hypervisor;
+	struct heki_page *list = args->head;
+	phys_addr_t list_pa = args->head_pa;
+	int ret;
+
+	if (!list)
+		return;
+
+	/* Protect guest memory in the host page table. */
+	ret = hypervisor->protect_memory(list_pa, args->nranges);
+	if (ret) {
+		pr_warn_ratelimited("Failed to set memory permission\n");
+		return;
+	}
+
+	heki_cleanup_args(args);
+}
-- 
2.42.0


From 6f3f264dad9d26f03d45197b964e727b762c0dcc Mon Sep 17 00:00:00 2001
From: "Madhavan T. Venkataraman" <madvenka@linux.microsoft.com>
Date: Tue, 9 Jul 2024 11:46:21 -0500
Subject: [PATCH 15/22] Send kernel data to VTL1

Kernel data needs to be sent from VTL0 to VTL1 for various purposes.
The VTL call load_kdata() is implemented for this.

For starters, send module certificates. VTL1 will create a trusted
keyring from the module certificates.

In the future, this will be used by VTL1 to verify the signature of a
module when the module is loaded in VTL0.

A kernel data structure is virtually contiguous, but not necessarily
physically contiguous. So, multiple ranges may be sent for a single
data structure. Also, multiple data structures can be sent in a single
call.

To manage this, assign a unique identifier to each kernel data structure.
Set the attributes field of each range in a data structure to the
identifier. This enables VTL0 to gather the ranges of a single data
structure and map them in a single mapping.

Signed-off-by: Madhavan T. Venkataraman <madvenka@linux.microsoft.com>
---
 drivers/hv/hv_vsm.c  | 14 ++++++++
 drivers/hv/hv_vsm.h  |  1 +
 include/linux/heki.h | 10 ++++++
 virt/heki/Makefile   |  1 +
 virt/heki/main.c     |  1 +
 virt/heki/module.c   | 83 ++++++++++++++++++++++++++++++++++++++++++++
 6 files changed, 110 insertions(+)
 create mode 100644 virt/heki/module.c

diff --git a/drivers/hv/hv_vsm.c b/drivers/hv/hv_vsm.c
index 2a2750cb5a65..55e235ac7d0f 100644
--- a/drivers/hv/hv_vsm.c
+++ b/drivers/hv/hv_vsm.c
@@ -138,10 +138,24 @@ static int hv_vsm_protect_memory(phys_addr_t pa, unsigned long nranges)
 	return hv_vsm_vtlcall(&args);
 }
 
+static int hv_vsm_load_kdata(phys_addr_t pa, unsigned long nranges)
+{
+	struct vtlcall_param args = {0};
+
+	if (!hv_vsm_boot_success)
+		return -ENOTSUPP;
+
+	args.a0 = VSM_VTL_CALL_FUNC_ID_LOAD_KDATA;
+	args.a1 = pa;
+	args.a2 = nranges;
+	return hv_vsm_vtlcall(&args);
+}
+
 static struct heki_hypervisor hyperv_heki_hypervisor = {
 	.lock_crs = hv_vsm_lock_crs,
 	.finish_boot = hv_vsm_signal_end_of_boot,
 	.protect_memory = hv_vsm_protect_memory,
+	.load_kdata = hv_vsm_load_kdata,
 };
 
 void __init hv_vsm_init_heki(void)
diff --git a/drivers/hv/hv_vsm.h b/drivers/hv/hv_vsm.h
index 5d25cb2b7624..c93885f38730 100644
--- a/drivers/hv/hv_vsm.h
+++ b/drivers/hv/hv_vsm.h
@@ -14,6 +14,7 @@
 #define VSM_VTL_CALL_FUNC_ID_LOCK_REGS		0x1FFE2
 #define VSM_VTL_CALL_FUNC_ID_SIGNAL_END_OF_BOOT	0x1FFE3
 #define VSM_VTL_CALL_FUNC_ID_PROTECT_MEMORY	0x1FFE4
+#define VSM_VTL_CALL_FUNC_ID_LOAD_KDATA		0x1FFE5
 
 extern bool hv_vsm_boot_success;
 extern bool hv_vsm_mbec_enabled;
diff --git a/include/linux/heki.h b/include/linux/heki.h
index c0ac454c9007..76c3d7bb1b29 100644
--- a/include/linux/heki.h
+++ b/include/linux/heki.h
@@ -15,6 +15,7 @@
 #include <linux/kernel.h>
 #include <linux/printk.h>
 #include <linux/mm.h>
+#include <linux/vmalloc.h>
 #include <linux/xarray.h>
 
 #ifdef CONFIG_HEKI
@@ -47,6 +48,11 @@ struct heki_page {
 	struct heki_range ranges[];
 };
 
+enum heki_kdata_type {
+	HEKI_MODULE_CERTS,
+	HEKI_KDATA_MAX,
+};
+
 /*
  * A hypervisor that supports Heki will instantiate this structure to
  * provide hypervisor specific functions for Heki.
@@ -60,6 +66,9 @@ struct heki_hypervisor {
 
 	/* Protect guest memory */
 	int (*protect_memory)(phys_addr_t pa, unsigned long nranges);
+
+	/* Load kernel data */
+	int (*load_kdata)(phys_addr_t pa, unsigned long nranges);
 };
 
 /*
@@ -112,6 +121,7 @@ void heki_protect(unsigned long va, unsigned long end, struct heki_args *args);
 void heki_add_range(struct heki_args *args, unsigned long va,
 		    phys_addr_t pa, phys_addr_t epa);
 void heki_cleanup_args(struct heki_args *args);
+void heki_load_kdata(void);
 
 /* Arch-specific functions. */
 void heki_arch_init(void);
diff --git a/virt/heki/Makefile b/virt/heki/Makefile
index 9dc49588faa3..bcfee61a4e2a 100644
--- a/virt/heki/Makefile
+++ b/virt/heki/Makefile
@@ -3,3 +3,4 @@
 obj-y += main.o
 obj-y += walk.o
 obj-y += protect.o
+obj-y += module.o
diff --git a/virt/heki/main.c b/virt/heki/main.c
index cdd1447fc5f0..a0fdc6021b44 100644
--- a/virt/heki/main.c
+++ b/virt/heki/main.c
@@ -50,6 +50,7 @@ void heki_late_init(void)
 	pr_warn("Control registers locked\n");
 
 	heki_arch_init();
+	heki_load_kdata();
 
 	/* 
 	 * Signal end of kernel boot.
diff --git a/virt/heki/module.c b/virt/heki/module.c
new file mode 100644
index 000000000000..7e84291c5fbb
--- /dev/null
+++ b/virt/heki/module.c
@@ -0,0 +1,83 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Hypervisor Enforced Kernel Integrity (Heki) - Authentication
+ *
+ * Copyright © 2024 Microsoft Corporation
+ */
+
+#include <linux/heki.h>
+
+#include "common.h"
+
+extern __initconst const u8 system_certificate_list[];
+extern __initconst const unsigned long module_cert_size;
+
+static u8 *heki_module_certs;
+static unsigned long heki_module_cert_size;
+
+static int __init heki_copy_module_certs(void)
+{
+	heki_module_certs = vmalloc(module_cert_size);
+	if (!heki_module_certs) {
+		pr_warn("Failed to alloc module certificates.\n");
+		return -ENOMEM;
+	}
+	heki_module_cert_size = module_cert_size;
+
+	/*
+	 * Copy the module certificates because they will be freed at
+	 * the end of init.
+	 */
+	memcpy(heki_module_certs, system_certificate_list, module_cert_size);
+	return 0;
+}
+core_initcall(heki_copy_module_certs);
+
+static void heki_get_ranges(struct heki_args *args)
+{
+	phys_addr_t pa, pa_end, pa_next;
+	unsigned long va, va_end, va_next;
+
+	if (!pfn_valid(args->pa >> PAGE_SHIFT))
+		return;
+
+	va_end = args->va + args->size;
+	pa_end = args->pa + args->size;
+	for (va = args->va, pa = args->pa;
+	     pa < pa_end;
+	     va = va_next, pa = pa_next) {
+		va_next = (va & PAGE_MASK) + PAGE_SIZE;
+		pa_next = (pa & PAGE_MASK) + PAGE_SIZE;
+		if (pa_next > pa_end) {
+			va_next = va_end;
+			pa_next = pa_end;
+		}
+		heki_add_range(args, va, pa, pa_next);
+	}
+}
+
+void heki_load_kdata(void)
+{
+	struct heki_hypervisor *hypervisor = heki.hypervisor;
+	struct heki_args args = {};
+
+	if (!hypervisor || !heki_module_certs)
+		return;
+
+	mutex_lock(&heki.lock);
+
+	args.attributes = HEKI_MODULE_CERTS;
+	heki_walk((unsigned long) heki_module_certs,
+		  (unsigned long) heki_module_certs + heki_module_cert_size,
+		  heki_get_ranges, &args);
+
+	if (hypervisor->load_kdata(args.head_pa, args.nranges))
+		pr_warn("Failed to load kernel data.\n");
+	else
+		pr_warn("Loaded kernel data\n");
+
+	mutex_unlock(&heki.lock);
+
+	heki_cleanup_args(&args);
+	vfree(heki_module_certs);
+}
-- 
2.42.0


From cb612df15a18a7ea8e32d36a93d7d3a919780217 Mon Sep 17 00:00:00 2001
From: "Madhavan T. Venkataraman" <madvenka@linux.microsoft.com>
Date: Tue, 9 Jul 2024 12:47:34 -0500
Subject: [PATCH 16/22] Validate VTL0 module in VTl1

When a module is loaded, use a hypercall to pass the module blob and
module contents to VTL1. This enables VTL1 to check the signature,
generate module contents independently and validate the module contents.

Signed-off-by: Madhavan T. Venkataraman <madvenka@linux.microsoft.com>
---
 drivers/hv/hv_vsm.c      | 16 +++++++++++++++
 drivers/hv/hv_vsm.h      |  1 +
 include/linux/heki.h     | 21 +++++++++++++++++++
 include/linux/module.h   |  1 +
 kernel/module/internal.h |  2 ++
 kernel/module/main.c     | 30 ++++++++++++++++++++++++++-
 virt/heki/module.c       | 44 ++++++++++++++++++++++++++++++++++++++++
 7 files changed, 114 insertions(+), 1 deletion(-)

diff --git a/drivers/hv/hv_vsm.c b/drivers/hv/hv_vsm.c
index 55e235ac7d0f..45eedcf4b04f 100644
--- a/drivers/hv/hv_vsm.c
+++ b/drivers/hv/hv_vsm.c
@@ -151,11 +151,27 @@ static int hv_vsm_load_kdata(phys_addr_t pa, unsigned long nranges)
 	return hv_vsm_vtlcall(&args);
 }
 
+static long hv_vsm_validate_module(phys_addr_t pa, unsigned long nranges,
+				   unsigned long flags)
+{
+	struct vtlcall_param args = {0};
+
+	if (!hv_vsm_boot_success)
+		return -ENOTSUPP;
+
+	args.a0 = VSM_VTL_CALL_FUNC_ID_VALIDATE_MODULE;
+	args.a1 = pa;
+	args.a2 = nranges;
+	args.a3 = flags;
+	return hv_vsm_vtlcall(&args);
+}
+
 static struct heki_hypervisor hyperv_heki_hypervisor = {
 	.lock_crs = hv_vsm_lock_crs,
 	.finish_boot = hv_vsm_signal_end_of_boot,
 	.protect_memory = hv_vsm_protect_memory,
 	.load_kdata = hv_vsm_load_kdata,
+	.validate_module = hv_vsm_validate_module,
 };
 
 void __init hv_vsm_init_heki(void)
diff --git a/drivers/hv/hv_vsm.h b/drivers/hv/hv_vsm.h
index c93885f38730..39591c2202b0 100644
--- a/drivers/hv/hv_vsm.h
+++ b/drivers/hv/hv_vsm.h
@@ -15,6 +15,7 @@
 #define VSM_VTL_CALL_FUNC_ID_SIGNAL_END_OF_BOOT	0x1FFE3
 #define VSM_VTL_CALL_FUNC_ID_PROTECT_MEMORY	0x1FFE4
 #define VSM_VTL_CALL_FUNC_ID_LOAD_KDATA		0x1FFE5
+#define VSM_VTL_CALL_FUNC_ID_VALIDATE_MODULE	0x1FFE6
 
 extern bool hv_vsm_boot_success;
 extern bool hv_vsm_mbec_enabled;
diff --git a/include/linux/heki.h b/include/linux/heki.h
index 76c3d7bb1b29..acc634166c35 100644
--- a/include/linux/heki.h
+++ b/include/linux/heki.h
@@ -15,6 +15,7 @@
 #include <linux/kernel.h>
 #include <linux/printk.h>
 #include <linux/mm.h>
+#include <linux/module.h>
 #include <linux/vmalloc.h>
 #include <linux/xarray.h>
 
@@ -53,6 +54,12 @@ enum heki_kdata_type {
 	HEKI_KDATA_MAX,
 };
 
+/*
+ * Attribute value for module info that does not conflict with any of the
+ * values in enum mod_mem_type.
+ */
+#define MOD_ELF		MOD_MEM_NUM_TYPES
+
 /*
  * A hypervisor that supports Heki will instantiate this structure to
  * provide hypervisor specific functions for Heki.
@@ -69,6 +76,13 @@ struct heki_hypervisor {
 
 	/* Load kernel data */
 	int (*load_kdata)(phys_addr_t pa, unsigned long nranges);
+
+	/*
+	 * Pass a module blob (ELF file) and module contents to KVM for
+	 * validation.
+	 */
+	long (*validate_module)(phys_addr_t pa, unsigned long nranges,
+				unsigned long flags);
 };
 
 /*
@@ -109,6 +123,7 @@ typedef void (*heki_func_t)(struct heki_args *args);
 
 extern struct heki heki;
 extern bool heki_enabled;
+struct load_info;
 
 void heki_early_init(void);
 void heki_late_init(void);
@@ -122,6 +137,7 @@ void heki_add_range(struct heki_args *args, unsigned long va,
 		    phys_addr_t pa, phys_addr_t epa);
 void heki_cleanup_args(struct heki_args *args);
 void heki_load_kdata(void);
+long heki_validate_module(struct module *mod, struct load_info *info, int flags);
 
 /* Arch-specific functions. */
 void heki_arch_init(void);
@@ -135,6 +151,11 @@ static inline void heki_early_init(void)
 static inline void heki_late_init(void)
 {
 }
+static inline long heki_validate_module(struct module *mod,
+					struct load_info *info, int flags)
+{
+	return 0;
+}
 
 #endif /* CONFIG_HEKI */
 
diff --git a/include/linux/module.h b/include/linux/module.h
index a98e188cf37b..95e0edadef79 100644
--- a/include/linux/module.h
+++ b/include/linux/module.h
@@ -583,6 +583,7 @@ struct module {
 #ifdef CONFIG_DYNAMIC_DEBUG_CORE
 	struct _ddebug_info dyndbg_info;
 #endif
+	long heki_token;
 } ____cacheline_aligned __randomize_layout;
 #ifndef MODULE_ARCH_INIT
 #define MODULE_ARCH_INIT {}
diff --git a/kernel/module/internal.h b/kernel/module/internal.h
index c8b7b4dcf782..e0af200676af 100644
--- a/kernel/module/internal.h
+++ b/kernel/module/internal.h
@@ -63,7 +63,9 @@ struct load_info {
 	/* pointer to module in temporary copy, freed at end of load_module() */
 	struct module *mod;
 	Elf_Ehdr *hdr;
+	Elf_Ehdr *orig_hdr;
 	unsigned long len;
+	unsigned long orig_len;
 	Elf_Shdr *sechdrs;
 	char *secstrings, *strtab;
 	unsigned long symoffs, stroffs, init_typeoffs, core_typeoffs;
diff --git a/kernel/module/main.c b/kernel/module/main.c
index 34d9e718c2c7..7a06ca6064cb 100644
--- a/kernel/module/main.c
+++ b/kernel/module/main.c
@@ -56,6 +56,7 @@
 #include <linux/dynamic_debug.h>
 #include <linux/audit.h>
 #include <linux/cfi.h>
+#include <linux/heki.h>
 #include <linux/debugfs.h>
 #include <uapi/linux/module.h>
 #include "internal.h"
@@ -1978,6 +1979,7 @@ static void free_copy(struct load_info *info, int flags)
 		module_decompress_cleanup(info);
 	else
 		vfree(info->hdr);
+	vfree(info->orig_hdr);
 }
 
 static int rewrite_section_headers(struct load_info *info, int flags)
@@ -2826,6 +2828,16 @@ static int early_mod_check(struct load_info *info, int flags)
 	return err;
 }
 
+static int save_hdr(struct load_info *info)
+{
+	info->orig_len = info->len;
+	info->orig_hdr = vmalloc(info->len);
+	if (!info->orig_hdr)
+		return -ENOMEM;
+	memcpy(info->orig_hdr, info->hdr, info->len);
+	return 0;
+}
+
 /*
  * Allocate and load the module: note that size of section 0 is always
  * zero, and we rely on this for optional sections.
@@ -2835,9 +2847,18 @@ static int load_module(struct load_info *info, const char __user *uargs,
 {
 	struct module *mod;
 	bool module_allocated = false;
-	long err = 0;
+	long err = 0, token;
 	char *after_dashes;
 
+	/*
+	 * The header gets modified in this function. But the original
+	 * header needs to be passed to the hypervisor for verification
+	 * if this is a guest. So, save the original header.
+	 */
+	err = save_hdr(info);
+	if (err)
+		return err;
+
 	/*
 	 * Do the signature check (if any) first. All that
 	 * the signature check needs is info->len, it does
@@ -2874,6 +2895,13 @@ static int load_module(struct load_info *info, const char __user *uargs,
 		goto free_copy;
 	}
 
+	token = heki_validate_module(mod, info, flags);
+	if (token < 0) {
+		err = token;
+		goto free_copy;
+	}
+	mod->heki_token = token;
+
 	module_allocated = true;
 
 	audit_log_kern_module(mod->name);
diff --git a/virt/heki/module.c b/virt/heki/module.c
index 7e84291c5fbb..be7e22d7d64a 100644
--- a/virt/heki/module.c
+++ b/virt/heki/module.c
@@ -6,6 +6,7 @@
  */
 
 #include <linux/heki.h>
+#include "../../kernel/module/internal.h"
 
 #include "common.h"
 
@@ -81,3 +82,46 @@ void heki_load_kdata(void)
 	heki_cleanup_args(&args);
 	vfree(heki_module_certs);
 }
+
+long heki_validate_module(struct module *mod, struct load_info *info, int flags)
+{
+	struct heki_hypervisor *hypervisor = heki.hypervisor;
+	struct heki_args args = {};
+	long token;
+
+	if (!hypervisor)
+		return 0;
+
+	mutex_lock(&heki.lock);
+
+	/* Load original unmodified module ELF buffer. */
+	args.attributes = MOD_ELF;
+	heki_walk((unsigned long) info->orig_hdr,
+		  (unsigned long) info->orig_hdr + info->orig_len,
+		  heki_get_ranges, &args);
+
+	/* Load module sections. */
+	for_each_mod_mem_type(type) {
+		struct module_memory *mem = &mod->mem[type];
+
+		if (!mem->size)
+			continue;
+
+		args.attributes = type;
+		heki_walk((unsigned long) mem->base,
+			  (unsigned long) mem->base + mem->size,
+			  heki_get_ranges, &args);
+	}
+
+	token = hypervisor->validate_module(args.head_pa, args.nranges, flags);
+	if (token < 0) {
+		pr_warn("Failed to validate module %s (%ld).\n",
+			info->name, token);
+	}
+
+	heki_cleanup_args(&args);
+
+	mutex_unlock(&heki.lock);
+
+	return token;
+}
-- 
2.42.0


From 4b7ced520e3cd48347d9bf184b594011d5a04f71 Mon Sep 17 00:00:00 2001
From: "Madhavan T. Venkataraman" <madvenka@linux.microsoft.com>
Date: Tue, 9 Jul 2024 15:41:13 -0500
Subject: [PATCH 17/22] Send kernel symbol tables to VTL1

Send kernel symbol tables to VTL1. VTL1 will use the symbol tables
to resolve VTL0 module symbols and finalize symbol addresses. This is
in preparation for VTL0 module relocation in VTL1.

Signed-off-by: Madhavan T. Venkataraman <madvenka@linux.microsoft.com>
---
 include/linux/heki.h |  9 +++++++++
 kernel/module/main.c | 14 +++++++-------
 virt/heki/module.c   | 23 +++++++++++++++++++++++
 3 files changed, 39 insertions(+), 7 deletions(-)

diff --git a/include/linux/heki.h b/include/linux/heki.h
index acc634166c35..b7c9b61df233 100644
--- a/include/linux/heki.h
+++ b/include/linux/heki.h
@@ -51,6 +51,8 @@ struct heki_page {
 
 enum heki_kdata_type {
 	HEKI_MODULE_CERTS,
+	HEKI_KERNEL_INFO,
+	HEKI_KERNEL_DATA,
 	HEKI_KDATA_MAX,
 };
 
@@ -60,6 +62,13 @@ enum heki_kdata_type {
  */
 #define MOD_ELF		MOD_MEM_NUM_TYPES
 
+struct heki_kinfo {
+	struct kernel_symbol	*ksymtab_start;
+	struct kernel_symbol	*ksymtab_end;
+	struct kernel_symbol	*ksymtab_gpl_start;
+	struct kernel_symbol	*ksymtab_gpl_end;
+};
+
 /*
  * A hypervisor that supports Heki will instantiate this structure to
  * provide hypervisor specific functions for Heki.
diff --git a/kernel/module/main.c b/kernel/module/main.c
index 7a06ca6064cb..31d377789ea0 100644
--- a/kernel/module/main.c
+++ b/kernel/module/main.c
@@ -2895,13 +2895,6 @@ static int load_module(struct load_info *info, const char __user *uargs,
 		goto free_copy;
 	}
 
-	token = heki_validate_module(mod, info, flags);
-	if (token < 0) {
-		err = token;
-		goto free_copy;
-	}
-	mod->heki_token = token;
-
 	module_allocated = true;
 
 	audit_log_kern_module(mod->name);
@@ -2949,6 +2942,13 @@ static int load_module(struct load_info *info, const char __user *uargs,
 	if (err < 0)
 		goto free_modinfo;
 
+	token = heki_validate_module(mod, info, flags);
+	if (token < 0) {
+		err = token;
+		goto free_modinfo;
+	}
+	mod->heki_token = token;
+
 	err = apply_relocations(mod, info);
 	if (err < 0)
 		goto free_modinfo;
diff --git a/virt/heki/module.c b/virt/heki/module.c
index be7e22d7d64a..3e989c03b59b 100644
--- a/virt/heki/module.c
+++ b/virt/heki/module.c
@@ -8,11 +8,15 @@
 #include <linux/heki.h>
 #include "../../kernel/module/internal.h"
 
+#include <asm-generic/sections.h>
+
 #include "common.h"
 
 extern __initconst const u8 system_certificate_list[];
 extern __initconst const unsigned long module_cert_size;
 
+static struct heki_kinfo heki_kinfo;
+
 static u8 *heki_module_certs;
 static unsigned long heki_module_cert_size;
 
@@ -72,6 +76,25 @@ void heki_load_kdata(void)
 		  (unsigned long) heki_module_certs + heki_module_cert_size,
 		  heki_get_ranges, &args);
 
+	heki_kinfo.ksymtab_start =
+			(struct kernel_symbol *) __start___ksymtab;
+	heki_kinfo.ksymtab_end =
+			(struct kernel_symbol *) __stop___ksymtab;
+	heki_kinfo.ksymtab_gpl_start =
+			(struct kernel_symbol *) __start___ksymtab_gpl;
+	heki_kinfo.ksymtab_gpl_end =
+			(struct kernel_symbol *) __stop___ksymtab_gpl;
+
+	args.attributes = HEKI_KERNEL_INFO;
+	heki_walk((unsigned long) &heki_kinfo,
+		  (unsigned long) &heki_kinfo + sizeof(heki_kinfo),
+		  heki_get_ranges, &args);
+
+	args.attributes = HEKI_KERNEL_DATA;
+	heki_walk((unsigned long) __start_rodata,
+		  (unsigned long) __end_rodata,
+		  heki_get_ranges, &args);
+
 	if (hypervisor->load_kdata(args.head_pa, args.nranges))
 		pr_warn("Failed to load kernel data.\n");
 	else
-- 
2.42.0


From 07ee39ee95823cf7c2da1f9498295343f67e5624 Mon Sep 17 00:00:00 2001
From: "Madhavan T. Venkataraman" <madvenka@microsoft.com>
Date: Mon, 1 Jul 2024 14:43:01 -0500
Subject: [PATCH 18/22] Validate guest module in VTL1 after relocations

Pass the module contents from VTL0 to VTL1 after applying relocations.
Perform the same relocations on the module copy in VTL1. After that,
compare the VTL0 and VTL1 module contents to validate the VTL0 module.

Signed-off-by: Madhavan T. Venkataraman <madvenka@linux.microsoft.com>
---
 kernel/module/main.c | 8 ++++----
 1 file changed, 4 insertions(+), 4 deletions(-)

diff --git a/kernel/module/main.c b/kernel/module/main.c
index 31d377789ea0..fb2d920485a2 100644
--- a/kernel/module/main.c
+++ b/kernel/module/main.c
@@ -2942,6 +2942,10 @@ static int load_module(struct load_info *info, const char __user *uargs,
 	if (err < 0)
 		goto free_modinfo;
 
+	err = apply_relocations(mod, info);
+	if (err < 0)
+		goto free_modinfo;
+
 	token = heki_validate_module(mod, info, flags);
 	if (token < 0) {
 		err = token;
@@ -2949,10 +2953,6 @@ static int load_module(struct load_info *info, const char __user *uargs,
 	}
 	mod->heki_token = token;
 
-	err = apply_relocations(mod, info);
-	if (err < 0)
-		goto free_modinfo;
-
 	err = post_relocation(mod, info);
 	if (err < 0)
 		goto free_modinfo;
-- 
2.42.0


From f1827db020c7a52aa0700f8b099fafa6c92075cc Mon Sep 17 00:00:00 2001
From: "Madhavan T. Venkataraman" <madvenka@linux.microsoft.com>
Date: Wed, 10 Jul 2024 01:08:16 -0500
Subject: [PATCH 19/22] Pass a VTL0 module to VTL1 after post relocation fixups

Move the call to validate a VTL0 module after post relocation fixes
to just before complete_formation(). In VTL1, the same post relocation
fixes will be applied independently and module contents will be
validated.

After that, VTL1 will set proper EPT permissions for the VTL0 module
sections.

Signed-off-by: Madhavan T. Venkataraman <madvenka@linux.microsoft.com>
---
 arch/x86/Kconfig            |  2 ++
 arch/x86/include/asm/heki.h | 13 +++++++++++++
 arch/x86/mm/heki.c          |  9 +++++++++
 include/linux/heki.h        |  4 ++++
 kernel/module/main.c        | 14 +++++++-------
 virt/heki/module.c          |  2 ++
 6 files changed, 37 insertions(+), 7 deletions(-)
 create mode 100644 arch/x86/include/asm/heki.h

diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
index 57fd391da195..21fe6bdbd408 100644
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -1811,6 +1811,7 @@ config X86_KERNEL_IBT
 	prompt "Indirect Branch Tracking"
 	def_bool y
 	depends on X86_64 && CC_HAS_IBT && HAVE_OBJTOOL
+	depends on !HEKI
 	# https://github.com/llvm/llvm-project/commit/9d7001eba9c4cb311e03cd8cdc231f9e579f2d0f
 	depends on !LD_IS_LLD || LLD_VERSION >= 140000
 	select OBJTOOL
@@ -2446,6 +2447,7 @@ config PAGE_TABLE_ISOLATION
 
 config RETPOLINE
 	bool "Avoid speculative indirect branches in kernel"
+	depends on !HEKI
 	select OBJTOOL if HAVE_OBJTOOL
 	default y
 	help
diff --git a/arch/x86/include/asm/heki.h b/arch/x86/include/asm/heki.h
new file mode 100644
index 000000000000..76025d7d63fa
--- /dev/null
+++ b/arch/x86/include/asm/heki.h
@@ -0,0 +1,13 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _ASM_X86_HEKI_H
+#define _ASM_X86_HEKI_H
+
+#include <asm/paravirt_types.h>
+
+struct heki_arch_kinfo {
+	struct paravirt_patch_template	pv_ops;
+	void				(*pv_bug)(void);
+	void				(*pv_nop)(void);
+};
+
+#endif /* _ASM_X86_HEKI_H */
diff --git a/arch/x86/mm/heki.c b/arch/x86/mm/heki.c
index 635a7e5640ba..4c2a07c948ca 100644
--- a/arch/x86/mm/heki.c
+++ b/arch/x86/mm/heki.c
@@ -72,3 +72,12 @@ unsigned long heki_flags_to_permissions(unsigned long flags)
 
 	return permissions;
 }
+
+void heki_load_arch_kinfo(struct heki_kinfo *kinfo)
+{
+	struct heki_arch_kinfo *arch_kinfo = &kinfo->arch;
+
+	arch_kinfo->pv_ops = pv_ops;
+	arch_kinfo->pv_bug = paravirt_BUG;
+	arch_kinfo->pv_nop = _paravirt_nop;
+}
diff --git a/include/linux/heki.h b/include/linux/heki.h
index b7c9b61df233..48e46145cc6b 100644
--- a/include/linux/heki.h
+++ b/include/linux/heki.h
@@ -21,6 +21,8 @@
 
 #ifdef CONFIG_HEKI
 
+#include <asm/heki.h>
+
 /*
  * This structure contains a guest physical range and its attributes (e.g.,
  * permissions (RWX)).
@@ -67,6 +69,7 @@ struct heki_kinfo {
 	struct kernel_symbol	*ksymtab_end;
 	struct kernel_symbol	*ksymtab_gpl_start;
 	struct kernel_symbol	*ksymtab_gpl_end;
+	struct heki_arch_kinfo	arch;
 };
 
 /*
@@ -151,6 +154,7 @@ long heki_validate_module(struct module *mod, struct load_info *info, int flags)
 /* Arch-specific functions. */
 void heki_arch_init(void);
 unsigned long heki_flags_to_permissions(unsigned long flags);
+void heki_load_arch_kinfo(struct heki_kinfo *kinfo);
 
 #else /* !CONFIG_HEKI */
 
diff --git a/kernel/module/main.c b/kernel/module/main.c
index fb2d920485a2..3e120a27367b 100644
--- a/kernel/module/main.c
+++ b/kernel/module/main.c
@@ -2946,13 +2946,6 @@ static int load_module(struct load_info *info, const char __user *uargs,
 	if (err < 0)
 		goto free_modinfo;
 
-	token = heki_validate_module(mod, info, flags);
-	if (token < 0) {
-		err = token;
-		goto free_modinfo;
-	}
-	mod->heki_token = token;
-
 	err = post_relocation(mod, info);
 	if (err < 0)
 		goto free_modinfo;
@@ -2971,6 +2964,13 @@ static int load_module(struct load_info *info, const char __user *uargs,
 	/* Ftrace init must be called in the MODULE_STATE_UNFORMED state */
 	ftrace_module_init(mod);
 
+	token = heki_validate_module(mod, info, flags);
+	if (token < 0) {
+		err = token;
+		goto ddebug_cleanup;
+	}
+	mod->heki_token = token;
+
 	/* Finally it's fully formed, ready to start executing. */
 	err = complete_formation(mod, info);
 	if (err)
diff --git a/virt/heki/module.c b/virt/heki/module.c
index 3e989c03b59b..274d68f2a038 100644
--- a/virt/heki/module.c
+++ b/virt/heki/module.c
@@ -85,6 +85,8 @@ void heki_load_kdata(void)
 	heki_kinfo.ksymtab_gpl_end =
 			(struct kernel_symbol *) __stop___ksymtab_gpl;
 
+	heki_load_arch_kinfo(&heki_kinfo);
+
 	args.attributes = HEKI_KERNEL_INFO;
 	heki_walk((unsigned long) &heki_kinfo,
 		  (unsigned long) &heki_kinfo + sizeof(heki_kinfo),
-- 
2.42.0


From 0fa594bbcf2857df56a1b5d41eee5f401553fa2f Mon Sep 17 00:00:00 2001
From: "Madhavan T. Venkataraman" <madvenka@linux.microsoft.com>
Date: Wed, 10 Jul 2024 02:41:54 -0500
Subject: [PATCH 20/22] Free module init sections

VTL0 calls this prior to freeing a module's init sections. Pass the module
token to VTL1 so it can reset the EPT permissions for the init sections
to RW_. Also, make the ro_after_init section read-only in the EPT.
After this step, free module init sections in VTL1.

Signed-off-by: Madhavan T. Venkataraman <madvenka@linux.microsoft.com>
---
 drivers/hv/hv_vsm.c  | 13 +++++++++++++
 drivers/hv/hv_vsm.h  |  1 +
 include/linux/heki.h |  4 ++++
 kernel/module/main.c |  1 +
 virt/heki/module.c   | 19 +++++++++++++++++++
 5 files changed, 38 insertions(+)

diff --git a/drivers/hv/hv_vsm.c b/drivers/hv/hv_vsm.c
index 45eedcf4b04f..1517fe7f0aeb 100644
--- a/drivers/hv/hv_vsm.c
+++ b/drivers/hv/hv_vsm.c
@@ -166,12 +166,25 @@ static long hv_vsm_validate_module(phys_addr_t pa, unsigned long nranges,
 	return hv_vsm_vtlcall(&args);
 }
 
+static int hv_vsm_free_module_init(long token)
+{
+	struct vtlcall_param args = {0};
+
+	if (!hv_vsm_boot_success)
+		return -ENOTSUPP;
+
+	args.a0 = VSM_VTL_CALL_FUNC_ID_FREE_MODULE_INIT;
+	args.a1 = token;
+	return hv_vsm_vtlcall(&args);
+}
+
 static struct heki_hypervisor hyperv_heki_hypervisor = {
 	.lock_crs = hv_vsm_lock_crs,
 	.finish_boot = hv_vsm_signal_end_of_boot,
 	.protect_memory = hv_vsm_protect_memory,
 	.load_kdata = hv_vsm_load_kdata,
 	.validate_module = hv_vsm_validate_module,
+	.free_module_init = hv_vsm_free_module_init,
 };
 
 void __init hv_vsm_init_heki(void)
diff --git a/drivers/hv/hv_vsm.h b/drivers/hv/hv_vsm.h
index 39591c2202b0..2ce3b7938c46 100644
--- a/drivers/hv/hv_vsm.h
+++ b/drivers/hv/hv_vsm.h
@@ -16,6 +16,7 @@
 #define VSM_VTL_CALL_FUNC_ID_PROTECT_MEMORY	0x1FFE4
 #define VSM_VTL_CALL_FUNC_ID_LOAD_KDATA		0x1FFE5
 #define VSM_VTL_CALL_FUNC_ID_VALIDATE_MODULE	0x1FFE6
+#define VSM_VTL_CALL_FUNC_ID_FREE_MODULE_INIT	0x1FFE7
 
 extern bool hv_vsm_boot_success;
 extern bool hv_vsm_mbec_enabled;
diff --git a/include/linux/heki.h b/include/linux/heki.h
index 48e46145cc6b..178122fc8c58 100644
--- a/include/linux/heki.h
+++ b/include/linux/heki.h
@@ -95,6 +95,9 @@ struct heki_hypervisor {
 	 */
 	long (*validate_module)(phys_addr_t pa, unsigned long nranges,
 				unsigned long flags);
+
+	/* Free module init sections. */
+	int (*free_module_init)(long token);
 };
 
 /*
@@ -150,6 +153,7 @@ void heki_add_range(struct heki_args *args, unsigned long va,
 void heki_cleanup_args(struct heki_args *args);
 void heki_load_kdata(void);
 long heki_validate_module(struct module *mod, struct load_info *info, int flags);
+void heki_free_module_init(struct module *mod);
 
 /* Arch-specific functions. */
 void heki_arch_init(void);
diff --git a/kernel/module/main.c b/kernel/module/main.c
index 3e120a27367b..17ecbd354887 100644
--- a/kernel/module/main.c
+++ b/kernel/module/main.c
@@ -2575,6 +2575,7 @@ static noinline int do_init_module(struct module *mod)
 	/* Switch to core kallsyms now init is done: kallsyms may be walking! */
 	rcu_assign_pointer(mod->kallsyms, &mod->core_kallsyms);
 #endif
+	heki_free_module_init(mod);
 	module_enable_ro(mod, true);
 	mod_tree_remove_init(mod);
 	module_arch_freeing_init(mod);
diff --git a/virt/heki/module.c b/virt/heki/module.c
index 274d68f2a038..40ebc2fa3e30 100644
--- a/virt/heki/module.c
+++ b/virt/heki/module.c
@@ -150,3 +150,22 @@ long heki_validate_module(struct module *mod, struct load_info *info, int flags)
 
 	return token;
 }
+
+void heki_free_module_init(struct module *mod)
+{
+	struct heki_hypervisor *hypervisor = heki.hypervisor;
+	int err;
+
+	if (!hypervisor)
+		return;
+
+	mutex_lock(&heki.lock);
+
+	err = hypervisor->free_module_init(mod->heki_token);
+	if (err) {
+		pr_warn("Failed to free module %s init (%d).\n",
+			mod->name, err);
+	}
+
+	mutex_unlock(&heki.lock);
+}
-- 
2.42.0


From 5d149ad12199c367872aff7e5431f503af089f71 Mon Sep 17 00:00:00 2001
From: "Madhavan T. Venkataraman" <madvenka@linux.microsoft.com>
Date: Wed, 10 Jul 2024 03:21:51 -0500
Subject: [PATCH 21/22] Unload VTL0 module

When a module is unloaded in VTL0, pass the module token to VTL1 so it
can restore default EPT permissions (RW) for module sections. Once this
is done, the module can free its memory in VTL0.

Signed-off-by: Madhavan T. Venkataraman <madvenka@linux.microsoft.com>
---
 drivers/hv/hv_vsm.c  | 13 +++++++++++++
 drivers/hv/hv_vsm.h  |  1 +
 include/linux/heki.h |  4 ++++
 kernel/module/main.c |  6 +++++-
 virt/heki/module.c   | 19 +++++++++++++++++++
 5 files changed, 42 insertions(+), 1 deletion(-)

diff --git a/drivers/hv/hv_vsm.c b/drivers/hv/hv_vsm.c
index 1517fe7f0aeb..3ae1b089fc40 100644
--- a/drivers/hv/hv_vsm.c
+++ b/drivers/hv/hv_vsm.c
@@ -178,6 +178,18 @@ static int hv_vsm_free_module_init(long token)
 	return hv_vsm_vtlcall(&args);
 }
 
+static int hv_vsm_unload_module(long token)
+{
+	struct vtlcall_param args = {0};
+
+	if (!hv_vsm_boot_success)
+		return -ENOTSUPP;
+
+	args.a0 = VSM_VTL_CALL_FUNC_ID_UNLOAD_MODULE;
+	args.a1 = token;
+	return hv_vsm_vtlcall(&args);
+}
+
 static struct heki_hypervisor hyperv_heki_hypervisor = {
 	.lock_crs = hv_vsm_lock_crs,
 	.finish_boot = hv_vsm_signal_end_of_boot,
@@ -185,6 +197,7 @@ static struct heki_hypervisor hyperv_heki_hypervisor = {
 	.load_kdata = hv_vsm_load_kdata,
 	.validate_module = hv_vsm_validate_module,
 	.free_module_init = hv_vsm_free_module_init,
+	.unload_module = hv_vsm_unload_module,
 };
 
 void __init hv_vsm_init_heki(void)
diff --git a/drivers/hv/hv_vsm.h b/drivers/hv/hv_vsm.h
index 2ce3b7938c46..0ae9160fc3b1 100644
--- a/drivers/hv/hv_vsm.h
+++ b/drivers/hv/hv_vsm.h
@@ -17,6 +17,7 @@
 #define VSM_VTL_CALL_FUNC_ID_LOAD_KDATA		0x1FFE5
 #define VSM_VTL_CALL_FUNC_ID_VALIDATE_MODULE	0x1FFE6
 #define VSM_VTL_CALL_FUNC_ID_FREE_MODULE_INIT	0x1FFE7
+#define VSM_VTL_CALL_FUNC_ID_UNLOAD_MODULE	0x1FFE8
 
 extern bool hv_vsm_boot_success;
 extern bool hv_vsm_mbec_enabled;
diff --git a/include/linux/heki.h b/include/linux/heki.h
index 178122fc8c58..e3adc43197f9 100644
--- a/include/linux/heki.h
+++ b/include/linux/heki.h
@@ -98,6 +98,9 @@ struct heki_hypervisor {
 
 	/* Free module init sections. */
 	int (*free_module_init)(long token);
+
+	/* Unload module. */
+	int (*unload_module)(long token);
 };
 
 /*
@@ -154,6 +157,7 @@ void heki_cleanup_args(struct heki_args *args);
 void heki_load_kdata(void);
 long heki_validate_module(struct module *mod, struct load_info *info, int flags);
 void heki_free_module_init(struct module *mod);
+void heki_unload_module(struct module *mod);
 
 /* Arch-specific functions. */
 void heki_arch_init(void);
diff --git a/kernel/module/main.c b/kernel/module/main.c
index 17ecbd354887..c5140e87752f 100644
--- a/kernel/module/main.c
+++ b/kernel/module/main.c
@@ -757,6 +757,7 @@ SYSCALL_DEFINE2(delete_module, const char __user *, name_user,
 	blocking_notifier_call_chain(&module_notify_list,
 				     MODULE_STATE_GOING, mod);
 	klp_module_going(mod);
+	heki_unload_module(mod);
 	ftrace_release_mod(mod);
 
 	async_synchronize_full();
@@ -2624,6 +2625,7 @@ static noinline int do_init_module(struct module *mod)
 	blocking_notifier_call_chain(&module_notify_list,
 				     MODULE_STATE_GOING, mod);
 	klp_module_going(mod);
+	heki_unload_module(mod);
 	ftrace_release_mod(mod);
 	free_module(mod);
 	wake_up_all(&module_wq);
@@ -2975,7 +2977,7 @@ static int load_module(struct load_info *info, const char __user *uargs,
 	/* Finally it's fully formed, ready to start executing. */
 	err = complete_formation(mod, info);
 	if (err)
-		goto ddebug_cleanup;
+		goto heki_unload;
 
 	err = prepare_coming_module(mod);
 	if (err)
@@ -3029,6 +3031,8 @@ static int load_module(struct load_info *info, const char __user *uargs,
 	module_bug_cleanup(mod);
 	mutex_unlock(&module_mutex);
 
+ heki_unload:
+	heki_unload_module(mod);
  ddebug_cleanup:
 	ftrace_release_mod(mod);
 	synchronize_rcu();
diff --git a/virt/heki/module.c b/virt/heki/module.c
index 40ebc2fa3e30..4366d058ea4f 100644
--- a/virt/heki/module.c
+++ b/virt/heki/module.c
@@ -169,3 +169,22 @@ void heki_free_module_init(struct module *mod)
 
 	mutex_unlock(&heki.lock);
 }
+
+void heki_unload_module(struct module *mod)
+{
+	struct heki_hypervisor *hypervisor = heki.hypervisor;
+	int err;
+
+	if (!hypervisor)
+		return;
+
+	mutex_lock(&heki.lock);
+
+	err = hypervisor->unload_module(mod->heki_token);
+	if (err) {
+		pr_warn("Failed to unload module %s (%d).\n",
+			mod->name, err);
+	}
+
+	mutex_unlock(&heki.lock);
+}
-- 
2.42.0


From 10a41eb678b013b04ccd8e652c9890b257285ae7 Mon Sep 17 00:00:00 2001
From: Angelina Vu <angelinavu@microsoft.com>
Date: Thu, 15 Aug 2024 05:49:00 +0000
Subject: [PATCH 22/22] include text-patching.h

---
 arch/x86/xen/enlighten_pv.c | 1 +
 1 file changed, 1 insertion(+)

diff --git a/arch/x86/xen/enlighten_pv.c b/arch/x86/xen/enlighten_pv.c
index aeb33e0a3f76..98b59d46b63a 100644
--- a/arch/x86/xen/enlighten_pv.c
+++ b/arch/x86/xen/enlighten_pv.c
@@ -72,6 +72,7 @@
 #include <asm/mwait.h>
 #include <asm/pci_x86.h>
 #include <asm/cpu.h>
+#include <asm/text-patching.h>
 #ifdef CONFIG_X86_IOPL_IOPERM
 #include <asm/io_bitmap.h>
 #endif
-- 
2.42.0

